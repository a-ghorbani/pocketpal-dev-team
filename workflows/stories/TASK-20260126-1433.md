# Story: Fix Memory Estimation for Model Loading by Adding Native getAvailableMemory() API

## Metadata
- **Task ID**: TASK-20260126-1433
- **Issue**: Action Tracker Item - "Memory Estimation for Model Loading"
- **Source**: action-tracker (P2 priority)
- **Complexity**: standard
- **Native Changes**: YES
- **Created**: 2026-01-26
- **Status**: draft

## Environment
- **Worktree**: `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433`
- **Branch**: `feature/TASK-20260126-1433`
- **Base**: `main`

---

## Progress Tracking

### Current Phase
**Phase 1**: `[X] Planning → [X] Approved → [X] Implementing → [X] Testing → [X] Reviewing → [X] PR Created`
**Phase 2**: `[X] Planning → [X] Approved → [X] Implementing → [X] Testing → [X] Reviewing → [X] PR Updated`

### Checkpoints (Updated by Agents)

| Checkpoint | Status | Agent | Commit | Notes |
|------------|--------|-------|--------|-------|
| Worktree created | DONE | orchestrator | - | |
| Story approved | DONE | human | - | |
| Step 1 complete | DONE | implementer | a4e3e41 | Android getAvailableMemory() |
| Step 2 complete | DONE | implementer | 85f65e4 | iOS getAvailableMemory() |
| Step 3 complete | DONE | implementer | 044dd9a | Update TurboModule spec |
| Step 4 complete | DONE | implementer | 3067109 | Update useMemoryCheck logic |
| Step 5 complete | DONE | implementer | d249574 | Update mocks |
| iOS header fix | DONE | implementer | 5016e1b | Fixed os/proc.h import |
| Tests verified | DONE | implementer | - | Existing tests pass |
| Platform builds | DONE | implementer | - | iOS and Android succeed |
| Tests written | DONE | tester | 355a850 | 3 new tests added |
| Review passed | DONE | reviewer | - | All checks pass |
| PR created | DONE | reviewer | - | PR #544 |
| **Phase 2** | | | | |
| Phase 2 planning | DONE | human | - | Hybrid memory tracking approach |
| Phase 2 approved | DONE | human | - | |
| Step 8: Add observable | DONE | implementer | df836c9 | loadedModelMemoryUsage added |
| Step 9: Measure delta | DONE | implementer | d03d4f5 | Memory before/after tracking |
| Step 10: Clear on release | DONE | implementer | 65176a4 | Clear in finally block |
| Step 11: Effective available | DONE | implementer | ed04082 | Calculate effective memory |
| Prettier fix | DONE | implementer | b46f15b | Formatting |
| Phase 2 tests | DONE | implementer | 1ad99e6 | 3 new tests added |
| mmproj memory fix | DONE | implementer | fbeb971 | Measure after multimodal init |
| Phase 2 review | DONE | reviewer | - | All checks pass |
| PR updated | DONE | reviewer | - | PR #544 updated |
| **Clean Redesign** | | | | |
| Clean design doc | DONE | human | - | FINAL DESIGN section added |
| memoryEstimator created | DONE | implementer | 8229ef1 | Single estimation function |
| GGUFMetadata added | DONE | implementer | 8229ef1 | Types updated |
| ModelStore calibration | DONE | implementer | 814a636 | Two variables + lifecycle |
| useMemoryCheck updated | DONE | implementer | c524c41 | Calibration-based logic |
| Tests updated | DONE | implementer | e7e5fb2 | 6/6 tests passing |
| Lint fixes | DONE | implementer | 7683418 | Prettier formatting |
| **Issue Fixes** | | | | |
| Issue 6: Filter mmproj | DONE | implementer | 7e86fe8 | Exclude projection models from metadata loading |
| Issue 7A: Clear metadata | DONE | implementer | a891c3d | Clear ggufMetadata in resetModels for migration |
| Issue 7B: Defensive conversion | DONE | implementer | b1e031a | Add Number() conversion in memoryEstimator |
| **Memory Display UI** | | | | |
| Step 1: L10n strings | DONE | implementer | 8f596e5 | Added memory display strings (en, ja, zh) |
| Step 2: memoryDisplay utils | DONE | implementer | 85c5cfa | Created getMemoryFitStatus, getDeviceMemoryInfo |
| Step 3: MemoryRequirement | DONE | implementer | fa24613 | Created component with MobX observer |
| Step 4: DeviceMemoryBar | DONE | implementer | b02cbb1 | Created bar with Tooltip from paper |
| Step 5: Export components | DONE | implementer | 40fa0f2 | Added to src/components/index.ts |
| Step 6: ModelCard | DONE | implementer | abc9f4d | Added MemoryRequirement after model name |
| Step 7: ModelFileCard | DONE | implementer | 93df032 | Added MemoryRequirement after filename |
| Step 8: ModelsScreen | DONE | implementer | 10992ac | Added DeviceMemoryBar to header |

### Last Agent Handoff
```yaml
from_agent: implementer
to_agent: tester
timestamp: 2026-01-30T23:30:00Z
status: "Memory Display UI implemented - ready for tests"
completed:
  - Step 1: Added L10n strings for memory display (en, ja, zh) - commit 8f596e5
  - Step 2: Created memoryDisplay.ts utility functions - commit 85c5cfa
  - Step 3: Created MemoryRequirement component - commit fa24613
  - Step 4: Created DeviceMemoryBar component - commit b02cbb1
  - Step 5: Exported components in index.ts - commit 40fa0f2
  - Step 6: Added MemoryRequirement to ModelCard - commit abc9f4d
  - Step 7: Added MemoryRequirement to ModelFileCard - commit 93df032
  - Step 8: Added DeviceMemoryBar to ModelsScreen header - commit 10992ac
  - Lint: PASS, TypeCheck: PASS
  - All 8 implementation steps complete
next_steps:
  - Write unit tests for MemoryRequirement component
  - Write unit tests for DeviceMemoryBar component
  - Add integration tests to existing screen tests
  - Verify accessibility
blockers: []
context_for_next_agent: |
  **Issue 6 Fix: Filter mmproj Files from Metadata Loading**

  Problem: loadMissingGGUFMetadata() was processing CLIP projection models, causing log spam.

  Solution: Added ModelType.PROJECTION filter in loadMissingGGUFMetadata():
  ```typescript
  const modelsNeedingMetadata = this.models.filter(
    m =>
      m.isDownloaded &&
      !m.ggufMetadata &&
      m.modelType !== ModelType.PROJECTION,
  );
  ```

  Result: Projection models are now excluded from metadata loading.

  **Issue 7 Fix: Persisted GGUF Metadata Has String Values**

  Problem: Old persisted metadata had string values ("576" instead of 576), causing:
  - String concatenation instead of addition: "49280" + "576" = "49280576"
  - Massive compute buffer calculations: 100+ GB instead of 0.26 GB

  Solution - Two Layers:

  Layer 1 (Migration): resetModels() clears ggufMetadata for all models:
  ```typescript
  // Clear GGUF metadata to force re-fetch with correct number types
  model.ggufMetadata = undefined;
  ```
  Then calls loadMissingGGUFMetadata() to re-fetch.

  Layer 2 (Defense): memoryEstimator.ts defensively converts all numeric fields:
  ```typescript
  // In calculateKVCacheMemory()
  const n_layers = Number(metadata.n_layers);
  const n_embd_head_k = Number(metadata.n_embd_head_k);
  // ... etc

  // In calculateComputeBuffer()
  const n_vocab = Number(metadata.n_vocab);
  const n_embd = Number(metadata.n_embd);
  ```

  Result:
  - Immediate fix: Existing string metadata now works correctly via defensive conversion
  - Permanent fix: resetModels() triggers re-fetch with correct number types
  - Future-proof: Number() conversion handles any type inconsistencies

  **Files Modified:**
  - src/store/ModelStore.ts: Added projection filter, metadata clearing
  - src/utils/memoryEstimator.ts: Added defensive Number() conversions

  ---

  **Memory Display UI Implementation (Phase 3)**

  Successfully implemented user-facing memory display components following the plan in "Future Enhancement: Display Memory Values to Users" section.

  **Components Created:**
  1. `src/utils/memoryDisplay.ts` - Utility functions:
     - `getMemoryFitStatus(model, projectionModel)` - Returns 'fits', 'tight', or 'wont_fit'
     - `getDeviceMemoryInfo()` - Returns availableBytes and totalBytes

  2. `src/components/MemoryRequirement/` - Shows "Required RAM: ~2.1 GB (fits ✓)" on models:
     - MobX observer pattern to track calibration changes
     - Uses existing `formatBytes()` from formatters.ts
     - Uses existing `getModelMemoryRequirement()` from memoryEstimator.ts
     - Color-coded status: primary (fits), error (tight/won't fit)

  3. `src/components/DeviceMemoryBar/` - Shows "~3.2/8 GB usable" bar in ModelsScreen:
     - MobX observer pattern to track calibration changes
     - Uses Tooltip from react-native-paper for tap interaction
     - Custom progress bar with text overlay (not ProgressBar)
     - Right-aligned in header

  **Integration Points:**
  - ModelCard: Added MemoryRequirement after model name (downloaded models only)
  - ModelFileCard: Added MemoryRequirement after filename (all HF files)
  - ModelsScreen: Added DeviceMemoryBar to ListHeaderComponent

  **L10n Support:**
  - Added strings to all 3 languages (en, ja, zh):
    - requiredRAM, fits, tight, wontFit
    - deviceMemory, deviceMemoryTooltip

  **Key Design Choices:**
  - REUSED existing utilities: formatBytes, Tooltip, getModelMemoryRequirement
  - MobX reactivity ensures displays update when models load/unload
  - Graceful fallback when no calibration data (uses 60% heuristic)
  - testID attributes for testing
  - Accessibility labels for screen readers

  **Files Created:**
  - src/utils/memoryDisplay.ts
  - src/components/MemoryRequirement/MemoryRequirement.tsx
  - src/components/MemoryRequirement/index.ts
  - src/components/DeviceMemoryBar/DeviceMemoryBar.tsx
  - src/components/DeviceMemoryBar/index.ts

  **Files Modified:**
  - src/utils/l10n.ts (3 languages)
  - src/components/index.ts
  - src/screens/ModelsScreen/ModelCard/ModelCard.tsx
  - src/screens/ModelsScreen/HFModelSearch/DetailsView/ModelFileCard/ModelFileCard.tsx
  - src/screens/ModelsScreen/ModelsScreen.tsx
  - src/screens/ModelsScreen/styles.ts
```

---

## Context (For Recovery After Context Reset)

> **If you're an agent resuming work on this story:**
> 1. Read the "Progress Tracking" section above
> 2. Check `git log` in the worktree for commits
> 3. Read the "Last Agent Handoff" section
> 4. Continue from the next incomplete checkpoint

### Background

PocketPal AI currently uses a heuristic formula to estimate available memory for model loading:
```typescript
const availableMemory = Math.min(totalMemoryGB * 0.65, totalMemoryGB - 1.2);
```

This formula was created based on limited device testing and overestimates available memory on high-end devices. For example:
- **Pixel 9 (12GB RAM)**: Formula estimates 7.7GB available, but actual is 6.3GB (22% overestimate)
- **OnePlus 6 (8GB RAM)**: Formula estimates 5.2GB available, actual is ~4.8GB (8% overestimate)

This causes Out-Of-Memory (OOM) crashes, which account for ~10% of total app crashes (18 OOM crashes reported with signatures like `lm_ggml_gallocr_alloc_graph`, `lm_ggml_abort`).

User feedback validates this: "Unsuitable Model Size Recommendations For Device" mentioned 3x in recent feedback.

The solution is to query the OS directly for **actual available memory** instead of estimating.

### Current State

**File**: `src/hooks/useMemoryCheck.ts`
- Imports `DeviceInfo.getTotalMemory()` from `react-native-device-info`
- Calculates `availableMemory` using heuristic: `Math.min(totalMemoryGB * 0.65, totalMemoryGB - 1.2)`
- Exports `hasEnoughMemory(modelSize, isMultimodal)` function
- Used by `ModelStore.initContext()` to check if model can load (line 1031 in ModelStore.ts)

**File**: `android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt`
- TurboModule that provides `getCPUInfo()`, `getGPUInfo()`, `getChipset()`
- Does NOT currently provide memory information
- Follows React Native TurboModule pattern (extends `NativeHardwareInfoSpec`)

**File**: `ios/PocketPal/HardwareInfoModule.mm`
- Objective-C++ module that provides `getCPUInfo()`, `getGPUInfo()`
- Does NOT currently provide memory information
- Uses `RCT_EXPORT_METHOD` macros

**File**: `src/specs/NativeHardwareInfo.ts`
- TurboModule spec defining the interface for native modules
- Exports `CPUInfo`, `GPUInfo` interfaces
- Does NOT include memory methods

**File**: `jest/setup.ts` (lines 56-75)
- Mocks `NativeHardwareInfo` globally for tests
- Returns mock data for `getCPUInfo()` and `getGPUInfo()`

**File**: `__mocks__/external/react-native-device-info.js`
- Mocks `DeviceInfo.getTotalMemory()` and `getUsedMemory()` 
- Uses fixtures from `jest/fixtures/device-info.ts`

### Target State

After this change:
1. `NativeHardwareInfo` TurboModule exposes `getAvailableMemory(): Promise<number>` method
2. Android implementation reads `/proc/meminfo` MemAvailable field
3. iOS implementation uses `os_proc_available_memory()` API
4. `useMemoryCheck.ts` uses native available memory with 10% safety margin + fallback to heuristic
5. Tests updated to mock `getAvailableMemory()` 
6. Platform builds succeed (pod install, iOS/Android release builds)

---

## Requirements

### Functional
1. [MUST] Android `HardwareInfoModule.kt` implements `getAvailableMemory()` using `ActivityManager.getMemoryInfo()` (official Android API)
2. [MUST] iOS `HardwareInfoModule.mm` implements `getAvailableMemory()` using `os_proc_available_memory()` API
3. [MUST] `NativeHardwareInfo.ts` TurboModule spec includes `getAvailableMemory(): Promise<number>` method
4. [MUST] `useMemoryCheck.ts` calls native `getAvailableMemory()` instead of calculating heuristic
5. [MUST] Apply 10% safety margin to available memory (return `availableMemory * 0.9`)
6. [MUST] Fallback to current heuristic if native API fails (for robustness)
7. [MUST] Update test mocks in `jest/setup.ts` and `__mocks__/external/react-native-device-info.js`
8. [SHOULD] Log actual vs estimated memory in debug builds for validation

### Non-Functional
- **Performance**: Native call should be fast (<10ms), cached if called frequently
- **Compatibility**: Must work on Android API 21+ and iOS 14+
- **Reliability**: Must not crash if API unavailable (graceful fallback)
- **Testing**: Must maintain 60%+ test coverage

### Platform Verification (NATIVE_CHANGES=YES)
- [ ] `pod install` succeeds
- [ ] iOS Release build succeeds
- [ ] Android Release build succeeds
- [ ] `ios/Podfile.lock` changes committed (if any)

---

## Acceptance Criteria

- [ ] Android uses `ActivityManager.getMemoryInfo()` and returns `availMem` in bytes
- [ ] iOS uses `os_proc_available_memory()` and returns bytes
- [ ] `getAvailableMemory()` returns actual available memory in bytes
- [ ] `useMemoryCheck.ts` applies 10% safety margin
- [ ] If native call fails, falls back to heuristic (no crash)
- [ ] All existing tests pass
- [ ] New tests added for `getAvailableMemory()` 
- [ ] Coverage >= 60%
- [ ] Platform builds succeed (iOS + Android Release)
- [ ] Manual testing on real device shows accurate memory estimation

---

## Phase 2: Track Loaded Model Memory (Enhancement)

### Problem Identified Post-Implementation

The Phase 1 implementation queries current available memory, but this creates an issue when a model is already loaded:

| Scenario | Available Memory | Loaded Model | Check for New 4GB Model |
|----------|------------------|--------------|-------------------------|
| No model | 5GB | - | ✅ Pass (5GB > 4GB needed) |
| 2GB model loaded | 3GB | Using ~2GB | ❌ Fail - but would fit if current released! |

**Issue**: User sees "not enough memory" warning when switching models, even though the new model would fit after releasing the current one.

### Solution: Hybrid Memory Tracking (Approach 4)

1. **Track actual memory delta** when loading a model (before/after measurement)
2. **Fallback to estimate** if delta unavailable (app restart, first check)
3. **Add back loaded model memory** when checking if new model fits

### Phase 2 Requirements

#### Functional
1. [MUST] Track actual memory consumption when model loads (before - after)
2. [MUST] Store `loadedModelMemoryUsage` in ModelStore as observable
3. [MUST] Clear `loadedModelMemoryUsage` when model is released
4. [MUST] In `hasEnoughMemory()`, calculate effective available as: `nativeAvailable + loadedModelMemoryUsage`
5. [MUST] Fallback to `memoryRequirementEstimate()` of current model if actual delta unavailable
6. [SHOULD] Persist memory usage to handle app restart (optional, estimate fallback is acceptable)

#### Non-Functional
- Must not add noticeable latency to model loading
- Memory tracking should work even if interrupted (fallback to estimate)

### Phase 2 Acceptance Criteria

- [ ] ModelStore has `loadedModelMemoryUsage: number | undefined` observable
- [ ] Memory delta measured in `initContext()` (before load - after load)
- [ ] `loadedModelMemoryUsage` cleared in `releaseContext()`
- [ ] `hasEnoughMemory()` adds back current model memory to available
- [ ] Fallback to estimate when `loadedModelMemoryUsage` is undefined
- [ ] User can switch from 2GB model to 4GB model when 5GB total available
- [ ] Tests verify effective available calculation
- [ ] Tests verify fallback behavior

### Phase 2 Affected Files

| File | Action | Reason |
|------|--------|--------|
| `src/store/ModelStore.ts` | MODIFY | Add loadedModelMemoryUsage tracking |
| `src/hooks/useMemoryCheck.ts` | MODIFY | Add effective available calculation |
| `src/hooks/__tests__/useMemoryCheck.test.ts` | MODIFY | Test effective available + fallback |

---

## Affected Files

| File | Action | Reason | Status |
|------|--------|--------|--------|
| `android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt` | MODIFY | Add getAvailableMemory() method | PENDING |
| `ios/PocketPal/HardwareInfoModule.mm` | MODIFY | Add getAvailableMemory() method | PENDING |
| `src/specs/NativeHardwareInfo.ts` | MODIFY | Add getAvailableMemory() to Spec interface | PENDING |
| `src/hooks/useMemoryCheck.ts` | MODIFY | Use native API instead of heuristic | PENDING |
| `src/hooks/__tests__/useMemoryCheck.test.ts` | MODIFY | Update tests to verify native API usage | PENDING |
| `jest/setup.ts` | MODIFY | Add getAvailableMemory() to NativeHardwareInfo mock | PENDING |
| `__mocks__/external/react-native-device-info.js` | MODIFY | (Optional) Add getAvailableMemory() if needed | PENDING |
| `jest/fixtures/device-info.ts` | MODIFY | Add availableMemory fixture data | PENDING |
| `ios/Podfile.lock` | UPDATE | May change after pod install | PENDING |

---

## Implementation Plan

### Step 1: Add getAvailableMemory() to Android Native Module
**Files**: `android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Add import for `android.app.ActivityManager` and `android.content.Context`
- [ ] Add `getAvailableMemory(promise: Promise)` method to `HardwareInfoModule` class
- [ ] Override method from `NativeHardwareInfoSpec` (will be added in Step 3)
- [ ] Use `ActivityManager.getMemoryInfo()` to get available memory (official Android API)
- [ ] Resolve promise with `memInfo.availMem` as Double (already in bytes)
- [ ] Handle errors gracefully (reject promise with error message)

**Why ActivityManager instead of /proc/meminfo:**
- Official Android API - works reliably across all devices and Android versions
- No SELinux permission issues on newer Android or OEM ROMs
- Abstracts away kernel differences (MemAvailable wasn't added until Linux 3.14)
- Provides bonus `lowMemory` flag for free
- Same underlying data: `availMem` equals `MemFree + Cached` from /proc/meminfo

**Pattern Reference**: See `HardwareInfoModule.kt:133-197` (getCPUInfo method) for promise pattern

**Code Guidance**:
```kotlin
// Add imports at top of file
import android.app.ActivityManager
import android.content.Context

// Add method in HardwareInfoModule class
override fun getAvailableMemory(promise: Promise) {
  try {
    val activityManager = reactApplicationContext
      .getSystemService(Context.ACTIVITY_SERVICE) as ActivityManager
    val memInfo = ActivityManager.MemoryInfo()
    activityManager.getMemoryInfo(memInfo)

    // availMem is already in bytes
    promise.resolve(memInfo.availMem.toDouble())
  } catch (e: Exception) {
    promise.reject("ERROR", e.message)
  }
}
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn lint
yarn typecheck
```

### Step 2: Add getAvailableMemory() to iOS Native Module
**Files**: `ios/PocketPal/HardwareInfoModule.mm`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Import `<mach/mach.h>` and `<sys/sysctl.h>` headers at top
- [ ] Add `RCT_EXPORT_METHOD(getAvailableMemory:...)` 
- [ ] Call `os_proc_available_memory()` to get available memory in bytes
- [ ] Resolve promise with bytes as NSNumber
- [ ] Handle errors gracefully (catch exceptions, reject promise)

**Pattern Reference**: See `HardwareInfoModule.mm:12-26` (getCPUInfo method) for promise pattern

**Code Guidance**:
```objective-c
#import <React/RCTBridgeModule.h>
#import <UIKit/UIKit.h>
#import <Metal/Metal.h>
#import <mach/mach.h>
#import <sys/sysctl.h>

// ... existing code ...

RCT_EXPORT_METHOD(getAvailableMemory:(RCTPromiseResolveBlock)resolve
                  rejecter:(RCTPromiseRejectBlock)reject)
{
  @try {
    // Get available memory using os_proc_available_memory()
    uint64_t availableMemory = os_proc_available_memory();
    
    if (availableMemory == 0) {
      reject(@"error_getting_available_memory", @"Could not retrieve available memory", nil);
      return;
    }
    
    resolve(@(availableMemory));
  } @catch (NSException *exception) {
    reject(@"error_getting_available_memory", @"Could not retrieve available memory", nil);
  }
}
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn lint
yarn typecheck
```

### Step 3: Update TurboModule Spec
**Files**: `src/specs/NativeHardwareInfo.ts`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Add `getAvailableMemory(): Promise<number>` to `Spec` interface (line 33-37)
- [ ] Add JSDoc comment explaining return value (bytes)

**Pattern Reference**: See `NativeHardwareInfo.ts:34-36` (getCPUInfo, getGPUInfo methods)

**Code Guidance**:
```typescript
export interface Spec extends TurboModule {
  getCPUInfo(): Promise<CPUInfo>;
  getGPUInfo(): Promise<GPUInfo>;
  getChipset?(): Promise<string>; // Android only
  /**
   * Get available memory in bytes from the operating system.
   * - Android: Reads MemAvailable from /proc/meminfo
   * - iOS: Uses os_proc_available_memory()
   * @returns Promise<number> Available memory in bytes
   */
  getAvailableMemory(): Promise<number>;
}
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn typecheck
```

### Step 4: Update useMemoryCheck to Use Native API
**Files**: `src/hooks/useMemoryCheck.ts`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Import `NativeHardwareInfo` at top: `import NativeHardwareInfo from '../specs/NativeHardwareInfo';`
- [ ] Replace `hasEnoughMemory()` implementation (lines 20-30):
  - Call `NativeHardwareInfo.getAvailableMemory()` to get actual available memory
  - Apply 10% safety margin: `actualAvailable * 0.9`
  - If native call fails, fallback to current heuristic formula
  - Add console.log in debug mode to compare actual vs heuristic
- [ ] Keep `memoryRequirementEstimate()` unchanged (model requirement calculation is correct)

**Pattern Reference**: See `src/utils/deviceCapabilities.ts:85-88` for calling NativeHardwareInfo pattern

**Code Guidance**:
```typescript
export const hasEnoughMemory = async (
  modelSize: number,
  isMultimodal = false,
): Promise<boolean> => {
  let availableMemoryGB: number;

  try {
    // Try native API first (actual available memory from OS)
    const availableBytes = await NativeHardwareInfo.getAvailableMemory();
    // Apply 10% safety margin
    const safeAvailableBytes = availableBytes * 0.9;
    availableMemoryGB = safeAvailableBytes / 1000 / 1000 / 1000;
    
    // Debug logging to validate accuracy
    if (__DEV__) {
      const totalMemory = await DeviceInfo.getTotalMemory();
      const totalMemoryGB = totalMemory / 1000 / 1000 / 1000;
      const heuristicAvailable = Math.min(totalMemoryGB * 0.65, totalMemoryGB - 1.2);
      console.log('[MemoryCheck] Actual available:', availableMemoryGB.toFixed(2), 'GB');
      console.log('[MemoryCheck] Heuristic estimate:', heuristicAvailable.toFixed(2), 'GB');
      console.log('[MemoryCheck] Difference:', ((heuristicAvailable - availableMemoryGB) / availableMemoryGB * 100).toFixed(1), '%');
    }
  } catch (error) {
    // Fallback to heuristic if native API fails
    console.warn('[MemoryCheck] Native API failed, using heuristic:', error);
    const totalMemory = await DeviceInfo.getTotalMemory();
    const totalMemoryGB = totalMemory / 1000 / 1000 / 1000;
    availableMemoryGB = Math.min(totalMemoryGB * 0.65, totalMemoryGB - 1.2);
  }

  const memoryRequirement = memoryRequirementEstimate(modelSize, isMultimodal);
  return memoryRequirement <= availableMemoryGB;
};
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn lint
yarn typecheck
yarn test --findRelatedTests src/hooks/useMemoryCheck.ts
```

### Step 5: Update Test Mocks
**Files**: `jest/setup.ts`, `jest/fixtures/device-info.ts`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] In `jest/setup.ts` (lines 56-75), add `getAvailableMemory` to NativeHardwareInfo mock
- [ ] Mock should return realistic value: ~2.5GB available for 4GB total device
- [ ] In `jest/fixtures/device-info.ts`, add `availableMemory: 2.5 * 1000 ** 3` to fixture
- [ ] Ensure mock is consistent with totalMemory (availableMemory < totalMemory)

**Pattern Reference**: See `jest/setup.ts:60-74` for existing NativeHardwareInfo mock structure

**Code Guidance**:
```typescript
// jest/setup.ts (add to existing mock around line 74)
jest.mock('../src/specs/NativeHardwareInfo', () => ({
  __esModule: true,
  default: {
    getCPUInfo: jest.fn(() => Promise.resolve({cores: 4})),
    getGPUInfo: jest.fn(() =>
      Promise.resolve({
        renderer: 'Mock GPU',
        vendor: 'Mock Vendor',
        version: 'Mock Version',
        hasAdreno: false,
        hasMali: false,
        hasPowerVR: false,
        supportsOpenCL: false,
        gpuType: 'Mock GPU',
      }),
    ),
    getChipset: jest.fn(() => Promise.resolve('Mock Chipset')),
    getAvailableMemory: jest.fn(() => Promise.resolve(2.5 * 1000 * 1000 * 1000)), // 2.5GB
  },
}));
```

```typescript
// jest/fixtures/device-info.ts (add to deviceInfo object)
export const deviceInfo = {
  freeDiskStorage: 8 * 1000 ** 3,
  totalMemory: 4 * 1000 ** 3,
  usedMemory: 2 * 1000 ** 3,
  availableMemory: 2.5 * 1000 ** 3, // More accurate than totalMemory - usedMemory
  version: '1.0.0',
  buildNumber: '1',
};
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn test
```

### Step 6: Update useMemoryCheck Tests
**Files**: `src/hooks/__tests__/useMemoryCheck.test.ts`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Add test: "should use native getAvailableMemory() API"
- [ ] Add test: "should fallback to heuristic if native API fails"
- [ ] Add test: "should apply 10% safety margin to native available memory"
- [ ] Verify existing tests still pass with new implementation

**Pattern Reference**: See `useMemoryCheck.test.ts:58-92` for error handling test pattern

**Code Guidance**:
```typescript
import NativeHardwareInfo from '../../specs/NativeHardwareInfo';

// Add after existing tests
it('should use native getAvailableMemory() API', async () => {
  // Mock native API to return 3GB available
  (NativeHardwareInfo.getAvailableMemory as jest.Mock).mockResolvedValue(
    3 * 1000 * 1000 * 1000
  );

  const {result, waitForNextUpdate} = renderHook(() =>
    useMemoryCheck(localModel.size),
  );

  try {
    await waitForNextUpdate();
  } catch {
    // Ignoring timeout
  }

  // Should pass because model is small and 3GB * 0.9 = 2.7GB available
  expect(result.current).toEqual({
    memoryWarning: '',
    shortMemoryWarning: '',
    multimodalWarning: '',
  });
  
  expect(NativeHardwareInfo.getAvailableMemory).toHaveBeenCalled();
});

it('should fallback to heuristic if native API fails', async () => {
  (NativeHardwareInfo.getAvailableMemory as jest.Mock).mockRejectedValue(
    new Error('Native API error')
  );

  const consoleWarnSpy = jest.spyOn(console, 'warn').mockImplementation(() => {});

  const {result, waitForNextUpdate} = renderHook(() =>
    useMemoryCheck(localModel.size),
  );

  try {
    await waitForNextUpdate();
  } catch {
    // Ignoring timeout
  }

  // Should still work with fallback
  expect(result.current.memoryWarning).toBe('');
  expect(consoleWarnSpy).toHaveBeenCalledWith(
    expect.stringContaining('Native API failed'),
    expect.any(Error)
  );

  consoleWarnSpy.mockRestore();
});

it('should apply 10% safety margin to available memory', async () => {
  // Mock exactly enough memory (3.5GB available)
  // Model needs ~3.2GB (after margin it's 3.15GB available)
  const modelSize = 3.5 * 1000 * 1000 * 1000; // 3.5GB model
  (NativeHardwareInfo.getAvailableMemory as jest.Mock).mockResolvedValue(
    3.5 * 1000 * 1000 * 1000
  );

  const {result, waitForNextUpdate} = renderHook(() =>
    useMemoryCheck(modelSize),
  );

  try {
    await waitForNextUpdate();
  } catch {
    // Ignoring timeout
  }

  // After 10% margin, 3.5GB becomes 3.15GB available
  // Model requirement: 0.43 + (0.92 * 3.5) = 3.65GB
  // Should show warning because 3.65GB > 3.15GB
  expect(result.current.memoryWarning).toBe(l10n.en.memory.warning);
});
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn test src/hooks/__tests__/useMemoryCheck.test.ts
```

### Step 7: Platform Verification (NATIVE_CHANGES=YES)
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Run `cd ios && pod install && cd ..`
- [ ] Commit Podfile.lock changes (if any)
- [ ] Run `yarn ios --configuration Release` (verify iOS build succeeds)
- [ ] Run `yarn android --variant=release` (verify Android build succeeds)
- [ ] Test on real device (optional but recommended for validation)

**Verification**:
```bash
cd "${WORKTREE_PATH}"

# iOS
cd ios && pod install && cd ..
yarn ios --configuration Release

# Android
yarn android --variant=release

# Manual test on device
# 1. Open app on physical device
# 2. Check logs for "[MemoryCheck] Actual available" debug output
# 3. Try loading a model near memory limit
# 4. Verify no OOM crash
```

---

## Test Requirements

### Unit Tests
| Test Case | File | Priority | Status |
|-----------|------|----------|--------|
| Should use native getAvailableMemory() API | `useMemoryCheck.test.ts` | MUST | PENDING |
| Should fallback to heuristic if native API fails | `useMemoryCheck.test.ts` | MUST | PENDING |
| Should apply 10% safety margin | `useMemoryCheck.test.ts` | MUST | PENDING |
| Existing: returns no warning when safe | `useMemoryCheck.test.ts` | MUST | PENDING |
| Existing: returns warning when unsafe | `useMemoryCheck.test.ts` | MUST | PENDING |
| Existing: handles errors gracefully | `useMemoryCheck.test.ts` | MUST | PENDING |

### Integration Tests
| Test Case | File | Priority | Status |
|-----------|------|----------|--------|
| ModelStore.initContext() uses new memory check | Manual | SHOULD | PENDING |

### Manual Testing
- [ ] Test on Android device (Pixel 9 preferred for validation)
- [ ] Verify debug logs show "Actual available" vs "Heuristic estimate"
- [ ] Load model near memory limit - should not crash
- [ ] Compare memory estimation accuracy to heuristic

---

## Coding Standards

### Testing Infrastructure (CRITICAL)
```
# Read these BEFORE writing tests:
${WORKTREE_PATH}/jest/setup.ts      # Global mocks
${WORKTREE_PATH}/jest/test-utils.tsx # Custom render
${WORKTREE_PATH}/__mocks__/stores/  # Mock stores

# DO NOT mock stores inline - they're globally mocked
# Use runInAction() for MobX state changes
# Import render from jest/test-utils, NOT @testing-library/react-native
```

### Patterns to Follow
- **Native Modules**: Follow existing TurboModule pattern in `HardwareInfoModule.kt` and `HardwareInfoModule.mm`
- **Hooks**: Follow existing hook pattern in `useMemoryCheck.ts` (async, error handling)
- **Types**: Strict TypeScript, avoid `any`
- **Error Handling**: Always have try-catch with graceful fallback

### Commit Format (enforced by commitlint)
```
type(scope): subject
```

**Rules**:
- Header max: 100 chars total
- Types allowed: `feat`, `fix`, `docs`, `chore` (only these 4)
- No Co-Authored-By needed
- Keep it short and clear

**Examples**:
```
feat(memory): add native getAvailableMemory() API
fix(memory): fallback to heuristic if native API fails
chore(native): update HardwareInfoModule with memory API
```

### Naming Conventions
- Native methods: camelCase (`getAvailableMemory`)
- TypeScript: camelCase for functions, PascalCase for types/interfaces
- Test files: Match source file name with `.test.ts` suffix

---

## Reference Code

### Pattern Example: Android File Reading
**File**: `android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt`
**Lines**: 133-197
```kotlin
override fun getCPUInfo(promise: Promise) {
  try {
    val cpuInfo = Arguments.createMap()
    cpuInfo.putInt("cores", Runtime.getRuntime().availableProcessors())

    val processors = Arguments.createArray()
    val features = mutableSetOf<String>()
    val cpuInfoFile = File("/proc/cpuinfo")

    if (cpuInfoFile.exists()) {
      val cpuInfoLines = cpuInfoFile.readLines()
      var currentProcessor = Arguments.createMap()
      var hasData = false

      for (line in cpuInfoLines) {
        // ... parse lines ...
      }
    }
    
    promise.resolve(cpuInfo)
  } catch (e: Exception) {
    promise.reject("ERROR", e.message)
  }
}
```

### Pattern Example: iOS Promise Method
**File**: `ios/PocketPal/HardwareInfoModule.mm`
**Lines**: 12-26
```objective-c
RCT_EXPORT_METHOD(getCPUInfo:(RCTPromiseResolveBlock)resolve
                  rejecter:(RCTPromiseRejectBlock)reject)
{
  @try {
    NSUInteger numberOfCPUCores = [[NSProcessInfo processInfo] activeProcessorCount];

    NSDictionary *result = @{
      @"cores": @(numberOfCPUCores)
    };

    resolve(result);
  } @catch (NSException *exception) {
    reject(@"error_getting_cpu_info", @"Could not retrieve CPU info", nil);
  }
}
```

### Pattern Example: Calling Native API
**File**: `src/utils/deviceCapabilities.ts`
**Lines**: 85-88
```typescript
const [gpuInfo, cpuInfo] = await Promise.all([
  NativeHardwareInfo.getGPUInfo(),
  NativeHardwareInfo.getCPUInfo(),
]);
```

---

## Dependencies

### Blocked By
- None

### Blocks
- None (standalone improvement)

---

## Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| `ActivityManager.getMemoryInfo()` unavailable | Very Low | Medium | API available since Android API 1; fallback to heuristic |
| `os_proc_available_memory()` requires iOS 13+ | Low | Low | PocketPal already targets iOS 14+, documented in code |
| Native API returns incorrect value | Low | High | 10% safety margin + debug logging for validation |
| Performance overhead of API call | Very Low | Low | ActivityManager API is fast, only called once per model load |
| Breaking existing tests | Medium | Medium | Update mocks comprehensively in Step 5 |
| iOS build fails after adding new method | Medium | Medium | Follow exact TurboModule codegen pattern, test builds in Step 7 |
| Android build fails with Kotlin compilation error | Medium | Medium | Follow existing method patterns in HardwareInfoModule.kt |

---

## Open Questions

### For Human
- [ ] Should we add caching for `getAvailableMemory()` to reduce native calls? (Current: called once per model load, probably fine)
- [ ] Should we expose this API to users in DeviceInfoCard screen? (Current: internal only)
- [ ] Should we collect telemetry on actual vs heuristic memory to validate improvement? (Analytics)

### Resolved
- [x] **Which Android API to use?** → Use `ActivityManager.getMemoryInfo()` instead of `/proc/meminfo`
  - Official Android API, works reliably across all devices
  - No SELinux permission issues on newer Android or OEM ROMs
  - `availMem` equals `MemFree + Cached` from /proc/meminfo (same data)
  - Sources: [Android Developers](https://developer.android.com/reference/android/app/ActivityManager.MemoryInfo), [Memory Management Guide](https://developer.android.com/topic/performance/memory)

---

## Agent Reports

### Planner Report
```
Research completed: 2026-01-26
Agent: pocketpal-planner

Findings:
1. Current memory estimation in useMemoryCheck.ts uses heuristic formula
2. Native modules exist for CPU/GPU info, but not memory
3. iOS has os_proc_available_memory() API available (iOS 12+, PocketPal targets 14+)
4. TurboModule pattern requires: Spec update -> Native implementation -> JS usage
5. Tests mock NativeHardwareInfo globally in jest/setup.ts
6. ModelStore.initContext() calls hasEnoughMemory() at line 1031

Android API Research (Updated):
- Initially planned to use /proc/meminfo (Linux kernel interface)
- After research, switched to ActivityManager.getMemoryInfo() because:
  * Official Android API - works reliably across all devices
  * No SELinux permission issues on newer Android or OEM ROMs
  * Abstracts kernel differences (MemAvailable added in Linux 3.14)
  * Same data: availMem equals MemFree + Cached from /proc/meminfo
  * Sources: developer.android.com/reference/android/app/ActivityManager.MemoryInfo

Key files researched:
- android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt (198 lines)
- ios/PocketPal/HardwareInfoModule.mm (63 lines)
- src/specs/NativeHardwareInfo.ts (40 lines)
- src/hooks/useMemoryCheck.ts (74 lines)
- jest/setup.ts (mocking patterns)

Implementation approach:
1. Add native methods (Android: ActivityManager, iOS: os_proc_available_memory)
2. Update TurboModule spec
3. Update JS logic with fallback
4. Update test mocks
5. Platform builds verification

Risks: Minimal - fallback to heuristic ensures no regression. Safety margin prevents over-estimation.

Story complexity: Standard (requires native code + tests)
```

### Implementation Report
```
Date: 2026-01-26
Agent: pocketpal-implementer

Environment:
- Task ID: TASK-20260126-1433
- Worktree: /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
- Branch: feature/TASK-20260126-1433

Story: Fix Memory Estimation for Model Loading by Adding Native getAvailableMemory() API

Status: COMPLETE

Changes Made:

| File | Change | Commit |
|------|--------|--------|
| android/app/.../HardwareInfoModule.kt | Added getAvailableMemory() using ActivityManager | a4e3e41 |
| ios/PocketPal/HardwareInfoModule.mm | Added getAvailableMemory() using os_proc_available_memory() | 85f65e4 |
| src/specs/NativeHardwareInfo.ts | Added getAvailableMemory() to Spec interface | 044dd9a |
| src/hooks/useMemoryCheck.ts | Updated to use native API with 10% margin + fallback | 3067109 |
| jest/setup.ts | Added getAvailableMemory mock (3GB) | d249574 |
| jest/fixtures/device-info.ts | Added availableMemory: 3GB | d249574 |
| ios/PocketPal/HardwareInfoModule.mm | Fixed header to use <os/proc.h> | 5016e1b |

Implementation Details:
1. Android: Uses ActivityManager.getMemoryInfo().availMem - official API, no permission issues
2. iOS: Uses os_proc_available_memory() from <os/proc.h> header
3. JavaScript: Applies 10% safety margin, falls back to heuristic on error
4. Debug logging added to compare actual vs heuristic in __DEV__ mode
5. All existing tests pass with updated mocks

Deviations from Plan:
- Android: Used ActivityManager.getMemoryInfo() instead of /proc/meminfo
  Reason: Official API, more reliable, no SELinux permission issues on newer Android
- iOS: Fixed header from <sys/sysctl.h> to <os/proc.h>
  Reason: os_proc_available_memory() requires <os/proc.h>
- Mock memory: Increased from 2.5GB to 3GB
  Reason: After 10% margin (2.7GB), needed to fit 2GB test model (requires 2.27GB)

Verification Results:
- Lint: PASS (only pre-existing warnings in other files)
- TypeCheck: PASS
- Tests: PASS (3/3 tests in useMemoryCheck.test.ts)
- Pod Install: PASS (no Podfile.lock changes)
- iOS Build: PASS (Release configuration, iphonesimulator)
- Android Build: PASS (assembleRelease)

Notes for Tester:
- New tests needed for:
  1. Native API returns correct value
  2. Fallback works when native API fails
  3. 10% safety margin is applied correctly
- Mock is configured at 3GB available memory
- NativeHardwareInfo.getAvailableMemory is mocked in jest/setup.ts
- Test pattern should follow existing useMemoryCheck tests

Blockers: None
```

### Test Report
```
[Filled by tester after tests written]
```

### Phase 2 Implementation Report
```
Date: 2026-01-26
Agent: pocketpal-implementer

Environment:
- Task ID: TASK-20260126-1433
- Worktree: /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
- Branch: feature/TASK-20260126-1433

Story: Phase 2 - Track Loaded Model Memory for Accurate Model Switching

Status: COMPLETE

Changes Made:

| File | Change | Commit |
|------|--------|--------|
| src/store/ModelStore.ts | Added loadedModelMemoryUsage observable | df836c9 |
| src/store/ModelStore.ts | Import NativeHardwareInfo | d03d4f5 |
| src/store/ModelStore.ts | Measure memory before/after model load | d03d4f5 |
| src/store/ModelStore.ts | Clear loadedModelMemoryUsage on release | 65176a4 |
| src/hooks/useMemoryCheck.ts | Import modelStore | ed04082 |
| src/hooks/useMemoryCheck.ts | Calculate effective available memory | ed04082 |
| src/store/ModelStore.ts | Prettier formatting fix | b46f15b |
| __mocks__/stores/modelStore.ts | Add loadedModelMemoryUsage to mock | 1ad99e6 |
| __mocks__/stores/modelStore.ts | Add isMultimodalActive to mock | 1ad99e6 |
| src/hooks/__tests__/useMemoryCheck.test.ts | Add 3 Phase 2 tests | 1ad99e6 |
| src/store/ModelStore.ts | Move memory measurement after initMultimodal | fbeb971 |

Implementation Details:
1. ModelStore now tracks actual memory consumption during model load
2. Measures memory before initLlama() and AFTER initMultimodal()
3. Stores delta in loadedModelMemoryUsage observable (bytes)
4. Only stores positive deltas (sanity check)
5. Clears tracking when context is released (finally block)
6. hasEnoughMemory() now calculates effective available memory:
   - If loadedModelMemoryUsage exists: uses actual measured value
   - Else if activeModel exists: uses estimated memory requirement
   - Else: uses raw available memory (no adjustment)
7. Debug logging added to show measured vs estimated memory

mmproj Fix (commit fbeb971):
- Original measurement was taken right after initLlama(), missing mmproj
- For vision models, mmproj adds ~1.8GB which wasn't captured
- Fixed by moving measurement to AFTER initMultimodal() block
- Now delta captures: main model + mmproj (if multimodal enabled)
- Updated debug log: "Memory consumed (including mmproj if multimodal)"

Deviations from Plan:
- Added post-implementation fix for mmproj memory tracking (fbeb971)

Verification Results:
- Lint: PASS (only pre-existing warnings)
- TypeCheck: PASS
- Tests: PASS (all 1361 tests, 9/9 in useMemoryCheck.test.ts)
- Coverage: Tests cover all 3 scenarios (actual, estimate, no model)
- No platform builds needed (JS-only changes)

Notes for Reviewer:
- Phase 2 is pure JavaScript - no native changes
- All new code paths are tested
- Memory tracking is defensive (undefined fallback)
- Debug logs will help validate accuracy in production
- Effective available calculation enables better model switching UX
  (Users won't see "not enough memory" when switching models)

Blockers: None
```

### Review Report
```
[Filled by reviewer after review]
```

---

## Phase 3: Improved Memory Estimation (Research & Design)

### Problem Observed Post-Phase-2

Testing revealed that Android's `availMem` (and `/proc/meminfo` `MemAvailable`) are **inherently volatile**:

```
# Measurements taken within minutes on same device:
MemAvailable: 2824356 kB (~2.69 GB)
MemAvailable: 3286816 kB (~3.13 GB)
MemAvailable: 3054040 kB (~2.91 GB)
```

**Key observation**: When memory pressure is applied (model loaded), Android makes MORE memory available than the API reported beforehand:
- Passive check (no pressure): ~2.83 GB available
- After release, before new load: 3.02 → 3.28 → 3.29 GB (converging to actual ceiling)

### Industry Research Findings

**This is an industry-wide problem. No one has solved accurate "available memory" detection on Android.**

| Project | Approach | Status |
|---------|----------|--------|
| **llama.cpp** | No pre-load estimation | [Issue #4315](https://github.com/ggml-org/llama.cpp/issues/4315) closed due to inactivity |
| **MLC LLM** | Compile-time static estimation | [Issue #2748](https://github.com/mlc-ai/mlc-llm/issues/2748) - can't measure runtime accurately |
| **Ollama** | Pre-calculated estimates | [Issue #10359](https://github.com/ollama/ollama/issues/10359) - overestimates by 2.2x |
| **Google Memory Advice API** | ML-based multi-signal | **Deprecated** - not reliable enough |
| **TensorFlow Lite** | Profiling tools only | No pre-load estimation |

**Common workarounds:**
1. Static model-size estimation (ignore volatile runtime APIs)
2. Device tiering (% of total RAM by device class)
3. Warn but allow override + handle failure gracefully
4. User choice / manual configuration

---

## FINAL DESIGN (Clean Implementation)

> **Status**: This is the definitive design after multiple iterations. Previous sections contain research and deliberation history.

### Problem Statement

We need to determine if a device can load a given model. Two questions:
1. **How much memory does the model need?** → Model Memory Estimation
2. **How much memory can the device provide?** → Available Memory Calibration

### Core Principles

1. **One estimation function** - Single source of truth for model memory requirements
2. **Two calibration variables** - Simple, no complex branching
3. **Consistency** - Same formula used for checking AND tracking
4. **Reliability** - Only use `getAvailableMemory()` at clean states (no model loaded)

### Part 1: Model Memory Estimation

**One function** that handles everything:

```
getModelMemoryRequirement(model, projectionModel?, contextSettings?) → bytes

  IF model has GGUF metadata AND contextSettings available:
    Use accurate formula: (Weights + KV Cache + Compute Buffer) × 1.1
  ELSE:
    Use fallback: (modelSize + mmProjSize) × 1.1
```

**Key points:**
- Lives in `memoryEstimator.ts`
- Takes Model object directly (not scattered parameters)
- Handles multimodal (mmproj) internally
- Returns bytes (consistent unit)
- Used everywhere: memory checks, calibration tracking

### Part 2: Available Memory Calibration

**Two variables** (persisted):

| Variable | Purpose | When Updated |
|----------|---------|--------------|
| `availableMemoryCeiling` | OS-reported ceiling | App startup (if undefined), after model release |
| `largestSuccessfulLoad` | Ground truth of what worked | After successful model load |

**Memory check logic:**
```
ceiling = max(largestSuccessfulLoad, availableMemoryCeiling)
safeCeiling = ceiling × 0.9   // 10% safety margin
PASS if modelRequirement <= safeCeiling
```

**Calibration lifecycle:**
```
APP STARTUP (before any model loads):
  if availableMemoryCeiling is undefined:
    availableMemoryCeiling = getAvailableMemory()

SUCCESSFUL LOAD:
  estimated = getModelMemoryRequirement(model, projectionModel, contextSettings)
  largestSuccessfulLoad = max(largestSuccessfulLoad, estimated)

MODEL RELEASE:
  availableMemoryCeiling = max(availableMemoryCeiling, getAvailableMemory())
```

### Part 3: GGUF-Based Estimation Formula

When GGUF metadata is available:

```
Total = (Weights + KV Cache + Compute Buffer) × 1.1

Weights = file_size (already quantized in GGUF)

KV Cache = n_layers × effective_ctx × (head_k + head_v) × n_head_kv × bytes_per_element
  - effective_ctx = min(n_ctx, sliding_window) for SWA layers
  - bytes_per_element: f16=2.0, q8_0≈1.06, q4_0≈0.56

Compute Buffer = (n_vocab + n_embd) × n_ubatch × 4

Multimodal: Add mmProjSize × 1.1
```

### Why This Design

1. **Single function** → No scattered/duplicated logic
2. **Two variables** → Simple state, no complex "cold start" vs "calibrated" branching
3. **Estimator for tracking** → Don't use unreliable memory delta measurements
4. **getAvailableMemory() only at clean states** → More reliable readings

---

## Historical Research (Reference)

> The sections below contain research and deliberation history that led to the final design.

### Previous Approaches (Superseded)
  const totalModelSize = modelSize + (mmProjSize || 0);
  return totalModelSize * 1.1;  // 10% overhead for compute buffers
}
```

**Why simplified:**
- Uses actual `projectionModel.size` instead of hardcoded 1.8GB
- Single multiplier (1.1x) is defensible and minimal
- Calibration system compensates for any estimation errors
- Complex KV cache formulas can be added later if empirically validated

### mmproj Size Ranges (Reference)

| Model Family | mmproj Size Range |
|--------------|-------------------|
| SmolVLM | ~104 MB |
| LLaVA | 177 MB - 700 MB |
| Qwen2-VL/Qwen2.5-VL | 500 MB - 2.7 GB |

**Conclusion**: Must use `projectionModel.size` instead of hardcoded 1.8GB.

### Industry Formulas (Reference Only)

These formulas are preserved for future reference if we need more precise estimation:

<details>
<summary>KV Cache Formulas (not implemented - needs validation)</summary>

**KolosalAI Simple:**
```
KV Cache = bytes_per_value × hidden_size × n_layers × context_tokens
```

**Theoretical:**
```
KV Cache = 2 × n_layers × n_kv_heads × head_dim × n_tokens × bytes_per_element
```

**Oobabooga VRAM (365 MiB median error):**
```
vram = (size_per_layer - 17.99 + 3.15e-05 * kv_cache_factor)
       * (gpu_layers + ...)
```

</details>

### Phase 3 Implementation Steps

| Step | Description | Priority |
|------|-------------|----------|
| 1 | Add `CalibrationData` interface and AsyncStorage persistence | HIGH |
| 2 | Update `largestSuccessfulLoad` on successful model load | HIGH |
| 3 | Measure and track `maxAvailMemAfterRelease` on model release | HIGH |
| 4 | Update `hasEnoughMemory()` to use calibrated ceiling | HIGH |
| 5 | Replace hardcoded 1.8GB with `projectionModel.size` in estimation | HIGH |
| 6 | Update cold start logic: `max(totalRAM × 0.5, currentAvailMem)` | MEDIUM |

---

## Phase 4: GGUF-Based Memory Estimation (Planned)

### Problem

Phase 3 uses `totalModelSize × 1.1` which doesn't account for:
1. **Context length**: KV cache scales linearly with `n_ctx`
2. **KV cache quantization**: `cache_type_k`/`cache_type_v` (q8_0, q4_0, f16) significantly affects memory
3. **Architecture variations**: SWA models (Gemma), hybrid models (LFM), GQA (different n_head_kv)

Testing with `temp/memory_estimator.py` shows that it accurately estimates memory for most of the models and with 10% overhead on top of the formula, we can almost always cover the rest.

### Proposed Solution

Use GGUF metadata from `loadLlamaModelInfo()` combined with user's context settings.

#### Memory Formula

```
Total Memory = (Weights + KV Cache + Compute Buffer) × 1.1

Where:
  Weights = file_size (already quantized)

  KV Cache = n_layers × n_ctx × (n_embd_head_k + n_embd_head_v) × n_head_kv × bytes_per_element
    - bytes_per_element depends on cache_type_k/cache_type_v:
      f16: 2.0, q8_0: 1.0625, q4_0: 0.5625, etc.
    - SWA models: SWA layers use min(n_ctx, sliding_window)
    - Hybrid models: only attention layers have KV cache

  Compute Buffer = (n_vocab + n_embd) × n_ubatch × 4

Note: Output Buffer (n_vocab × 4) dropped - contributes <1% to total.
```

#### Data Model

Add `ggufMetadata` field to Model type:

```typescript
interface GGUFMetadata {
  architecture: string;
  n_layers: number;
  n_embd: number;
  n_head: number;
  n_head_kv: number;
  n_vocab: number;
  n_embd_head_k?: number;  // Key head dimension
  n_embd_head_v?: number;  // Value head dimension
  sliding_window?: number; // For Gemma SWA models
}
```

#### Data Flow

```
DOWNLOADED MODELS:
  Download completes → loadLlamaModelInfo() → Parse & persist in Model.ggufMetadata
  Memory check → Use ggufMetadata + n_ctx + cache_type_k/v → Accurate estimate

HF BROWSING (not downloaded):
  Model card → Fallback to file_size × 1.1 (acceptable pre-download)
```

#### Integration Points

1. **After download completes**: Call `loadLlamaModelInfo()`, parse relevant fields, persist
2. **`memoryRequirementEstimate()`**: Use GGUF metadata if available, else fallback
3. **Context from settings**: Read `n_ctx`, `cache_type_k`, `cache_type_v` from `contextInitParams`

### Fallback Chain

```
1. ggufMetadata available → Full formula with n_ctx + kv_type
2. Downloaded but no metadata → Read once, persist, then use
3. HF browsing → file_size × 1.1 (Phase 3 formula)
4. All else fails → file_size × 1.1
```

### Phase 4 Implementation Steps

| Step | Description | Priority |
|------|-------------|----------|
| 1 | Create `src/utils/memoryEstimator.ts` with core calculation functions | HIGH |
| 2 | Add `GGUFMetadata` interface and add to Model type | HIGH |
| 3 | Parse and persist GGUF metadata after download completes | HIGH |
| 4 | Update `memoryRequirementEstimate()` to use GGUF-based calculation | HIGH |
| 5 | Pass `n_ctx`, `cache_type_k`, `cache_type_v` from context settings | MEDIUM |
| 6 | Handle SWA models (Gemma) with sliding window | LOW |
| 7 | Handle hybrid models (LFM) with per-layer KV heads | LOW |

### KV Cache Type Bytes Reference

| Type | Bytes per element |
|------|-------------------|
| f32 | 4.0 |
| f16 | 2.0 |
| bf16 | 2.0 |
| q8_0 | 1.0625 (34/32) |
| q4_0 | 0.5625 (18/32) |
| q4_1 | 0.625 (20/32) |
| q5_0 | 0.6875 (22/32) |
| q5_1 | 0.75 (24/32) |

### Sources

- [llama.cpp Memory Estimation Issue #4315](https://github.com/ggml-org/llama.cpp/issues/4315)
- [MLC LLM Memory Measurement Issue #2748](https://github.com/mlc-ai/mlc-llm/issues/2748)
- [Ollama Memory Allocation Issue #10359](https://github.com/ollama/ollama/issues/10359)
- [Oobabooga VRAM Formula](https://oobabooga.github.io/blog/posts/gguf-vram-formula/)
- [KolosalAI Memory Calculator](https://github.com/KolosalAI/model-memory-calculator)
- [llama.cpp Memory Discussion #9936](https://github.com/ggml-org/llama.cpp/discussions/9936)
- [Android Memory Management](https://developer.android.com/topic/performance/memory-management)

---

## Identified Issues (Post-Implementation Review)

### Issue: Vision Model Memory Underestimation in UI Components

**Severity**: HIGH
**Status**: IDENTIFIED (not yet fixed)
**Identified By**: pocketpal-reviewer (structured flow analysis)
**Date**: 2026-01-30

#### Problem Description

UI components (`ModelCard`, `ModelFileCard`) call `useMemoryCheck(model)` **without** passing the projection model, while `ModelStore.checkMemoryAndConfirm()` correctly passes `hasEnoughMemory(model, projectionModel)` **with** the projection model.

This creates inconsistent memory warnings between the UI and actual loading:

| Location | projectionModel Passed | Vision Memory Accurate |
|----------|------------------------|------------------------|
| **ModelCard.tsx** | ❌ NO | ❌ Underestimated |
| **ModelFileCard.tsx** | ❌ NO | ❌ Underestimated |
| **ModelStore.ts** | ✅ YES | ✅ Correct |

#### User Impact

```
Example User Journey:
1. User sees ModelCard for vision model (4 GB LLM + 150 MB mmproj)
2. Memory warning calculated: ~4.8 GB (LLM only × 1.2) → NO WARNING
3. User clicks "Load"
4. ModelStore checks: ~5.0 GB (LLM + mmproj × 1.2) → WARNING or FAIL
```

Users get different memory warnings in UI vs at load time, causing confusion and failed loads.

#### Root Cause

The `useMemoryCheck` hook accepts an **optional** `projectionModel` parameter but:
1. **ModelCard** doesn't resolve or pass it (line 91-92)
2. **ModelFileCard** doesn't resolve or pass it (line 94-95)
3. Only **ModelStore** resolves and passes it correctly (line 1132)

#### Recommended Fix

Update both UI components to resolve and pass the projection model:

**ModelCard.tsx:**
```typescript
// Add before useMemoryCheck call
const projectionModelForCheck = useMemo(() => {
  if (
    model.supportsMultimodal &&
    modelStore.getModelVisionPreference(model) &&
    model.defaultProjectionModel
  ) {
    return modelStore.models.find(m => m.id === model.defaultProjectionModel);
  }
  return undefined;
}, [model]);

const {memoryWarning, shortMemoryWarning, multimodalWarning} =
  useMemoryCheck(model, projectionModelForCheck);
```

**ModelFileCard.tsx:**
```typescript
// Add before useMemoryCheck call
const projectionModelForCheck = useMemo(() => {
  if (
    convertedModel.supportsMultimodal &&
    convertedModel.defaultProjectionModel
  ) {
    return modelStore.models.find(
      m => m.id === convertedModel.defaultProjectionModel
    );
  }
  return undefined;
}, [convertedModel]);

const {shortMemoryWarning, multimodalWarning} =
  useMemoryCheck(convertedModel, projectionModelForCheck);
```

#### Additional Observations

1. **ModelFileCard Size Display Inconsistency**: The component correctly displays total download size (LLM + mmproj) via `getVisionModelSizeBreakdown()`, but memory warning is calculated WITHOUT mmproj. User sees "4.5 GB" download size but memory check is based on ~3.8 GB.

2. **Test Coverage Gap**: Tests don't cover vision model memory estimation with projection model.

---

### Issue 2: GGUF Metadata Values Are Strings (Compute Buffer = 2626 GB!)

**Severity**: HIGH (CRITICAL BUG)
**Status**: IDENTIFIED (not yet fixed)
**Identified By**: Testing + pocketpal-reviewer analysis
**Date**: 2026-01-30

#### Problem Description

The GGUF metadata values extracted from `loadLlamaModelInfo()` are **strings**, not numbers. This causes the compute buffer calculation to perform string concatenation instead of numeric addition:

```
n_vocab = "128256" (string)
n_embd = "3072" (string)

Formula: (n_vocab + n_embd) * n_ubatch * 4
Actual:  ("128256" + "3072") * 512 * 4
       = "1282563072" * 512 * 4
       = 2626692915200 bytes = 2626.69 GB ← WRONG!
```

#### Root Cause

In `ModelStore.ts` (lines 1038-1048), metadata is assigned without type conversion:
```typescript
const metadata = {
  n_layers: (modelInfo as any)['llama.block_count'],     // STRING!
  n_embd: (modelInfo as any)['llama.embedding_length'],  // STRING!
  n_vocab: (modelInfo as any)['llama.vocab_size'],       // STRING!
  // ...
};
```

#### Recommended Fix

Add `Number()` conversions:
```typescript
const metadata: GGUFMetadata = {
  architecture: (modelInfo as any)['general.architecture'],
  n_layers: Number((modelInfo as any)['llama.block_count']),
  n_embd: Number((modelInfo as any)['llama.embedding_length']),
  n_head: Number((modelInfo as any)['llama.attention.head_count']),
  n_head_kv: Number((modelInfo as any)['llama.attention.head_count_kv']),
  n_vocab: Number((modelInfo as any)['llama.vocab_size']),
  n_embd_head_k: Number((modelInfo as any)['llama.attention.key_length']),
  n_embd_head_v: Number((modelInfo as any)['llama.attention.value_length']),
  sliding_window: (modelInfo as any)['llama.attention.sliding_window']
    ? Number((modelInfo as any)['llama.attention.sliding_window'])
    : undefined,
};
```

---

### Issue 3: GGUF Metadata Not Persisted / Fetched Every Startup

**Severity**: HIGH (Performance)
**Status**: IDENTIFIED (not yet fixed)
**Identified By**: Testing + pocketpal-reviewer analysis
**Date**: 2026-01-30

#### Problem Description

Every app startup shows `loadLlamaModelInfo` calls for ALL downloaded models:
```
loadLlamaModelInfo time: 367  LFM2.5-VL-1.6B-Q4_0.
loadLlamaModelInfo time: 379  SmolLM2-135M-Instruct-Q2_K.
... (all downloaded models)
```

This indicates GGUF metadata is NOT being persisted or restored correctly.

#### Root Cause

1. `initializeStore()` does NOT load GGUF metadata for already-downloaded models
2. Metadata is only fetched on-demand via `hasEnoughMemory` → triggers for ALL models
3. Possible issue: `mergeModelLists()` may not preserve `ggufMetadata` when merging

#### Recommended Fix

Add background metadata loading in `initializeStore()`:
```typescript
// After existing initialization
this.loadMissingGGUFMetadata();

private loadMissingGGUFMetadata = () => {
  const modelsNeedingMetadata = this.models.filter(
    m => m.isDownloaded && !m.ggufMetadata
  );

  // Fetch in background, don't block startup
  (async () => {
    for (const model of modelsNeedingMetadata) {
      await this.fetchAndPersistGGUFMetadata(model);
    }
  })();
};
```

Also verify `mergeModelLists()` preserves `ggufMetadata` from persisted models.

---

### Issue 4: Memory Warning Doesn't Update After Model Release

**Severity**: MEDIUM (UX)
**Status**: IDENTIFIED (not yet fixed)
**Identified By**: Testing + pocketpal-reviewer analysis
**Date**: 2026-01-30

#### Problem Description

The memory warning chip in ModelCard doesn't update after `availableMemoryCeiling` changes (e.g., after releasing a model).

#### Root Cause

`useMemoryCheck` hook has incomplete dependencies:
```typescript
useEffect(() => {
  // ... memory check
}, [model, projectionModel, l10n]);  // Missing MobX observables!
```

Missing dependencies:
- `modelStore.availableMemoryCeiling`
- `modelStore.largestSuccessfulLoad`

#### Recommended Fix

Add MobX observables to dependencies:
```typescript
useEffect(() => {
  // ... memory check
}, [
  model,
  projectionModel,
  l10n,
  modelStore.availableMemoryCeiling,    // Add
  modelStore.largestSuccessfulLoad,     // Add
]);
```

---

### Issue 5: Cold Start Path Inefficiency (Suggestion)

**Severity**: LOW (Optimization)
**Status**: IDENTIFIED (suggestion)
**Identified By**: Code review suggestion
**Date**: 2026-01-30

#### Problem Description

If `NativeHardwareInfo.getAvailableMemory()` fails during initialization, `availableMemoryCeiling` remains `undefined`, forcing the cold start fallback in `hasEnoughMemory()` for EVERY memory check.

#### Suggested Improvement

Always set `availableMemoryCeiling` during initialization with a fallback:
```typescript
if (this.availableMemoryCeiling === undefined) {
  try {
    const availableBytes = await NativeHardwareInfo.getAvailableMemory();
    this.availableMemoryCeiling = availableBytes;
  } catch (error) {
    // Fallback when native call fails
    const totalMemory = await DeviceInfo.getTotalMemory();
    this.availableMemoryCeiling = totalMemory * 0.5;
  }
}
```

This would eliminate the cold start path in `hasEnoughMemory()` entirely.

---

### Issue 6: loadMissingGGUFMetadata Processes mmproj Files

**Severity**: MEDIUM (Unnecessary processing + log spam)
**Status**: IDENTIFIED (not yet fixed)
**Identified By**: Testing + code review
**Date**: 2026-01-30

#### Problem

The `loadMissingGGUFMetadata()` function iterates over all models in `modelStore.models` that are downloaded but lack GGUF metadata. However, mmproj (multimodal projection) files are stored as separate Model entries and are included in this iteration.

mmproj files are CLIP models with a completely different metadata structure:
- Architecture: `clip` instead of `llama`, `qwen2`, etc.
- Different field names and semantics
- Not used for memory estimation (their size is added separately via `projectionModel?.size`)

#### Current Behavior

```
[ModelStore] Loading GGUF metadata for 18 models in background
[ModelStore] Incomplete GGUF metadata, skipping {architecture: 'clip', n_layers: NaN, ...}
[ModelStore] Incomplete GGUF metadata, skipping {architecture: 'clip', n_layers: NaN, ...}
```

This causes:
1. **Unnecessary processing** - Trying to parse CLIP metadata we don't need
2. **Log spam** - "Incomplete GGUF metadata, skipping" warnings for every mmproj
3. **Confusion** - Makes it look like metadata parsing is failing

#### Root Cause

mmproj models are stored in `modelStore.models` array alongside LLM models. The `loadMissingGGUFMetadata()` function doesn't filter them out.

#### Detection

mmproj files can be identified by:
1. **Filename pattern**: Contains `mmproj` in the filename (e.g., `mmproj-model-f16.gguf`)
2. **Model property**: The parent LLM model references them via `defaultProjectionModel` ID
3. **Architecture**: GGUF metadata shows `architecture: 'clip'`

#### Recommended Fix

Filter out mmproj files in `loadMissingGGUFMetadata()`:

```typescript
private loadMissingGGUFMetadata = () => {
  const modelsNeedingMetadata = this.models.filter(
    m => m.isDownloaded &&
         !m.ggufMetadata &&
         !isProjectionModel(m.filename || m.name)  // <-- Add this filter
  );
  // ...
};
```

The `isProjectionModel()` utility already exists in `src/utils/modelUtils.ts`.

#### Files to Change

1. `src/store/ModelStore.ts` - Add filter in `loadMissingGGUFMetadata()`

#### Alternative Approach

Could also skip at the parsing level by checking architecture after reading:

```typescript
// In fetchAndPersistGGUFMetadata
const architecture = (modelInfo as any)['general.architecture'];
if (architecture === 'clip') {
  // mmproj file, skip memory metadata extraction
  return;
}
```

This is less efficient (still reads the file) but more robust.

---

### Issue 7: Persisted GGUF Metadata Has String Values (Migration Needed)

**Severity**: HIGH (Causes 100+ GB compute buffer estimates)
**Status**: IDENTIFIED (not yet fixed)
**Identified By**: Testing + code review
**Date**: 2026-01-30

#### Problem

The previous buggy code stored GGUF metadata values as strings instead of numbers:

```javascript
// Persisted in AsyncStorage (WRONG - strings):
{
  architecture: "llama",
  n_embd: "576",      // Should be 576 (number)
  n_vocab: "49280",   // Should be 49280 (number)
  // ...
}
```

The new `fetchAndPersistGGUFMetadata()` code correctly converts to numbers, but this only runs when fetching **new** metadata. Existing models loaded from AsyncStorage still have string values.

#### Symptoms

```
[MemoryEstimator] Using GGUF-based calculation: SmolVLM-256M-Instruct-Q8_0
  Compute Buffer: 100.93 GB   <-- WRONG! Should be ~0.26 GB

metadata: {n_embd: "576", n_vocab: "49280", ...}  <-- Strings not numbers!
```

The compute buffer formula `(n_vocab + n_embd) * n_ubatch * 4` becomes string concatenation:
- `"49280" + "576"` = `"49280576"` (string concat, not addition!)
- Then `"49280576" * 512 * 4` coerces to number = 100.93 GB

#### Root Cause

1. Old code didn't convert GGUF values to numbers before persisting
2. AsyncStorage preserves the string types
3. `loadMissingGGUFMetadata()` skips models that already have `ggufMetadata`
4. `memoryEstimator.ts` doesn't defensively convert strings to numbers

#### Recommended Fixes

**Fix A: Clear metadata in `resetModels()` (User-triggered migration)**

```typescript
resetModels = () => {
  // ... existing code ...

  // Clear GGUF metadata to force re-fetch with correct types
  this.models.forEach(model => {
    if (model.ggufMetadata) {
      model.ggufMetadata = undefined;
    }
  });

  // Re-fetch metadata in background
  this.loadMissingGGUFMetadata();
};
```

**Fix B: Defensive number conversion in `memoryEstimator.ts`**

```typescript
function calculateComputeBuffer(metadata: GGUFMetadata, ...): number {
  // Defensive: ensure numbers even if metadata has strings
  const n_vocab = Number(metadata.n_vocab);
  const n_embd = Number(metadata.n_embd);
  const {n_ubatch} = contextSettings;

  return (n_vocab + n_embd) * n_ubatch * 4;
}
```

**Fix C: Auto-migration in `initializeStore()` (Transparent to user)**

```typescript
// After loading models from storage, check for string metadata
this.models.forEach(model => {
  if (model.ggufMetadata && typeof model.ggufMetadata.n_embd === 'string') {
    // Old format with strings, clear to force re-fetch
    model.ggufMetadata = undefined;
  }
});
this.loadMissingGGUFMetadata();
```

#### Recommended Approach

Implement all three fixes for defense in depth:
1. **Fix B** - Immediate safety net, prevents any string-related bugs
2. **Fix C** - Auto-migrates users transparently on next app launch
3. **Fix A** - Gives users manual reset option if needed

#### Files to Change

1. `src/utils/memoryEstimator.ts` - Fix B: Defensive number conversion
2. `src/store/ModelStore.ts` - Fix A: Clear metadata in resetModels()
3. `src/store/ModelStore.ts` - Fix C: Auto-migration in initializeStore()

---

### Future Enhancement: Display Memory Values to Users

**Severity**: LOW (UX Enhancement)
**Status**: DESIGNED (ready for implementation)
**Identified By**: Human review + brainstorming
**Date**: 2026-01-30

#### Overview

Display memory requirements on model cards to help users make informed decisions before downloading or loading models.

#### Design: Required RAM Display

**Location**: Under the model name on ModelCard and ModelFileCard

**Format**: `Required RAM: ~X.X GB (status)`

**Status Indicators**:

| Status | Icon | Color | Condition | Meaning |
|--------|------|-------|-----------|---------|
| fits | ✓ | Green | requirement ≤ available ceiling | Confident it will work |
| tight | ⚠️ | Orange | requirement > available BUT < total RAM | Might work, worth trying |
| won't fit | ✗ | Red | requirement > total RAM | Physically impossible |

**Logic Explanation**:

- **available ceiling** = `max(largestSuccessfulLoad, availableMemoryCeiling)` - the learned safe zone
- **total RAM** = device's total physical memory
- The 10% safety margin is already included in the memory requirement estimate

**Example on 8GB device with 3.2GB calibrated ceiling**:

```
Model A (2.5 GB required): Required RAM: ~2.5 GB (fits ✓)
Model B (5.0 GB required): Required RAM: ~5.0 GB (tight ⚠️)
Model C (9.0 GB required): Required RAM: ~9.0 GB (won't fit ✗)
```

**Why "tight" is valuable**: It tells the user "our learned limit says this might not work, but your device *could* have more headroom. You can try - if it works, we'll learn a higher ceiling."

#### Visual Mock-up

**ModelFileCard (HF Search)**:
```
┌──────────────────────────────────────────┐
│ Llama-3.2-1B-Instruct-Q4_K_M.gguf        │
│ Required RAM: ~1.8 GB (fits ✓)           │  ← New line
│ 1.5 GB                          [📥] [🔖] │
│ [⚠️ Storage] [🔒 Gated]                   │
└──────────────────────────────────────────┘
```

**ModelCard**:
```
┌─────────────────────────────────────────┐
│ 🦙 Llama 3.2 1B Instruct         1.5 G  │
│ Required RAM: ~1.8 GB (fits ✓)          │  ← New line
│                                         │
│             [Load/Download] [Settings]  │
└─────────────────────────────────────────┘
```

**Tight/Won't Fit Examples**:
```
Required RAM: ~5.0 GB (tight ⚠️)      # Orange text
Required RAM: ~9.0 GB (won't fit ✗)   # Red text
```

#### Device Available Memory Display

**Location**: App bar on Models screen (between title and menu icon)

**Design**: Visual progress bar with text embedded inside

```
┌──────────────────────────────────────────────────────────┐
│ Models  [████ ~3.2/8 GB usable ████░░░░░░░░]  ≡         │
├──────────────────────────────────────────────────────────┤
│ Model cards...                                           │
└──────────────────────────────────────────────────────────┘
```

**Visual breakdown**:
- `████` (filled) = Estimated usable memory (learned ceiling)
- `░░░░` (empty) = System reserved / not available for models
- Text inside bar = `~X.X/Y GB usable`
- Always use `~` prefix since this is an estimate

**Colors**:
- Filled portion: Theme primary color or subtle green
- Empty portion: Muted gray
- Text: White or high-contrast color for readability inside bar

**Interaction - Tap for tooltip**:
```
"Estimated usable memory. As you use the app,
we keep learning what works for your device."
```

**Edge cases**:
- Very low memory: Text may need to overflow outside bar or use smaller font
- Cold start: Already handled by `availableMemoryCeiling` initialization which uses fallback `min(60%, total - 1.2GB)` - no special UI handling needed
- Always show `~` to indicate this is an estimate

**Why this design works**:
- Always visible while browsing models (no need to go to Settings)
- Visual proportion is intuitive at a glance
- Exact numbers for users who want precision
- "usable" is user-friendly language (not technical jargon)
- Tooltip explains adaptive behavior without cluttering UI
- Compact - doesn't take extra vertical space

#### Implementation Notes

**Files to modify**:

1. `src/screens/ModelsScreen/ModelCard/ModelCard.tsx`
   - Add RAM requirement line under model name
   - Use `getModelMemoryRequirement()` for estimate
   - Compare against ceiling and total RAM for status

2. `src/screens/ModelsScreen/HFModelSearch/DetailsView/ModelFileCard/ModelFileCard.tsx`
   - Same pattern as ModelCard

3. `src/screens/ModelsScreen/ModelsScreen.tsx` (or ModelsHeader component)
   - Add memory bar to app bar / header area
   - Show `~X.X/Y GB usable` with progress visualization
   - Add tap handler for tooltip

**Helper function** (new utility):

```typescript
type MemoryFitStatus = 'fits' | 'tight' | 'wont_fit';

function getMemoryFitStatus(
  requiredMemory: number,
  availableCeiling: number,
  totalMemory: number,
): MemoryFitStatus {
  if (requiredMemory <= availableCeiling) {
    return 'fits';
  } else if (requiredMemory <= totalMemory) {
    return 'tight';
  } else {
    return 'wont_fit';
  }
}
```

**L10n strings needed**:

```typescript
memory: {
  // Per-model requirement
  required: 'Required RAM: ~{size}',
  fits: 'fits',
  tight: 'tight',
  wontFit: "won't fit",

  // Device available memory bar
  usable: '~{available}/{total} GB usable',
  usableTooltip: 'Estimated usable memory. As you use the app, we keep learning what works for your device.',
}
```

#### Benefits

- **Transparency**: Users understand why warnings appear
- **Informed decisions**: Users can choose models that fit their device
- **Actionable**: "tight" encourages experimentation, "won't fit" prevents wasted time
- **Trust**: Shows the app is doing smart memory management
- **Learning feedback**: Successful "tight" loads improve the ceiling

---

## Changelog

| Date | Agent/Human | Change |
|------|-------------|--------|
| 2026-01-26 | orchestrator | Created worktree and task |
| 2026-01-26 | planner | Initial story draft with detailed research |
| 2026-01-27 | human | Added Phase 3 research: volatile memory problem & improved estimation design |
| 2026-01-27 | deliberation | AI deliberation (proposer/challenger) converged on calibration approach |
| 2026-01-27 | human | Refined solution: dual signals (largestSuccessfulLoad + maxAvailMemAfterRelease), simplified model estimation (totalModelSize × 1.1) |
| 2026-01-29 | implementer | Implemented Phase 3: calibration tracking, updated hasEnoughMemory with cold start logic |
| 2026-01-29 | human | Added Phase 4 plan: GGUF-based memory estimation using model metadata + context settings |
| 2026-01-29 | implementer | Implemented Phase 4: GGUF-based estimation, memoryEstimator.ts, GGUFMetadata in Model type |
| 2026-01-29 | human | Major refactor: simplified to 2 variables (largestSuccessfulLoad + availableMemoryCeiling), removed loadedModelMemoryUsage |
| 2026-01-29 | human | Final fix: use GGUF estimator for largestSuccessfulLoad instead of unreliable memory delta measurement |
| 2026-01-29 | human | **RESTART**: Code became fragmented. Documented clean design, reverting to implement from scratch |
| 2026-01-30 | implementer | Clean implementation: single estimation function, removed double safety margin, on-demand GGUF fetch |
| 2026-01-30 | reviewer | Comprehensive flow analysis: all memory paths verified CORRECT |
| 2026-01-30 | reviewer | **ISSUE FOUND**: Vision models underestimate memory in UI (ModelCard/ModelFileCard don't pass projectionModel) |
| 2026-01-30 | reviewer | **ISSUE FOUND**: GGUF metadata values are strings → compute buffer shows 2626 GB (string concatenation bug) |
| 2026-01-30 | reviewer | **ISSUE FOUND**: GGUF metadata not persisted, fetched for ALL models on every startup |
| 2026-01-30 | reviewer | **ISSUE FOUND**: Memory warning doesn't update after model release (missing MobX dependencies) |
| 2026-01-30 | reviewer | **SUGGESTION**: Always set availableMemoryCeiling fallback during init to eliminate cold start path |
| 2026-01-30 | human | **SUGGESTION**: Display memory values to users (device capacity + per-model requirement) |

---

## Implementation Status

**Current**: Reverting messy implementation to start fresh with clean architecture.

See **FINAL DESIGN** section above for the definitive approach.

### What Needs Implementation

1. **`memoryEstimator.ts`** - Single `getModelMemoryRequirement()` function
2. **`ModelStore.ts`** - Two variables, calibration lifecycle
3. **`useMemoryCheck.ts`** - Simple check using the single function
4. **Types** - `GGUFMetadata` in Model type

---

## Previous Implementation Report (Reference)

**Removed:**
- ~~`maxAvailMemAfterRelease`~~ - Merged into `availableMemoryCeiling`
- ~~`loadedModelMemoryUsage`~~ - Not needed with proper initialization timing
- ~~Cold start vs calibrated branching~~ - One simple path

### Files Modified

**ModelStore.ts:**
- Two observable properties: `largestSuccessfulLoad`, `availableMemoryCeiling`
- `initializeStore()`: Initialize `availableMemoryCeiling` at startup if undefined
- On successful load: Update `largestSuccessfulLoad` using **GGUF estimator** (not memory delta!)
- On release: Update `availableMemoryCeiling` with max (only place we use getAvailableMemory)

**Key Consistency Fix:**
- Old approach: `largestSuccessfulLoad = memoryBefore - memoryAfter` (unreliable!)
- New approach: `largestSuccessfulLoad = estimateMemory(model)` (consistent with checks)
- If we can't trust `getAvailableMemory()` for checking, we can't trust it for measurement
- Now the same GGUF-based formula is used everywhere

**useMemoryCheck.ts:**
- Simple logic: `ceiling = max(largestSuccessfulLoad, availableMemoryCeiling)`
- Fallback: `totalRAM × 0.5` if both undefined (rare edge case)
- Apply 10% safety margin
- Compare required vs safe ceiling

### Phase 4: GGUF-Based Estimation

**Files Created:**
- `src/utils/memoryEstimator.ts` - Core estimation module
- `src/utils/__tests__/memoryEstimator.test.ts` - 11 unit tests

**Files Modified:**
- `src/utils/types.ts` - Added GGUFMetadata interface, ggufMetadata field to Model
- `src/store/ModelStore.ts` - Added fetchAndPersistGGUFMetadata() method
- `src/hooks/useMemoryCheck.ts` - GGUF-based memoryRequirementEstimate()
- `src/screens/ModelsScreen/ModelCard/ModelCard.tsx` - Pass ggufMetadata to useMemoryCheck

**Key Implementation Details:**
- Memory formula: `(Weights + KV Cache + Compute Buffer) × 1.1`
- KV cache: `n_layers × n_ctx × (head_k + head_v) × n_head_kv × bytes_per_element`
- SWA support: Gemma models with sliding_window properly handled
- Fallback: `file_size × 1.1` when GGUF metadata unavailable
- GGUF metadata parsed from loadLlamaModelInfo() after download completes
- Context settings (n_ctx, cache_type_k, cache_type_v) read from modelStore.contextInitParams

### Test Results

All tests passing:
- `memoryEstimator.test.ts`: 11 tests
- `useMemoryCheck.test.ts`: 8 tests
- `ModelStore.test.ts`: 112 tests

---

## Implementation Plan for Issue Fixes

**Date**: 2026-01-30
**Planner**: pocketpal-planner
**Priority**: FIX ISSUE 2 FIRST (critical bug causing 2626 GB calculations)

### Overview

The clean memory estimation implementation has been completed and tested. Post-implementation review identified 4 critical/high-priority issues and 1 suggestion:

1. **Issue 2 (CRITICAL)**: GGUF metadata values are strings → compute buffer = 2626 GB
2. **Issue 1 (HIGH)**: Vision model memory underestimation in UI components
3. **Issue 3 (HIGH)**: GGUF metadata not persisted / fetched every startup
4. **Issue 4 (MEDIUM)**: Memory warning doesn't update after model release
5. **Issue 5 (LOW)**: Cold start path inefficiency (suggestion)

**Implementation Order**: Issues 2 → 1 → 3 → 4 → (5 optional)

---

### Fix 1: GGUF Metadata String Concatenation Bug (CRITICAL - FIX FIRST)

**Severity**: CRITICAL
**Complexity**: Simple (1 file, ~10 lines)
**Estimated Time**: 5 minutes

#### Problem

GGUF metadata values from `loadLlamaModelInfo()` are strings, causing string concatenation instead of numeric addition:

```typescript
// Current (WRONG):
n_vocab = "128256" (string)
n_embd = "3072" (string)
(n_vocab + n_embd) = "1282563072" → 2626 GB compute buffer!

// Expected:
n_vocab = 128256 (number)
n_embd = 3072 (number)
(n_vocab + n_embd) = 131328 → 0.26 GB compute buffer ✓
```

#### Files to Modify

| File | Lines | Change |
|------|-------|--------|
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/store/ModelStore.ts` | 1038-1048 | Add `Number()` conversions |

#### Implementation Steps

1. Open `ModelStore.ts` at line 1038
2. Wrap ALL numeric metadata values with `Number()`:
   ```typescript
   const metadata: GGUFMetadata = {
     architecture: (modelInfo as any)['general.architecture'],
     n_layers: Number((modelInfo as any)['llama.block_count']),
     n_embd: Number((modelInfo as any)['llama.embedding_length']),
     n_head: Number((modelInfo as any)['llama.attention.head_count']),
     n_head_kv: Number((modelInfo as any)['llama.attention.head_count_kv']),
     n_vocab: Number((modelInfo as any)['llama.vocab_size']),
     n_embd_head_k: Number((modelInfo as any)['llama.attention.key_length']),
     n_embd_head_v: Number((modelInfo as any)['llama.attention.value_length']),
     sliding_window: (modelInfo as any)['llama.attention.sliding_window']
       ? Number((modelInfo as any)['llama.attention.sliding_window'])
       : undefined,
   };
   ```

#### Verification

```bash
cd /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
yarn typecheck
yarn test src/utils/__tests__/memoryEstimator.test.ts
```

#### Acceptance Criteria

- [ ] All GGUF metadata numeric values wrapped with `Number()`
- [ ] TypeScript typecheck passes
- [ ] Memory estimator tests pass
- [ ] Manual test: Load a model, verify compute buffer is ~0.26 GB (not 2626 GB)

---

### Fix 2: Vision Model Memory Underestimation in UI Components (HIGH)

**Severity**: HIGH
**Complexity**: Medium (2 files, ~30 lines total)
**Estimated Time**: 15 minutes

#### Problem

`ModelCard` and `ModelFileCard` call `useMemoryCheck(model)` WITHOUT passing the projection model, while `ModelStore.checkMemoryAndConfirm()` correctly passes it. This causes:

```
User sees ModelCard: 4 GB LLM → memory estimate ~4.8 GB → NO WARNING
User clicks Load: 4 GB LLM + 150 MB mmproj → memory estimate ~5.0 GB → WARNING/FAIL
```

#### Files to Modify

| File | Lines | Change |
|------|-------|--------|
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/screens/ModelsScreen/ModelCard/ModelCard.tsx` | 91-95 | Add projection model resolution |
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/screens/ModelsScreen/HFModelSearch/DetailsView/ModelFileCard/ModelFileCard.tsx` | 94-95 | Add projection model resolution |

#### Implementation Steps

**Step 2.1: Fix ModelCard.tsx**

Add before the `useMemoryCheck` call (around line 91):

```typescript
// Resolve projection model for memory check (same logic as ModelStore.checkMemoryAndConfirm)
const projectionModelForCheck = useMemo(() => {
  if (
    model.supportsMultimodal &&
    modelStore.getModelVisionPreference(model) &&
    model.defaultProjectionModel
  ) {
    return modelStore.models.find(m => m.id === model.defaultProjectionModel);
  }
  return undefined;
}, [model, modelStore.models]); // Add dependency on models array

const {memoryWarning, shortMemoryWarning, multimodalWarning} =
  useMemoryCheck(model, projectionModelForCheck);
```

**Step 2.2: Fix ModelFileCard.tsx**

Add before the `useMemoryCheck` call (around line 94):

```typescript
// Resolve projection model for memory check
const projectionModelForCheck = useMemo(() => {
  if (
    convertedModel.supportsMultimodal &&
    convertedModel.defaultProjectionModel
  ) {
    return modelStore.models.find(
      m => m.id === convertedModel.defaultProjectionModel,
    );
  }
  return undefined;
}, [convertedModel, modelStore.models]);

const {shortMemoryWarning, multimodalWarning} =
  useMemoryCheck(convertedModel, projectionModelForCheck);
```

#### Verification

```bash
cd /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
yarn typecheck
yarn test src/screens/ModelsScreen/ModelCard/__tests__/ModelCard.test.tsx
# Manual: Open app, check vision model memory warnings match on card vs load
```

#### Acceptance Criteria

- [ ] ModelCard passes projection model to useMemoryCheck
- [ ] ModelFileCard passes projection model to useMemoryCheck
- [ ] Memory warnings consistent between UI and load time
- [ ] useMemo dependencies include `modelStore.models`
- [ ] TypeScript typecheck passes
- [ ] No visual regressions

---

### Fix 3: GGUF Metadata Not Persisted / Fetched Every Startup (HIGH)

**Severity**: HIGH (Performance)
**Complexity**: Medium (1 file, ~30 lines)
**Estimated Time**: 20 minutes

#### Problem

Every app startup shows `loadLlamaModelInfo` calls for almost ALL downloaded models (367ms per model), except one, indicating GGUF metadata is not persisted or restored correctly.

#### Root Cause Analysis

1. `initializeStore()` does NOT load metadata for already-downloaded models
2. Metadata is fetched on-demand when `hasEnoughMemory()` is called
3. Possible: `mergeModelLists()` may not preserve `ggufMetadata` field

#### Files to Modify

| File | Lines | Change |
|------|-------|--------|
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/store/ModelStore.ts` | 421-467 | Add background metadata loading |
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/store/ModelStore.ts` | 469-563 | Verify mergeModelLists preserves ggufMetadata |

#### Implementation Steps

**Step 3.1: Add background metadata loading to initializeStore()**

Add after line 463 (after availableMemoryCeiling initialization):

```typescript
// Load missing GGUF metadata for downloaded models (background, non-blocking)
this.loadMissingGGUFMetadata();
```

**Step 3.2: Implement loadMissingGGUFMetadata() method**

Add as a new private method in ModelStore class:

```typescript
/**
 * Load GGUF metadata for downloaded models that don't have it yet.
 * Runs in background, doesn't block startup.
 */
private loadMissingGGUFMetadata = () => {
  const modelsNeedingMetadata = this.models.filter(
    m => m.isDownloaded && !m.ggufMetadata,
  );

  if (modelsNeedingMetadata.length === 0) {
    if (__DEV__) {
      console.log('[ModelStore] All downloaded models have GGUF metadata');
    }
    return;
  }

  if (__DEV__) {
    console.log(
      '[ModelStore] Loading GGUF metadata for',
      modelsNeedingMetadata.length,
      'models in background',
    );
  }

  // Fetch in background, don't block startup
  (async () => {
    for (const model of modelsNeedingMetadata) {
      try {
        await this.fetchAndPersistGGUFMetadata(model);
      } catch (error) {
        // Log but continue - not critical for startup
        console.warn('[ModelStore] Failed to fetch metadata for', model.name, error);
      }
    }
    if (__DEV__) {
      console.log('[ModelStore] Background metadata loading complete');
    }
  })();
};
```

**Step 3.3: Verify mergeModelLists() preserves ggufMetadata**

Check lines 469-563 in `mergeModelLists()`. The merge logic should preserve `ggufMetadata`:

1. For PRESET models (lines 476-525): Metadata should be preserved from `existingModel`
2. For HF models (lines 527-556): Metadata should be preserved from persisted models

If NOT preserved, add:
```typescript
// In merge logic for both PRESET and HF models
if (existingModel.ggufMetadata) {
  mergedModel.ggufMetadata = existingModel.ggufMetadata;
}
```

#### Verification

```bash
cd /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
yarn typecheck

# Manual verification:
# 1. Clear app data
# 2. Download a model
# 3. Restart app → should see background metadata loading
# 4. Restart again → should NOT fetch metadata again (already persisted)
```

#### Acceptance Criteria

- [ ] `loadMissingGGUFMetadata()` method added
- [ ] Called from `initializeStore()` after ceiling initialization
- [ ] Background loading (doesn't block startup)
- [ ] `mergeModelLists()` preserves `ggufMetadata` field
- [ ] Second app startup doesn't fetch metadata again
- [ ] TypeScript typecheck passes

---

### Fix 4: Memory Warning Doesn't Update After Model Release (MEDIUM)

**Severity**: MEDIUM (UX)
**Complexity**: Simple (1 file, 2 lines)
**Estimated Time**: 5 minutes

#### Problem

The memory warning chip in ModelCard doesn't update after `availableMemoryCeiling` changes (e.g., after releasing a model). User must navigate away and back to see updated warning.

#### Root Cause

`useMemoryCheck` hook has incomplete dependencies - missing MobX observables:

```typescript
useEffect(() => {
  // ... memory check
}, [model, projectionModel, l10n]); // Missing observables!
```

#### Files to Modify

| File | Lines | Change |
|------|-------|--------|
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/hooks/useMemoryCheck.ts` | 141 | Add missing dependencies |

#### Implementation Steps

**Step 4.1: Add MobX observables to useEffect dependencies**

Update line 141:

```typescript
useEffect(() => {
  const checkMemory = async () => {
    // ... existing logic
  };

  checkMemory();
}, [
  model,
  projectionModel,
  l10n,
  modelStore.availableMemoryCeiling,  // Add
  modelStore.largestSuccessfulLoad,   // Add
]);
```

#### Verification

```bash
cd /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
yarn typecheck
yarn test src/hooks/__tests__/useMemoryCheck.test.ts

# Manual verification:
# 1. Load a large model → see memory warning on another model's card
# 2. Release the model → warning should disappear immediately
```

#### Acceptance Criteria

- [ ] `modelStore.availableMemoryCeiling` added to dependencies
- [ ] `modelStore.largestSuccessfulLoad` added to dependencies
- [ ] Memory warnings update immediately after model release
- [ ] TypeScript typecheck passes
- [ ] Tests pass

---

### Fix 5: Cold Start Path Inefficiency (OPTIONAL SUGGESTION)

**Severity**: LOW (Optimization)
**Complexity**: Simple (1 file, ~10 lines)
**Estimated Time**: 10 minutes

#### Problem

If `NativeHardwareInfo.getAvailableMemory()` fails during initialization, `availableMemoryCeiling` remains `undefined`, forcing the cold start fallback in `hasEnoughMemory()` for EVERY memory check.

#### Suggested Improvement

Always set `availableMemoryCeiling` during initialization with a fallback:

#### Files to Modify

| File | Lines | Change |
|------|-------|--------|
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/store/ModelStore.ts` | 444-463 | Add fallback to totalMemory × 0.5 |

#### Implementation Steps

**Step 5.1: Update initializeStore() with fallback**

Replace lines 444-463:

```typescript
// Initialize available memory ceiling at app startup if not set
if (this.availableMemoryCeiling === undefined) {
  try {
    const availableBytes = await NativeHardwareInfo.getAvailableMemory();
    runInAction(() => {
      this.availableMemoryCeiling = availableBytes;
    });
    if (__DEV__) {
      console.log(
        '[ModelStore] Initialized availableMemoryCeiling:',
        (availableBytes / 1e9).toFixed(2),
        'GB',
      );
    }
  } catch (error) {
    // Fallback when native call fails
    console.warn(
      '[ModelStore] Native getAvailableMemory failed, using fallback:',
      error,
    );
    const totalMemory = await DeviceInfo.getTotalMemory();
    runInAction(() => {
      this.availableMemoryCeiling = totalMemory * 0.5;
    });
    if (__DEV__) {
      console.log(
        '[ModelStore] Fallback availableMemoryCeiling (50% of RAM):',
        (totalMemory * 0.5 / 1e9).toFixed(2),
        'GB',
      );
    }
  }
}
```

**Step 5.2: Simplify cold start path in hasEnoughMemory()**

Since `availableMemoryCeiling` is now always defined, the cold start path in `useMemoryCheck.ts` (lines 47-65) can be simplified:

```typescript
// Calculate ceiling from calibration data
const ceiling = Math.max(
  largestSuccessfulLoad ?? 0,
  availableMemoryCeiling ?? 0,
);

if (__DEV__) {
  console.log(
    '[MemoryCheck] Ceiling:',
    (ceiling / 1e9).toFixed(2),
    'GB',
    '(largest:',
    ((largestSuccessfulLoad ?? 0) / 1e9).toFixed(2),
    'GB, available:',
    ((availableMemoryCeiling ?? 0) / 1e9).toFixed(2),
    'GB)',
  );
}
```

#### Verification

```bash
cd /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
yarn typecheck
yarn test src/hooks/__tests__/useMemoryCheck.test.ts
```

#### Acceptance Criteria

- [ ] Fallback to `totalMemory × 0.5` when native API fails
- [ ] Cold start path in `hasEnoughMemory()` simplified (optional)
- [ ] Debug logs show whether native or fallback was used
- [ ] TypeScript typecheck passes
- [ ] Tests pass

---

## Test Plan for Issue Fixes

### Unit Tests (Existing)

All existing tests should continue to pass:

```bash
cd /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
yarn test src/utils/__tests__/memoryEstimator.test.ts
yarn test src/hooks/__tests__/useMemoryCheck.test.ts
```

### Manual Testing Checklist

| Issue | Test Case | Expected Result |
|-------|-----------|-----------------|
| Fix 1 (strings) | Load any model, check debug logs | Compute buffer ~0.26 GB (not 2626 GB) |
| Fix 2 (vision UI) | Open ModelCard for vision model | Memory warning matches estimate including mmproj |
| Fix 2 (vision UI) | Open ModelFileCard for vision model | Memory warning matches download size |
| Fix 2 (vision UI) | Load vision model | No unexpected memory warning at load time |
| Fix 3 (metadata) | Restart app after downloading model | Background metadata loading logged |
| Fix 3 (metadata) | Restart app second time | No metadata fetching logged |
| Fix 4 (update) | Load model → check other model's card | Memory warning appears |
| Fix 4 (update) | Release model | Memory warning disappears immediately |
| Fix 5 (fallback) | Disable native API → restart app | Fallback to 50% of RAM logged |

### Regression Testing

- [ ] All 6 useMemoryCheck tests pass
- [ ] All 11 memoryEstimator tests pass
- [ ] ModelStore tests pass
- [ ] No TypeScript errors
- [ ] No visual regressions in ModelCard/ModelFileCard
- [ ] Memory estimation accuracy unchanged

---

## Risk Assessment

| Fix | Risk | Likelihood | Mitigation |
|-----|------|-----------|------------|
| Fix 1 | `Number()` returns NaN for invalid metadata | Low | Validation already exists (lines 1052-1061) |
| Fix 2 | Performance impact of useMemo | Very Low | Simple find() operation, already used in ModelStore |
| Fix 2 | projectionModel resolution logic mismatch | Low | Using exact same logic as ModelStore.checkMemoryAndConfirm |
| Fix 3 | Background loading blocks UI | Very Low | Async/non-blocking, wrapped in try-catch |
| Fix 3 | mergeModelLists overwrites metadata | Low | Verify merge logic preserves field |
| Fix 4 | useEffect runs too frequently | Low | MobX only triggers on actual changes |
| Fix 5 | Fallback overestimates available memory | Low | Conservative 50%, will be replaced after first release |

---

## Completion Checklist

### Issue 1: String Concatenation Bug
- [ ] Number() conversions added to all numeric fields
- [ ] TypeScript typecheck passes
- [ ] memoryEstimator tests pass
- [ ] Manual test: compute buffer ~0.26 GB

### Issue 2: Vision Model UI Underestimation
- [ ] ModelCard resolves projection model
- [ ] ModelFileCard resolves projection model
- [ ] useMemo dependencies correct
- [ ] TypeScript typecheck passes
- [ ] Manual test: warnings consistent

### Issue 3: Metadata Not Persisted
- [ ] loadMissingGGUFMetadata() implemented
- [ ] Called from initializeStore()
- [ ] mergeModelLists() preserves ggufMetadata
- [ ] Manual test: second startup doesn't fetch

### Issue 4: Warning Doesn't Update
- [ ] MobX observables added to dependencies
- [ ] TypeScript typecheck passes
- [ ] Manual test: warning updates immediately

### Issue 5: Cold Start Fallback (Optional)
- [x] Fallback to min(60%, total-1.2GB) added
- [x] Debug logs show native vs fallback
- [x] TypeScript typecheck passes

### Issue 6: Filter mmproj Files from Metadata Loading
- [ ] Add `isProjectionModel()` filter in `loadMissingGGUFMetadata()`
- [ ] Or skip `clip` architecture in `fetchAndPersistGGUFMetadata()`
- [ ] No more "Incomplete GGUF metadata" warnings for mmproj files
- [ ] TypeScript typecheck passes

### Issue 7: Persisted GGUF Metadata Has String Values
- [ ] `resetModels()` should clear `ggufMetadata` to force re-fetch
- [ ] `memoryEstimator.ts` should defensively convert strings to numbers
- [ ] After reset, verify compute buffer shows reasonable values (~0.26 GB)
- [ ] TypeScript typecheck passes

### Overall
- [ ] All tests pass
- [ ] No TypeScript errors
- [ ] No visual regressions
- [ ] Commit messages follow convention
- [ ] Ready for PR update

---

## Commit Strategy

**Separate commits per fix for easy review:**

```bash
# Fix 1: String bug (CRITICAL)
git add src/store/ModelStore.ts
git commit -m "fix(memory): convert GGUF metadata values to numbers

GGUF metadata from loadLlamaModelInfo() returns strings, causing
compute buffer to be 2626 GB due to string concatenation.
Add Number() conversions to all numeric metadata fields.

Issue: compute buffer = ('128256' + '3072') * 512 * 4 = 2626 GB
Fixed: compute buffer = (128256 + 3072) * 512 * 4 = 0.26 GB"

# Fix 2: Vision model UI
git add src/screens/ModelsScreen/ModelCard/ModelCard.tsx \
       src/screens/ModelsScreen/HFModelSearch/DetailsView/ModelFileCard/ModelFileCard.tsx
git commit -m "fix(memory): include projection model in UI memory checks

ModelCard and ModelFileCard were not passing projection model to
useMemoryCheck, causing underestimated memory warnings for vision
models. Users saw no warning in UI but got warnings at load time.

Now resolves projection model using same logic as ModelStore."

# Fix 3: Metadata persistence
git add src/store/ModelStore.ts
git commit -m "fix(memory): load GGUF metadata in background at startup

Metadata was fetched on-demand every time, causing 367ms delay
for each model. Now loads missing metadata in background during
initializeStore() and persists for future startups."

# Fix 4: Warning update
git add src/hooks/useMemoryCheck.ts
git commit -m "fix(memory): update warnings when ceiling changes

Memory warnings didn't update after model release because
useEffect was missing MobX observable dependencies. Added
availableMemoryCeiling and largestSuccessfulLoad to deps."

# Fix 5: Cold start fallback (optional)
git add src/store/ModelStore.ts src/hooks/useMemoryCheck.ts
git commit -m "feat(memory): add fallback when native API fails

If getAvailableMemory() fails at startup, always set
availableMemoryCeiling to totalMemory * 0.5 (conservative).
Eliminates undefined state and cold start path overhead."
```

---

## Notes for Implementer

1. **Fix Issue 2 FIRST** - It's causing completely wrong memory calculations (2626 GB!)
2. **Test after each fix** - Don't batch changes, verify each fix independently
3. **Manual testing critical** - Unit tests won't catch UI update issues
4. **Preserve existing behavior** - Only fix the identified bugs, don't refactor
5. **Issue 5 is optional** - Low priority, can skip if time-constrained

**All changes are pure JavaScript** - No native changes, no platform builds needed.


---

## Issue Fixes Implementation Report

```
Date: 2026-01-30
Agent: pocketpal-implementer

Environment:
- Task ID: TASK-20260126-1433
- Worktree: /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
- Branch: feature/TASK-20260126-1433

Story: Fix 5 Critical Memory Estimation Issues

Status: COMPLETE

Changes Made:

| File | Change | Commit |
|------|--------|--------|
| src/store/ModelStore.ts | Convert GGUF metadata strings to numbers | 2b0000f |
| src/screens/ModelsScreen/ModelCard/ModelCard.tsx | Add projection model resolution for memory check | d2d04e1 |
| src/screens/ModelsScreen/HFModelSearch/DetailsView/ModelFileCard/ModelFileCard.tsx | Add projection model resolution for memory check | d2d04e1 |
| src/store/ModelStore.ts | Add loadMissingGGUFMetadata() method | 04246ed |
| src/store/ModelStore.ts | Call loadMissingGGUFMetadata() from initializeStore() | 04246ed |
| src/hooks/useMemoryCheck.ts | Add MobX observables to useEffect deps | 4d615dd |
| src/store/ModelStore.ts | Add fallback for availableMemoryCeiling init | 5a40ed2 |

Implementation Details:
1. Fix 1 (CRITICAL): Added Number() conversions to all GGUF metadata numeric fields
   - Fixes 2626 GB compute buffer bug caused by string concatenation
   - All numeric fields now properly converted: n_layers, n_embd, n_head, n_head_kv, n_vocab, n_embd_head_k, n_embd_head_v
   - sliding_window field has conditional Number() conversion (can be undefined)

2. Fix 2 (HIGH): Vision model memory underestimation in UI
   - ModelCard.tsx now resolves projection model using useMemo
   - ModelFileCard.tsx now resolves projection model using useMemo
   - Both pass projectionModelForCheck to useMemoryCheck()
   - Dependencies include modelStore.models for reactivity

3. Fix 3 (HIGH): GGUF metadata persistence
   - Implemented loadMissingGGUFMetadata() private method
   - Method filters models needing metadata (isDownloaded && !ggufMetadata)
   - Runs in background via IIFE, doesn't block startup
   - Called from initializeStore() after availableMemoryCeiling init

4. Fix 4 (MEDIUM): Memory warning reactivity
   - Added modelStore.availableMemoryCeiling to useEffect dependencies
   - Added modelStore.largestSuccessfulLoad to useEffect dependencies
   - Memory warnings now update immediately when ceiling changes

5. Fix 5 (LOW): Cold start fallback
   - Added DeviceInfo import to ModelStore.ts
   - Catch block now falls back to totalMemory * 0.5
   - Debug logs show whether native or fallback was used
   - Ensures availableMemoryCeiling is always defined

Deviations from Plan:
- None - All fixes implemented exactly as specified in the plan

Verification Results:
- Lint: PASS
- TypeCheck: PASS
- Tests: PASS (120 test suites, 1356 tests passed, 2 skipped)
- Pod Install: N/A (no native changes)
- iOS Build: N/A (no native changes)
- Android Build: N/A (no native changes)

Notes for Tester:
- All 5 fixes are JavaScript-only changes, no native builds required
- Critical fix (Issue 2/Fix 1) prevents 2626 GB memory calculations
- Vision model memory warnings should now match between UI and load time
- GGUF metadata should persist across app restarts (check logs)
- Memory warnings should update immediately after model release
- No manual testing artifacts available (simulator required)

Blockers: None
```

---

## Last Agent Handoff

```yaml
from_agent: implementer
to_agent: tester (if needed) or reviewer
timestamp: 2026-01-30T15:00:00Z
status: "Implementation complete, all fixes verified"
completed:
  - Fix 1: GGUF metadata string to number conversion (commit 2b0000f)
  - Fix 2: Vision model memory check in UI (commit d2d04e1)
  - Fix 3: GGUF metadata persistence on startup (commit 04246ed)
  - Fix 4: Memory warning reactivity (commit 4d615dd)
  - Fix 5: Cold start fallback (commit 5a40ed2)
  - All tests pass (120 suites, 1356 tests)
  - TypeScript typecheck passes
next_steps:
  - Manual testing recommended (requires iOS/Android simulator)
  - Verify memory calculations are correct (not 2626 GB)
  - Verify vision model warnings match in UI vs load time
  - Verify metadata persists across app restarts
  - Update PR with new commits
blockers: []
context_for_next_agent: |
  All 5 critical memory estimation issues have been fixed:
  1. String concatenation bug causing 2626 GB calculations - FIXED
  2. Vision model memory underestimation in UI - FIXED
  3. GGUF metadata not persisted - FIXED
  4. Memory warning doesn't update after model release - FIXED
  5. Cold start fallback when native API fails - FIXED
  
  All changes are JavaScript-only, no native builds needed.
  Full test suite passes.
  Ready for manual testing or PR update.
```

---

## Implementation Plan for Issues 6 & 7 (Round 2 Fixes)

**Date**: 2026-01-30
**Planner**: pocketpal-planner
**Priority**: HIGH (Both issues cause incorrect behavior and user confusion)

### Overview

During post-implementation testing, two new issues were identified:

1. **Issue 6 (MEDIUM)**: mmproj files processed by loadMissingGGUFMetadata → log spam
2. **Issue 7 (HIGH)**: Persisted GGUF metadata has string values → 100+ GB compute buffer

**Implementation Order**: Issue 7 (HIGH - affects users with old data) → Issue 6 (MEDIUM - log spam)

---

### Fix 6: Filter mmproj Files from Metadata Loading (MEDIUM)

**Severity**: MEDIUM (Unnecessary processing + log spam)
**Complexity**: Simple (1 file, ~5 lines)
**Estimated Time**: 5 minutes

#### Problem

The `loadMissingGGUFMetadata()` function processes ALL downloaded models, including mmproj (CLIP) files. CLIP models have a different metadata structure (`architecture: 'clip'`) and are not used for memory estimation (their size is already added separately via `projectionModel?.size`).

Current behavior:
```
[ModelStore] Loading GGUF metadata for 18 models in background
[ModelStore] Incomplete GGUF metadata, skipping {architecture: 'clip', n_layers: NaN, ...}
[ModelStore] Incomplete GGUF metadata, skipping {architecture: 'clip', n_layers: NaN, ...}
```

This causes:
1. Unnecessary processing of CLIP metadata we don't need
2. Log spam with "Incomplete GGUF metadata, skipping" warnings
3. Confusion (looks like metadata parsing is failing)

#### Root Cause

`loadMissingGGUFMetadata()` (line 1158-1196) filters only by `m.isDownloaded && !m.ggufMetadata`, but doesn't exclude projection models.

#### Files to Modify

| File | Lines | Change |
|------|-------|--------|
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/store/ModelStore.ts` | 1159-1161 | Add projection model filter |

#### Implementation Steps

**Option A: Filter by ModelType (Preferred - More Explicit)**

In `loadMissingGGUFMetadata()`, change line 1159-1161 from:

```typescript
const modelsNeedingMetadata = this.models.filter(
  m => m.isDownloaded && !m.ggufMetadata,
);
```

To:

```typescript
const modelsNeedingMetadata = this.models.filter(
  m => m.isDownloaded && !m.ggufMetadata && m.modelType !== ModelType.PROJECTION,
);
```

**Option B: Filter by filename pattern (Alternative)**

Import `isProjectionModel` from utils (already imported as part of multimodalHelpers):

```typescript
const modelsNeedingMetadata = this.models.filter(
  m => m.isDownloaded && !m.ggufMetadata && !isProjectionModel(m.filename),
);
```

**Recommendation**: Use Option A (ModelType.PROJECTION) because:
- More explicit and type-safe
- Matches the existing pattern in `filterProjectionModels()` utility
- ModelType is already set correctly during model creation (see `hfAsModel()` in utils/index.ts:415)

#### Pattern Reference

See `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/utils/index.ts:23-26`:
```typescript
import {
  isVisionRepo,
  getMmprojFiles,
  isProjectionModel,
  getRecommendedProjectionModel,
  getVisionModelSizeBreakdown,
} from './multimodalHelpers';
```

See `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/utils/modelUtils.ts`:
```typescript
export function filterProjectionModels(models: Model[]): Model[] {
  return models.filter(m => m.modelType !== ModelType.PROJECTION);
}
```

#### Verification

```bash
cd /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
yarn typecheck
yarn lint
# Manual: Check logs - should see fewer models in "Loading GGUF metadata for N models"
# Manual: Should NOT see "Incomplete GGUF metadata, skipping" for CLIP models
```

#### Acceptance Criteria

- [ ] mmproj/CLIP models are excluded from metadata loading
- [ ] No more "Incomplete GGUF metadata, skipping" warnings for projection models
- [ ] TypeScript typecheck passes
- [ ] Lint passes
- [ ] Manual test: Check logs, verify only LLM models are processed

---

### Fix 7: Persisted GGUF Metadata Has String Values (HIGH - Migration)

**Severity**: HIGH (Causes 100+ GB compute buffer for users with old data)
**Complexity**: Medium (2 files, ~20 lines total)
**Estimated Time**: 15 minutes

#### Problem

**The Issue**: The previous buggy code (before Fix 1) stored GGUF metadata values as strings. Fix 1 only applies `Number()` conversion when **fetching new metadata**, but doesn't handle **existing persisted data** in AsyncStorage.

Users who already have models with string-based metadata will see:
```
metadata: {n_embd: "576", n_vocab: "49280", ...}  <-- Strings!
Compute Buffer: 100.93 GB  <-- WRONG! Should be ~0.26 GB
```

This is because the memory estimator does arithmetic on these values:
```typescript
// memoryEstimator.ts line 59
return (n_vocab + n_embd) * n_ubatch * 4;

// If n_vocab and n_embd are strings:
("49280" + "576") * 512 * 4 = "49280576" * 512 * 4 = 100.93 GB (WRONG!)

// If they were numbers (Fix 1):
(49280 + 576) * 512 * 4 = 49856 * 512 * 4 = 0.26 GB (CORRECT)
```

**Why Fix 1 Didn't Solve This**: Fix 1 added `Number()` conversions in `fetchAndPersistGGUFMetadata()` (line 1126-1135), which only runs when fetching **new** metadata from the GGUF file. Models that already have persisted metadata in AsyncStorage are loaded via `makePersistable()` and never go through `fetchAndPersistGGUFMetadata()` again.

#### Root Cause

1. **Persistence Layer**: MobX `makePersistable()` deserializes JSON from AsyncStorage as-is. If the JSON has `"576"` (string), it stays a string.
2. **No Migration**: There's no code to detect and fix old string-based metadata.
3. **No Defense in Depth**: The memory estimator assumes metadata values are numbers (per the TypeScript type), but doesn't validate at runtime.

#### Strategy: Defense in Depth (2 Layers)

**Layer 1**: Clear old metadata via `resetModels()` (forces re-fetch with correct types)
**Layer 2**: Add defensive number conversion in memory estimator (safety net)

This approach:
- ✅ Fixes the issue immediately (Layer 2)
- ✅ Fixes the root cause permanently (Layer 1)
- ✅ Protects against future type inconsistencies (Layer 2)
- ✅ No breaking changes (graceful migration)

#### Files to Modify

| File | Lines | Change |
|------|-------|--------|
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/store/ModelStore.ts` | 1956-1994 | Clear ggufMetadata in resetModels |
| `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/utils/memoryEstimator.ts` | 24-60 | Add defensive Number() conversions |

#### Implementation Steps

**Step 7.1: Layer 1 - Clear Old Metadata in resetModels()**

In `ModelStore.ts`, modify the `resetModels()` function to clear `ggufMetadata` for all models. This forces a re-fetch with the corrected `Number()` conversions from Fix 1.

Add after line 1968 (in the `localModels.forEach` loop):

```typescript
localModels.forEach(model => {
  const defaultSettings = getLocalModelDefaultSettings();
  // We change the default settings as well, in case the app introduces new settings.
  model.defaultChatTemplate = {...defaultSettings.chatTemplate};
  model.defaultStopWords = [
    ...(defaultSettings?.completionParams?.stop || []),
  ];
  model.chatTemplate = {...defaultSettings.chatTemplate};
  model.stopWords = [...(defaultSettings?.completionParams?.stop || [])];
  
  // NEW: Clear GGUF metadata to force re-fetch with correct number types
  model.ggufMetadata = undefined;
});
```

Add after line 1985 (in the `hfModels.forEach` loop):

```typescript
hfModels.forEach(model => {
  const defaultSettings = getHFDefaultSettings(
    model.hfModel as HuggingFaceModel,
  );
  // We change the default settings as well, in case the app introduces new settings.
  model.defaultChatTemplate = {...defaultSettings.chatTemplate};
  model.defaultStopWords = [
    ...(defaultSettings?.completionParams?.stop || []),
  ];
  model.chatTemplate = {...defaultSettings.chatTemplate};
  model.stopWords = [...(defaultSettings?.completionParams?.stop || [])];
  
  // NEW: Clear GGUF metadata to force re-fetch with correct number types
  model.ggufMetadata = undefined;
});
```

**Why This Works**: After `resetModels()` runs:
1. All models have `ggufMetadata = undefined`
2. Next app startup: `loadMissingGGUFMetadata()` detects missing metadata
3. `fetchAndPersistGGUFMetadata()` runs with Fix 1's `Number()` conversions
4. Metadata is re-persisted with correct number types

**Step 7.2: Layer 2 - Defensive Number Conversion in Memory Estimator**

Even after the migration, we should defensively convert to numbers to prevent future issues (e.g., if metadata comes from an external source or gets corrupted).

In `memoryEstimator.ts`, modify both calculation functions:

**In `calculateKVCacheMemory()` (line 24-46)**, change line 28-29 from:

```typescript
const {n_layers, n_embd_head_k, n_embd_head_v, n_head_kv, sliding_window} =
  metadata;
```

To:

```typescript
// Defensive: Convert to numbers in case metadata was persisted as strings
const n_layers = Number(metadata.n_layers);
const n_embd_head_k = Number(metadata.n_embd_head_k);
const n_embd_head_v = Number(metadata.n_embd_head_v);
const n_head_kv = Number(metadata.n_head_kv);
const sliding_window = metadata.sliding_window
  ? Number(metadata.sliding_window)
  : undefined;
```

**In `calculateComputeBuffer()` (line 51-60)**, change line 55-56 from:

```typescript
const {n_vocab, n_embd} = metadata;
const {n_ubatch} = contextSettings;
```

To:

```typescript
// Defensive: Convert to numbers in case metadata was persisted as strings
const n_vocab = Number(metadata.n_vocab);
const n_embd = Number(metadata.n_embd);
const {n_ubatch} = contextSettings;
```

**Why This Is Safe**:
- `Number("576")` → `576` (correct)
- `Number(576)` → `576` (idempotent)
- `Number(undefined)` → `NaN` (would fail validation elsewhere)
- No performance impact (Number() is extremely fast)

#### Pattern Reference

See existing defensive conversion in `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433/src/store/ModelStore.ts:1066-1070`:

```typescript
// Helper to get architecture-specific value with fallback
const getArchValue = (
  field: string,
  defaultValue?: number,
): number | undefined => {
  const key = `${architecture}.${field}`;
  const value = (modelInfo as any)[key];
  return value !== undefined ? Number(value) : defaultValue;
};
```

This function already uses `Number(value)` for defensive conversion.

#### Verification

```bash
cd /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
yarn typecheck
yarn test src/utils/__tests__/memoryEstimator.test.ts
# Manual: Reset models in Settings, verify compute buffer shows reasonable values
```

#### Acceptance Criteria

- [ ] `resetModels()` clears `ggufMetadata` for all models
- [ ] `calculateKVCacheMemory()` defensively converts metadata values to numbers
- [ ] `calculateComputeBuffer()` defensively converts metadata values to numbers
- [ ] TypeScript typecheck passes
- [ ] Memory estimator tests pass
- [ ] Manual test: After reset, compute buffer shows ~0.26 GB (not 100+ GB)
- [ ] Manual test: Logs show metadata being re-fetched after reset

#### Testing Notes

**Simulating the Bug** (for verification):
1. Before Fix 7: Check logs, you may see large compute buffer values for models with old metadata
2. Apply Fix 7 Layer 2 (defensive conversion): Compute buffer should immediately show correct values
3. Run "Reset Models" (triggers Layer 1): GGUF metadata cleared, will be re-fetched on next startup
4. Restart app: Verify metadata is re-fetched with correct number types

**Expected Behavior After Fix**:
- Users with old string-based metadata: Layer 2 immediately corrects calculations
- After running "Reset Models": Layer 1 clears old metadata, forces re-fetch
- New metadata: Fetched with Fix 1's `Number()` conversions, persisted as numbers
- Future-proof: Layer 2 protects against any type inconsistencies

---

## Updated Test Acceptance Criteria

### Issue 6: Filter mmproj Files from Metadata Loading
- [ ] Add `ModelType.PROJECTION` filter in `loadMissingGGUFMetadata()`
- [ ] No more "Incomplete GGUF metadata" warnings for mmproj files
- [ ] TypeScript typecheck passes
- [ ] Lint passes

### Issue 7: Persisted GGUF Metadata Has String Values
- [ ] `resetModels()` clears `ggufMetadata` to force re-fetch
- [ ] `memoryEstimator.ts` defensively converts all numeric fields to numbers
- [ ] After reset, verify compute buffer shows reasonable values (~0.26 GB)
- [ ] TypeScript typecheck passes
- [ ] Memory estimator tests pass

### Overall
- [ ] All tests pass
- [ ] No TypeScript errors
- [ ] No visual regressions
- [ ] Commit messages follow convention
- [ ] Ready for PR update

---

## Commit Strategy

**Separate commits per fix for easy review:**

```bash
# Fix 6: mmproj filter
git add src/store/ModelStore.ts
git commit -m "fix(metadata): exclude mmproj files from metadata loading

mmproj (CLIP) files have different metadata structure and are not
used for memory estimation. Filter them out to avoid log spam.

Before: 'Incomplete GGUF metadata, skipping' for every mmproj
After: Only LLM models are processed for metadata

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"

# Fix 7: String to number migration
git add src/store/ModelStore.ts src/utils/memoryEstimator.ts
git commit -m "fix(memory): migrate old string-based GGUF metadata

Layer 1 (migration): resetModels() clears old metadata, forces re-fetch
Layer 2 (defense): memoryEstimator defensively converts to numbers

Before: Old metadata has strings → 100+ GB compute buffer
After: Numbers enforced → ~0.26 GB compute buffer

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Risk Assessment

### Issue 6 Risks

**Low Risk** - Simple filter addition:
- ✅ No breaking changes (projection models weren't getting valid metadata anyway)
- ✅ No performance impact (fewer models to process = faster)
- ✅ Easy to verify (check logs for "Incomplete GGUF metadata" warnings)

**Mitigation**: None needed (improvement only)

### Issue 7 Risks

**Medium Risk** - Data migration:
- ⚠️ Users must run "Reset Models" to clear old metadata
- ⚠️ Metadata will be re-fetched on next startup (367ms per model)

**Mitigations**:
1. **Layer 2 First**: Defensive conversion fixes the symptom immediately, even before reset
2. **Graceful Degradation**: Old metadata still works (just gets converted defensively)
3. **No Data Loss**: resetModels() only clears metadata, not downloaded files
4. **User Control**: Users choose when to reset (not automatic)
5. **Fast Re-fetch**: Background process, doesn't block UI

**Worst Case Scenario**: User doesn't reset models
- **Impact**: Memory calculations still work (Layer 2 defensive conversion)
- **Drawback**: Metadata not persisted as numbers (will convert every time)
- **Solution**: Next time they reset models (eventually), metadata gets fixed

---

## Implementation Notes

### Why Not Automatic Migration?

We considered automatically migrating string metadata to numbers on app startup, but decided against it because:

1. **Complexity**: Would need to iterate all models, check metadata types, convert, and re-persist
2. **Timing**: Would need to run after hydration but before `loadMissingGGUFMetadata()`
3. **Edge Cases**: Hard to distinguish "missing metadata" from "cleared for migration"
4. **Simplicity**: Defensive conversion + user-triggered reset is simpler and safer

### Why resetModels() Is the Right Place

`resetModels()` is already the "reset to defaults" function:
- Resets chat templates to defaults
- Resets stop words to defaults
- Resets completion settings to defaults
- **Now**: Resets GGUF metadata (forces re-fetch)

Users already understand this as "clear customizations and re-sync", so clearing metadata fits the mental model.

### Testing Strategy

**Unit Tests**: Existing memory estimator tests already verify calculations
**Integration Tests**: Manual testing required (see Testing Notes above)
**Rollback Plan**: If issues arise, revert commits (both fixes are independent)

---

## Summary

**Issue 6**: Simple filter to exclude mmproj files (5 minutes)
**Issue 7**: Two-layer fix for string metadata (15 minutes)

**Total Time**: ~20 minutes
**Total Files**: 2 files modified
**Total Lines**: ~25 lines added/changed

Both fixes are minimal, defensive, and follow existing patterns. No breaking changes, no native builds required.


---

## PHASE: DISPLAY MEMORY VALUES TO USERS

### Status
- **Status**: Ready for Implementation
- **Native Changes**: NO
- **Complexity**: Standard Feature
- **Created**: 2026-01-30

### Context

The memory estimation system is now working accurately. The next enhancement is to expose this information to users for transparency and better decision-making. This phase implements the UX design documented in "Future Enhancement: Display Memory Values to Users" section above.

### Requirements

**MUST**:
1. Display required RAM on ModelCard with fit status (fits ✓, tight ⚠️, won't fit ✗)
2. Display required RAM on ModelFileCard with fit status
3. Display device memory bar in ModelsScreen header showing usable/total memory
4. Use proper L10n for all user-facing strings
5. Follow existing UI patterns and MobX observer patterns
6. Include proper accessibility labels

**SHOULD**:
1. Create reusable components where appropriate
2. Use theme colors consistently
3. Add tap interaction on memory bar to show tooltip
4. Ensure text is readable on all background colors

**MUST NOT**:
1. Modify the memory estimation logic (already working correctly)
2. Change ModelStore observables (largestSuccessfulLoad, availableMemoryCeiling)
3. Add new native code

### Affected Files

**New Files**:
- `src/components/MemoryRequirement/MemoryRequirement.tsx` (new component)
- `src/components/MemoryRequirement/index.ts` (export)
- `src/components/MemoryRequirement/__tests__/MemoryRequirement.test.tsx` (tests)
- `src/components/DeviceMemoryBar/DeviceMemoryBar.tsx` (new component)
- `src/components/DeviceMemoryBar/index.ts` (export)
- `src/components/DeviceMemoryBar/__tests__/DeviceMemoryBar.test.tsx` (tests)
- `src/utils/memoryDisplay.ts` (helper functions)

**Modified Files**:
- `src/screens/ModelsScreen/ModelCard/ModelCard.tsx` (add MemoryRequirement display)
- `src/screens/ModelsScreen/HFModelSearch/DetailsView/ModelFileCard/ModelFileCard.tsx` (add MemoryRequirement display)
- `src/screens/ModelsScreen/ModelsScreen.tsx` (add DeviceMemoryBar to header)
- `src/utils/l10n.ts` (add memory display strings)
- `src/components/index.ts` (export new components)

### Implementation Plan

#### Step 1: Add L10n Strings

**File**: `src/utils/l10n.ts`

Add to the `memory:` section (around line 174):

```typescript
memory: {
  shortWarning: 'Memory Warning',
  warning: 'Warning: Model size may exceed available memory...',
  multimodalWarning: 'This device may not have sufficient resources...',
  
  // NEW: Memory display strings
  requiredRAM: 'Required RAM: ~{size}',
  fits: 'fits',
  tight: 'tight',
  wontFit: "won't fit",
  deviceMemory: '~{available}/{total} GB usable',
  deviceMemoryTooltip: 'Estimated usable memory. As you use the app, we keep learning what works for your device.',
  
  alerts: {
    // existing alert strings...
  },
},
```

**Pattern**: Follow existing L10n structure with interpolation using `{variable}` syntax.

#### Step 2: Create Memory Display Utility Functions

**File**: `src/utils/memoryDisplay.ts` (NEW)

**NOTE**: Reuse existing `formatBytes` from `src/utils/formatters.ts` instead of creating `formatMemoryGB`.

```typescript
import DeviceInfo from 'react-native-device-info';
import {Model} from './types';
import {formatBytes} from './formatters';  // REUSE existing utility
import {getModelMemoryRequirement} from './memoryEstimator';
import {modelStore} from '../store';

export type MemoryFitStatus = 'fits' | 'tight' | 'wont_fit';

/**
 * Determine if a model will fit in device memory
 *
 * @param model - The model to check
 * @param projectionModel - Optional mmproj model for vision models
 * @returns Status: 'fits', 'tight', or 'wont_fit'
 */
export async function getMemoryFitStatus(
  model: Model,
  projectionModel?: Model,
): Promise<MemoryFitStatus> {
  // Get memory requirement
  const requiredMemory = getModelMemoryRequirement(
    model,
    projectionModel,
    modelStore.contextInitParams,
  );

  // Get device total memory
  const totalMemory = await DeviceInfo.getTotalMemory();

  // Get learned available ceiling
  const availableCeiling = Math.max(
    modelStore.largestSuccessfulLoad ?? 0,
    modelStore.availableMemoryCeiling ?? 0,
  );

  // Determine status
  if (requiredMemory <= availableCeiling) {
    return 'fits';
  } else if (requiredMemory <= totalMemory) {
    return 'tight';
  } else {
    return 'wont_fit';
  }
}

/**
 * Get device memory information for display
 *
 * @returns Object with availableBytes, totalBytes for formatting
 */
export async function getDeviceMemoryInfo(): Promise<{
  availableBytes: number;
  totalBytes: number;
}> {
  const totalBytes = await DeviceInfo.getTotalMemory();

  // Get learned available ceiling
  const availableCeiling = Math.max(
    modelStore.largestSuccessfulLoad ?? 0,
    modelStore.availableMemoryCeiling ?? 0,
  );

  // If no calibration data, use fallback
  let availableBytes = availableCeiling;
  if (availableCeiling === 0) {
    availableBytes = Math.min(totalBytes * 0.6, totalBytes - 1.2 * 1e9);
  }

  return {
    availableBytes,
    totalBytes,
  };
}

// NOTE: Use formatBytes from src/utils/formatters.ts for display:
// formatBytes(bytes, 1) → "3.2 GB"
// formatBytes(bytes, 0) → "8 GB"
```

**Reusable utilities from codebase**:
- `formatBytes(bytes, decimals)` from `src/utils/formatters.ts` - already used throughout app
- `Tooltip` from `react-native-paper` - used in BenchResultCard, DetailsView
- `useTheme()` hook for colors - standard pattern

**Pattern**: Utility functions that encapsulate memory logic, follow existing patterns in `src/utils/`.

#### Step 3: Create MemoryRequirement Component

**File**: `src/components/MemoryRequirement/MemoryRequirement.tsx` (NEW)

```typescript
import React, {useContext, useEffect, useState} from 'react';
import {View, StyleSheet} from 'react-native';
import {Text} from 'react-native-paper';
import {observer} from 'mobx-react-lite';

import {useTheme} from '../../hooks';
import {L10nContext, formatBytes} from '../../utils';  // REUSE formatBytes
import {
  getMemoryFitStatus,
  MemoryFitStatus,
} from '../../utils/memoryDisplay';
import {getModelMemoryRequirement} from '../../utils/memoryEstimator';
import {Model} from '../../utils/types';
import {modelStore} from '../../store';

interface MemoryRequirementProps {
  model: Model;
  projectionModel?: Model;
  /** Optional: Override computed fit status for testing */
  fitStatus?: MemoryFitStatus;
}

/**
 * Display memory requirement for a model with fit status indicator
 * 
 * Shows: "Required RAM: ~2.1 GB (fits ✓)"
 */
export const MemoryRequirement: React.FC<MemoryRequirementProps> = observer(
  ({model, projectionModel, fitStatus: fitStatusOverride}) => {
    const theme = useTheme();
    const l10n = useContext(L10nContext);
    const [fitStatus, setFitStatus] = useState<MemoryFitStatus>('fits');

    // Read MobX observables to trigger re-render when calibration changes
    const calibrationCeiling = Math.max(
      modelStore.largestSuccessfulLoad ?? 0,
      modelStore.availableMemoryCeiling ?? 0,
    );

    useEffect(() => {
      if (fitStatusOverride) {
        setFitStatus(fitStatusOverride);
        return;
      }

      getMemoryFitStatus(model, projectionModel).then(setFitStatus);
    }, [model, projectionModel, fitStatusOverride, calibrationCeiling]);

    // Get memory requirement
    const memoryRequirement = getModelMemoryRequirement(
      model,
      projectionModel,
      modelStore.contextInitParams,
    );

    // Get status text and icon
    const statusConfig = {
      fits: {
        text: l10n.memory.fits,
        icon: '✓',
        color: theme.colors.primary, // Green-ish
      },
      tight: {
        text: l10n.memory.tight,
        icon: '⚠️',
        color: theme.colors.error, // Orange/Yellow
      },
      wont_fit: {
        text: l10n.memory.wontFit,
        icon: '✗',
        color: theme.colors.error, // Red
      },
    };

    const config = statusConfig[fitStatus];
    const sizeText = formatBytes(memoryRequirement, 1);  // REUSE: "2.1 GB"
    const displayText = l10n.memory.requiredRAM.replace('{size}', sizeText);

    return (
      <View style={styles.container} testID="memory-requirement">
        <Text
          variant="bodySmall"
          style={[styles.text, {color: theme.colors.onSurfaceVariant}]}
          testID="memory-requirement-text">
          {displayText}{' '}
          <Text
            style={{color: config.color}}
            testID={`memory-status-${fitStatus}`}>
            ({config.text} {config.icon})
          </Text>
        </Text>
      </View>
    );
  },
);

const styles = StyleSheet.create({
  container: {
    marginTop: 2,
    marginBottom: 4,
  },
  text: {
    fontSize: 12,
  },
});
```

**Pattern**: 
- MobX `observer` component that tracks calibration changes
- Uses `useTheme` hook for theming
- Uses `L10nContext` for localization
- Follows React Native Paper Text components
- testID for testing

**File**: `src/components/MemoryRequirement/index.ts` (NEW)

```typescript
export {MemoryRequirement} from './MemoryRequirement';
```

#### Step 4: Create DeviceMemoryBar Component

**File**: `src/components/DeviceMemoryBar/DeviceMemoryBar.tsx` (NEW)

**NOTE**: Uses simple styled Views for the bar (not ProgressBar) since we need text inside. Uses `Tooltip` from react-native-paper for the tap interaction.

```typescript
import React, {useContext, useEffect, useState} from 'react';
import {View, StyleSheet} from 'react-native';
import {Text, Tooltip} from 'react-native-paper';  // REUSE: Tooltip from paper
import {observer} from 'mobx-react-lite';

import {useTheme} from '../../hooks';
import {L10nContext, formatBytes} from '../../utils';  // REUSE: formatBytes
import {getDeviceMemoryInfo} from '../../utils/memoryDisplay';
import {modelStore} from '../../store';

interface DeviceMemoryBarProps {
  /** Optional: Override memory info for testing (in bytes) */
  availableBytes?: number;
  totalBytes?: number;
}

/**
 * Display device memory capacity as a progress bar with text
 *
 * Shows: [████ ~3.2/8 GB usable ████░░░░░░░░]
 * Uses simple Views (not ProgressBar) since we need text inside.
 * Uses Tooltip from react-native-paper for tap interaction.
 */
export const DeviceMemoryBar: React.FC<DeviceMemoryBarProps> = observer(
  ({availableBytes: availableOverride, totalBytes: totalOverride}) => {
    const theme = useTheme();
    const l10n = useContext(L10nContext);
    const [memoryInfo, setMemoryInfo] = useState({
      availableBytes: availableOverride ?? 0,
      totalBytes: totalOverride ?? 0,
    });

    // Read MobX observables to trigger re-render when calibration changes
    const calibrationCeiling = Math.max(
      modelStore.largestSuccessfulLoad ?? 0,
      modelStore.availableMemoryCeiling ?? 0,
    );

    useEffect(() => {
      if (availableOverride !== undefined && totalOverride !== undefined) {
        setMemoryInfo({
          availableBytes: availableOverride,
          totalBytes: totalOverride,
        });
        return;
      }

      getDeviceMemoryInfo().then(setMemoryInfo);
    }, [availableOverride, totalOverride, calibrationCeiling]);

    const {availableBytes, totalBytes} = memoryInfo;
    const percentage = totalBytes > 0 ? (availableBytes / totalBytes) * 100 : 0;

    // REUSE: formatBytes for display
    const availableText = formatBytes(availableBytes, 1);  // "3.2 GB"
    const totalText = formatBytes(totalBytes, 0);          // "8 GB"
    const usableText = `~${availableText}/${totalText} usable`;

    return (
      <View style={styles.container} testID="device-memory-bar">
        {/* REUSE: Tooltip from react-native-paper */}
        <Tooltip
          title={l10n.memory.deviceMemoryTooltip}
          enterTouchDelay={0}
          leaveTouchDelay={3000}>
          <View
            style={[
              styles.progressBar,
              {backgroundColor: theme.colors.surfaceVariant},
            ]}
            accessibilityLabel={`Device memory: ${usableText}`}
            accessibilityHint="Tap for more information"
            testID="device-memory-bar-touchable">
            {/* Filled portion */}
            <View
              style={[
                styles.progressFill,
                {
                  width: `${percentage}%`,
                  backgroundColor: theme.colors.primary,
                },
              ]}
              testID="memory-bar-fill"
            />
            {/* Text overlay */}
            <View style={styles.textContainer}>
              <Text
                variant="labelSmall"
                style={[
                  styles.text,
                  {color: theme.colors.onPrimary},
                ]}
                testID="memory-bar-text">
                {usableText}
              </Text>
            </View>
          </View>
        </Tooltip>
      </View>
    );
  },
);

const styles = StyleSheet.create({
  container: {
    marginHorizontal: 8,
  },
  progressBar: {
    height: 24,
    width: 160,
    borderRadius: 8,
    position: 'relative',
    overflow: 'hidden',
  },
  progressFill: {
    position: 'absolute',
    left: 0,
    top: 0,
    bottom: 0,
    borderRadius: 8,
  },
  textContainer: {
    position: 'absolute',
    left: 0,
    right: 0,
    top: 0,
    bottom: 0,
    justifyContent: 'center',
    alignItems: 'center',
  },
  text: {
    fontWeight: '600',
    fontSize: 11,
  },
});
```

**Pattern**:
- MobX `observer` component
- REUSE: `Tooltip` from react-native-paper (no custom Portal needed)
- REUSE: `formatBytes` from utils for consistent formatting
- Simple Views for progress bar (not ProgressBar, since we need text inside)
- Accessibility labels for screen readers
- testID for testing

**File**: `src/components/DeviceMemoryBar/index.ts` (NEW)

```typescript
export {DeviceMemoryBar} from './DeviceMemoryBar';
```

#### Step 5: Update Component Exports

**File**: `src/components/index.ts`

Add to exports:

```typescript
export {MemoryRequirement} from './MemoryRequirement';
export {DeviceMemoryBar} from './DeviceMemoryBar';
```

**Pattern**: Re-export all components from central index.

#### Step 6: Add MemoryRequirement to ModelCard

**File**: `src/screens/ModelsScreen/ModelCard/ModelCard.tsx`

**Location**: After the model name, around line 200-220 where the model info is displayed.

**Changes**:
1. Import MemoryRequirement component
2. Add MemoryRequirement display after model name

```typescript
// Add to imports (top of file)
import {MemoryRequirement} from '../../../components';

// In the render section, after model name display:
{/* Model Name */}
<Text variant="titleMedium" style={styles.modelName}>
  {model.name}
</Text>

{/* NEW: Memory Requirement */}
{model.isDownloaded && (
  <MemoryRequirement
    model={model}
    projectionModel={projectionModelForCheck}
  />
)}

{/* Rest of existing content... */}
```

**Pattern**: 
- Only show for downloaded models (since we need GGUF metadata for accuracy)
- Pass projectionModel for vision models
- Place under model name, before action buttons

#### Step 7: Add MemoryRequirement to ModelFileCard

**File**: `src/screens/ModelsScreen/HFModelSearch/DetailsView/ModelFileCard/ModelFileCard.tsx`

**Location**: After the filename, around line 180-200 in the render section.

**Changes**:
1. Import MemoryRequirement component
2. Add MemoryRequirement display after filename

```typescript
// Add to imports (top of file)
import {MemoryRequirement} from '../../../../../components';

// In the render section, after filename:
{/* Filename */}
<Text variant="bodyMedium" style={styles.filename}>
  {modelFile.rfilename}
</Text>

{/* NEW: Memory Requirement */}
<MemoryRequirement
  model={convertedModel}
  projectionModel={projectionModelForCheck}
/>

{/* Size and action buttons... */}
```

**Pattern**: 
- Show for all files (uses fallback estimation if no GGUF metadata)
- Pass projectionModel for vision models
- Place under filename, before size display

#### Step 8: Add DeviceMemoryBar to ModelsScreen Header

**File**: `src/screens/ModelsScreen/ModelsScreen.tsx`

**Location**: This screen uses a `FlatList` without a custom header. We need to add a `ListHeaderComponent`.

**Changes**:
1. Import DeviceMemoryBar component
2. Add ListHeaderComponent to FlatList

```typescript
// Add to imports (top of file)
import {DeviceMemoryBar} from '../../components';

// Create header component (add before return statement, around line 330)
const renderHeader = () => (
  <View style={styles.header} testID="models-screen-header">
    <DeviceMemoryBar />
  </View>
);

// In the FlatList (around line 351):
<FlatList
  testID="flat-list"
  keyboardDismissMode="on-drag"
  keyboardShouldPersistTaps="handled"
  contentContainerStyle={styles.listContainer}
  data={flatListModels}
  keyExtractor={item => item.type}
  extraData={activeModelId}
  ListHeaderComponent={renderHeader}  // NEW
  renderItem={renderGroupHeader}
  // ... rest of props
/>
```

**Add to styles** (in `src/screens/ModelsScreen/styles.ts` or inline):

```typescript
header: {
  paddingVertical: 12,
  paddingHorizontal: 16,
  flexDirection: 'row',
  justifyContent: 'flex-end',
  alignItems: 'center',
  borderBottomWidth: 1,
  borderBottomColor: theme.colors.outlineVariant,
},
```

**Pattern**: 
- Use FlatList's ListHeaderComponent
- Right-align the memory bar
- Add subtle border separator

### Test Requirements

#### Unit Tests

**File**: `src/components/MemoryRequirement/__tests__/MemoryRequirement.test.tsx`

```typescript
import React from 'react';
import {render} from '../../../../jest/test-utils';
import {MemoryRequirement} from '../MemoryRequirement';
import {Model} from '../../../utils/types';

const mockModel: Model = {
  id: 'test-model',
  name: 'Test Model',
  size: 2.1 * 1e9, // 2.1 GB
  isDownloaded: true,
  // ... other required fields
};

describe('MemoryRequirement', () => {
  it('renders with fits status', () => {
    const {getByTestId, getByText} = render(
      <MemoryRequirement model={mockModel} fitStatus="fits" />
    );
    
    expect(getByTestId('memory-requirement')).toBeTruthy();
    expect(getByText(/fits/i)).toBeTruthy();
    expect(getByText(/✓/)).toBeTruthy();
  });

  it('renders with tight status', () => {
    const {getByText} = render(
      <MemoryRequirement model={mockModel} fitStatus="tight" />
    );
    
    expect(getByText(/tight/i)).toBeTruthy();
    expect(getByText(/⚠️/)).toBeTruthy();
  });

  it('renders with wont_fit status', () => {
    const {getByText} = render(
      <MemoryRequirement model={mockModel} fitStatus="wont_fit" />
    );
    
    expect(getByText(/won't fit/i)).toBeTruthy();
    expect(getByText(/✗/)).toBeTruthy();
  });

  it('displays memory requirement in GB', () => {
    const {getByText} = render(
      <MemoryRequirement model={mockModel} fitStatus="fits" />
    );
    
    expect(getByText(/2\.\d GB/)).toBeTruthy();
  });
});
```

**File**: `src/components/DeviceMemoryBar/__tests__/DeviceMemoryBar.test.tsx`

```typescript
import React from 'react';
import {fireEvent, render} from '../../../../jest/test-utils';
import {DeviceMemoryBar} from '../DeviceMemoryBar';

describe('DeviceMemoryBar', () => {
  it('renders memory bar', () => {
    const {getByTestId} = render(
      <DeviceMemoryBar availableGB={3.2} totalGB={8} />
    );
    
    expect(getByTestId('device-memory-bar')).toBeTruthy();
  });

  it('displays memory text', () => {
    const {getByText} = render(
      <DeviceMemoryBar availableGB={3.2} totalGB={8} />
    );
    
    expect(getByText(/~3\.2\/8 GB usable/)).toBeTruthy();
  });

  it('calculates correct percentage', () => {
    const {getByTestId} = render(
      <DeviceMemoryBar availableGB={4} totalGB={8} />
    );
    
    const fill = getByTestId('memory-bar-fill');
    // 4/8 = 50%
    expect(fill.props.style).toMatchObject({width: '50%'});
  });

  it('shows tooltip on tap', () => {
    const {getByTestId, queryByText} = render(
      <DeviceMemoryBar availableGB={3.2} totalGB={8} />
    );
    
    // Tooltip not visible initially
    expect(queryByText(/learning what works/i)).toBeNull();
    
    // Tap the bar
    fireEvent.press(getByTestId('device-memory-bar-touchable'));
    
    // Tooltip should appear (on Android)
    // Note: Platform-specific, may need platform mock
  });
});
```

**Pattern**:
- Use `render` from `jest/test-utils.tsx`
- Test different states
- Test user interactions
- Test accessibility

#### Integration Tests

Add tests to existing test files:

**File**: `src/screens/ModelsScreen/ModelCard/__tests__/ModelCard.test.tsx`

Add test case:

```typescript
it('displays memory requirement for downloaded models', () => {
  runInAction(() => {
    modelStore.models = [
      {...testModel, isDownloaded: true, ggufMetadata: {/* mock metadata */}},
    ];
  });

  const {getByTestId} = render(
    <ModelCard model={testModel} />,
    {withNavigation: true}
  );

  expect(getByTestId('memory-requirement')).toBeTruthy();
});
```

**File**: `src/screens/ModelsScreen/__tests__/ModelsScreen.test.tsx`

Add test case:

```typescript
it('displays device memory bar in header', () => {
  const {getByTestId} = render(<ModelsScreen />, {withNavigation: true});
  
  expect(getByTestId('device-memory-bar')).toBeTruthy();
});
```

### Risk Analysis

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Memory bar too wide on small screens | Medium | Low | Use responsive width, test on iPhone SE |
| Text not readable on progress bar | Low | Medium | Use high-contrast colors, test light/dark themes |
| Memory calculation slow (blocks render) | Low | Medium | Memoize calculations, use loading state |
| L10n string interpolation breaks | Low | High | Test all languages, add fallback |
| MobX reactivity doesn't update display | Medium | High | Ensure calibration signals are observed |

### Verification Checklist

- [ ] L10n strings added to `src/utils/l10n.ts`
- [ ] Memory display utilities created in `src/utils/memoryDisplay.ts`
- [ ] MemoryRequirement component created and exported
- [ ] DeviceMemoryBar component created and exported
- [ ] ModelCard shows memory requirement for downloaded models
- [ ] ModelFileCard shows memory requirement for all files
- [ ] ModelsScreen header shows device memory bar
- [ ] All components use MobX observer pattern
- [ ] All components have testID attributes
- [ ] Unit tests pass for new components
- [ ] Integration tests pass
- [ ] Visual inspection on light/dark themes
- [ ] Visual inspection on small screens (iPhone SE)
- [ ] Memory bar tap shows tooltip
- [ ] Memory displays update when models are loaded/released

### Notes for Implementer

1. **REUSE Existing Utilities**:
   - `formatBytes(bytes, decimals)` from `src/utils/formatters.ts` - for memory display
   - `Tooltip` from `react-native-paper` - already used in BenchResultCard, DetailsView
   - `getModelMemoryRequirement()` from `src/utils/memoryEstimator.ts` - for model memory
   - Do NOT create `formatMemoryGB` - use `formatBytes` instead

2. **MobX Reactivity**: Ensure components read `modelStore.largestSuccessfulLoad` and `modelStore.availableMemoryCeiling` in the render body (not just in useEffect) so MobX tracks them as dependencies.

3. **Performance**: The `getDeviceMemoryInfo()` and `getMemoryFitStatus()` functions are async. Cache results in useState and only recalculate when calibration changes.

4. **Testing**: Use fitStatus and memory overrides in props for easier testing without mocking device APIs.

5. **Styling**: Follow React Native Paper's theme system. Use `theme.colors.primary` for "fits", `theme.colors.error` for "tight" and "won't fit".

6. **Tooltip**: Use `Tooltip` from react-native-paper with `enterTouchDelay={0}` for immediate tap response.

7. **Vision Models**: Both ModelCard and ModelFileCard already resolve `projectionModelForCheck`. Reuse this for the MemoryRequirement component.

8. **Cold Start**: The display gracefully handles cold start (no calibration data) - `availableMemoryCeiling` initialization already provides fallback.

### Success Criteria

- Users can see memory requirements on every model card (downloaded and HF search)
- Users can see their device's usable memory at a glance
- Memory displays update in real-time when models are loaded/released
- All text is properly localized
- Components follow PocketPal's existing UI patterns
- Tests provide 80%+ coverage of new components

---

## Copilot Review Feedback (Post-PR)

### Feedback 1: Add guard for projection models in download completion

**Status**: VALID - Should fix
**Severity**: Low (log noise reduction)
**Location**: `src/store/ModelStore.ts` - onComplete callback in downloadHFModel

**Issue**: `onComplete` always calls `fetchAndPersistGGUFMetadata(model)`, but `loadMissingGGUFMetadata()` filters out `ModelType.PROJECTION`. This inconsistency causes unnecessary parsing and log noise for projection model downloads.

**Fix**:
```typescript
// In onComplete callback
if (model.modelType !== ModelType.PROJECTION) {
  await this.fetchAndPersistGGUFMetadata(model);
}
```

---

### Feedback 2: Circular dependency in useMemoryCheck

**Status**: VALID CONCERN - Low risk
**Severity**: Medium (potential Metro bundler issues)
**Location**: `src/hooks/useMemoryCheck.ts` ↔ `src/store/ModelStore.ts`

**Issue**:
- `useMemoryCheck.ts` imports `modelStore` from `../store`
- `ModelStore.ts` imports `hasEnoughMemory` from `../hooks/useMemoryCheck`

This circular dependency can cause partially-initialized modules in Metro/CommonJS.

**Analysis**:
In practice, this works because:
1. `modelStore` is a singleton that's instantiated after class definition
2. `hasEnoughMemory` is a function that's only called at runtime, not during module initialization

**Recommendation**: Keep as-is for now. Circular deps are common in React/MobX apps and this one is runtime-safe. Add a code comment documenting the intentional dependency.

---

### Feedback 3: MobX dependency tracking in useEffect

**Status**: PARTIALLY VALID - Monitor
**Severity**: Low
**Location**: `src/hooks/useMemoryCheck.ts` - useEffect dependency array

**Issue**: The model reference stays stable while fields like `ggufMetadata` can change, so useEffect won't re-run.

**Analysis**:
- We already track `calibrationCeiling` which triggers re-check when calibration changes
- Components using this hook are wrapped in `observer()` which handles MobX reactivity
- `ggufMetadata` updates happen after download, and new downloads pass fresh model objects

**Recommendation**: Monitor for stale warning issues. Current behavior is acceptable for main use cases.

---

### Feedback 4: getMemoryFitStatus unused

**Status**: VALID - Should clean up
**Severity**: Low (dead code)
**Location**: `src/utils/memoryDisplay.ts`

**Issue**: `getMemoryFitStatus` is exported but never used. `useMemoryCheck.ts` has its own `getMemoryFitDetails` function that duplicates this logic.

**Recommendation**: Remove unused `getMemoryFitStatus` from `memoryDisplay.ts`.

---

### Feedback 5: Unused #import <mach/mach.h>

**Status**: VALID - Should fix
**Severity**: Low (cleanup)
**Location**: `ios/PocketPal/HardwareInfoModule.mm`

**Issue**: The implementation uses `os_proc_available_memory()` from `<os/proc.h>`, not any functions from `<mach/mach.h>`.

**Fix**: Remove the unused import.

---

## Action Plan for Copilot Feedback

| # | Feedback | Action | Priority | Effort |
|---|----------|--------|----------|--------|
| 1 | Projection model guard | Add `modelType !== PROJECTION` check in onComplete | Medium | 5 min |
| 2 | Circular dependency | Add code comment documenting it's intentional | Low | 2 min |
| 3 | MobX deps tracking | Monitor for issues, no immediate action | Low | - |
| 4 | Unused getMemoryFitStatus | Remove from memoryDisplay.ts | Low | 5 min |
| 5 | Unused mach/mach.h | Remove import | Low | 2 min |

---

## Updated Changelog

| Date | Agent/Human | Change |
|------|-------------|--------|
| ... | ... | (previous entries) |
| 2026-01-30 | planner | Added implementation plan for "Display Memory Values to Users" feature |
| 2026-01-31 | human | Added Copilot review feedback analysis and action plan |

