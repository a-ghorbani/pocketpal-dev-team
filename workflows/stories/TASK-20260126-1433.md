# Story: Fix Memory Estimation for Model Loading by Adding Native getAvailableMemory() API

## Metadata
- **Task ID**: TASK-20260126-1433
- **Issue**: Action Tracker Item - "Memory Estimation for Model Loading"
- **Source**: action-tracker (P2 priority)
- **Complexity**: standard
- **Native Changes**: YES
- **Created**: 2026-01-26
- **Status**: draft

## Environment
- **Worktree**: `/Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433`
- **Branch**: `feature/TASK-20260126-1433`
- **Base**: `main`

---

## Progress Tracking

### Current Phase
**Phase 1**: `[X] Planning → [X] Approved → [X] Implementing → [X] Testing → [X] Reviewing → [X] PR Created`
**Phase 2**: `[X] Planning → [X] Approved → [X] Implementing → [X] Testing → [X] Reviewing → [X] PR Updated`

### Checkpoints (Updated by Agents)

| Checkpoint | Status | Agent | Commit | Notes |
|------------|--------|-------|--------|-------|
| Worktree created | DONE | orchestrator | - | |
| Story approved | DONE | human | - | |
| Step 1 complete | DONE | implementer | a4e3e41 | Android getAvailableMemory() |
| Step 2 complete | DONE | implementer | 85f65e4 | iOS getAvailableMemory() |
| Step 3 complete | DONE | implementer | 044dd9a | Update TurboModule spec |
| Step 4 complete | DONE | implementer | 3067109 | Update useMemoryCheck logic |
| Step 5 complete | DONE | implementer | d249574 | Update mocks |
| iOS header fix | DONE | implementer | 5016e1b | Fixed os/proc.h import |
| Tests verified | DONE | implementer | - | Existing tests pass |
| Platform builds | DONE | implementer | - | iOS and Android succeed |
| Tests written | DONE | tester | 355a850 | 3 new tests added |
| Review passed | DONE | reviewer | - | All checks pass |
| PR created | DONE | reviewer | - | PR #544 |
| **Phase 2** | | | | |
| Phase 2 planning | DONE | human | - | Hybrid memory tracking approach |
| Phase 2 approved | DONE | human | - | |
| Step 8: Add observable | DONE | implementer | df836c9 | loadedModelMemoryUsage added |
| Step 9: Measure delta | DONE | implementer | d03d4f5 | Memory before/after tracking |
| Step 10: Clear on release | DONE | implementer | 65176a4 | Clear in finally block |
| Step 11: Effective available | DONE | implementer | ed04082 | Calculate effective memory |
| Prettier fix | DONE | implementer | b46f15b | Formatting |
| Phase 2 tests | DONE | implementer | 1ad99e6 | 3 new tests added |
| mmproj memory fix | DONE | implementer | fbeb971 | Measure after multimodal init |
| Phase 2 review | DONE | reviewer | - | All checks pass |
| PR updated | DONE | reviewer | - | PR #544 updated |
| **Clean Redesign** | | | | |
| Clean design doc | DONE | human | - | FINAL DESIGN section added |
| memoryEstimator created | DONE | implementer | 8229ef1 | Single estimation function |
| GGUFMetadata added | DONE | implementer | 8229ef1 | Types updated |
| ModelStore calibration | DONE | implementer | 814a636 | Two variables + lifecycle |
| useMemoryCheck updated | DONE | implementer | c524c41 | Calibration-based logic |
| Tests updated | DONE | implementer | e7e5fb2 | 6/6 tests passing |
| Lint fixes | DONE | implementer | 7683418 | Prettier formatting |

### Last Agent Handoff
```yaml
from_agent: implementer
to_agent: human
timestamp: 2026-01-29T23:00:00Z
status: "Clean memory estimation design implemented and tested"
completed:
  - Created memoryEstimator.ts with single estimation function (commit 8229ef1)
  - Added GGUFMetadata interface to types.ts (commit 8229ef1)
  - Added two calibration variables to ModelStore (commit 814a636)
  - Implemented calibration lifecycle in ModelStore (commit 814a636)
  - Updated useMemoryCheck to use calibration (commit c524c41)
  - Updated mock store with calibration variables (commit c524c41)
  - Rewrote tests for calibration design (commit e7e5fb2)
  - Applied prettier formatting (commit 7683418)
  - All tests pass: 6/6 in useMemoryCheck.test.ts
  - Lint: PASS, TypeCheck: PASS
next_steps:
  - Human review of clean implementation
  - Verify approach aligns with FINAL DESIGN
  - Decide if ready for PR or needs additional work
blockers: []
context_for_next_agent: |
  Clean design implemented from scratch following FINAL DESIGN section.

  **What Was Implemented:**

  1. **Single Estimation Function** (memoryEstimator.ts):
     - getModelMemoryRequirement(model, projectionModel?, contextSettings?)
     - GGUF-based when metadata available: (Weights + KV Cache + Compute) × 1.1
     - Fallback when not available: (modelSize + mmProjSize) × 1.1
     - Same function used everywhere for consistency

  2. **Two Calibration Variables** (ModelStore):
     - availableMemoryCeiling: Set at startup, updated after release
     - largestSuccessfulLoad: Updated after successful load using estimator
     - Both persisted in AsyncStorage

  3. **Memory Check Logic** (useMemoryCheck):
     - ceiling = max(largestSuccessfulLoad, availableMemoryCeiling)
     - safeCeiling = ceiling × 0.9 (10% safety margin)
     - Cold start fallback: totalRAM × 0.5

  4. **GGUF Metadata Support**:
     - GGUFMetadata interface added to types
     - ggufMetadata field added to Model
     - fetchAndPersistGGUFMetadata() called after download completes

  **Key Principles Followed:**
  - ONE function for estimation (no scattered logic)
  - TWO variables (simple, no complex branching)
  - CONSISTENCY (same formula for checking AND tracking)
  - RELIABILITY (getAvailableMemory only at clean states)
  - NO memory delta measurements (use estimator instead)

  **No Native Changes:**
  Phase 1 native API (getAvailableMemory) already exists.
  This implementation is pure JavaScript.
```

---

## Context (For Recovery After Context Reset)

> **If you're an agent resuming work on this story:**
> 1. Read the "Progress Tracking" section above
> 2. Check `git log` in the worktree for commits
> 3. Read the "Last Agent Handoff" section
> 4. Continue from the next incomplete checkpoint

### Background

PocketPal AI currently uses a heuristic formula to estimate available memory for model loading:
```typescript
const availableMemory = Math.min(totalMemoryGB * 0.65, totalMemoryGB - 1.2);
```

This formula was created based on limited device testing and overestimates available memory on high-end devices. For example:
- **Pixel 9 (12GB RAM)**: Formula estimates 7.7GB available, but actual is 6.3GB (22% overestimate)
- **OnePlus 6 (8GB RAM)**: Formula estimates 5.2GB available, actual is ~4.8GB (8% overestimate)

This causes Out-Of-Memory (OOM) crashes, which account for ~10% of total app crashes (18 OOM crashes reported with signatures like `lm_ggml_gallocr_alloc_graph`, `lm_ggml_abort`).

User feedback validates this: "Unsuitable Model Size Recommendations For Device" mentioned 3x in recent feedback.

The solution is to query the OS directly for **actual available memory** instead of estimating.

### Current State

**File**: `src/hooks/useMemoryCheck.ts`
- Imports `DeviceInfo.getTotalMemory()` from `react-native-device-info`
- Calculates `availableMemory` using heuristic: `Math.min(totalMemoryGB * 0.65, totalMemoryGB - 1.2)`
- Exports `hasEnoughMemory(modelSize, isMultimodal)` function
- Used by `ModelStore.initContext()` to check if model can load (line 1031 in ModelStore.ts)

**File**: `android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt`
- TurboModule that provides `getCPUInfo()`, `getGPUInfo()`, `getChipset()`
- Does NOT currently provide memory information
- Follows React Native TurboModule pattern (extends `NativeHardwareInfoSpec`)

**File**: `ios/PocketPal/HardwareInfoModule.mm`
- Objective-C++ module that provides `getCPUInfo()`, `getGPUInfo()`
- Does NOT currently provide memory information
- Uses `RCT_EXPORT_METHOD` macros

**File**: `src/specs/NativeHardwareInfo.ts`
- TurboModule spec defining the interface for native modules
- Exports `CPUInfo`, `GPUInfo` interfaces
- Does NOT include memory methods

**File**: `jest/setup.ts` (lines 56-75)
- Mocks `NativeHardwareInfo` globally for tests
- Returns mock data for `getCPUInfo()` and `getGPUInfo()`

**File**: `__mocks__/external/react-native-device-info.js`
- Mocks `DeviceInfo.getTotalMemory()` and `getUsedMemory()` 
- Uses fixtures from `jest/fixtures/device-info.ts`

### Target State

After this change:
1. `NativeHardwareInfo` TurboModule exposes `getAvailableMemory(): Promise<number>` method
2. Android implementation reads `/proc/meminfo` MemAvailable field
3. iOS implementation uses `os_proc_available_memory()` API
4. `useMemoryCheck.ts` uses native available memory with 10% safety margin + fallback to heuristic
5. Tests updated to mock `getAvailableMemory()` 
6. Platform builds succeed (pod install, iOS/Android release builds)

---

## Requirements

### Functional
1. [MUST] Android `HardwareInfoModule.kt` implements `getAvailableMemory()` using `ActivityManager.getMemoryInfo()` (official Android API)
2. [MUST] iOS `HardwareInfoModule.mm` implements `getAvailableMemory()` using `os_proc_available_memory()` API
3. [MUST] `NativeHardwareInfo.ts` TurboModule spec includes `getAvailableMemory(): Promise<number>` method
4. [MUST] `useMemoryCheck.ts` calls native `getAvailableMemory()` instead of calculating heuristic
5. [MUST] Apply 10% safety margin to available memory (return `availableMemory * 0.9`)
6. [MUST] Fallback to current heuristic if native API fails (for robustness)
7. [MUST] Update test mocks in `jest/setup.ts` and `__mocks__/external/react-native-device-info.js`
8. [SHOULD] Log actual vs estimated memory in debug builds for validation

### Non-Functional
- **Performance**: Native call should be fast (<10ms), cached if called frequently
- **Compatibility**: Must work on Android API 21+ and iOS 14+
- **Reliability**: Must not crash if API unavailable (graceful fallback)
- **Testing**: Must maintain 60%+ test coverage

### Platform Verification (NATIVE_CHANGES=YES)
- [ ] `pod install` succeeds
- [ ] iOS Release build succeeds
- [ ] Android Release build succeeds
- [ ] `ios/Podfile.lock` changes committed (if any)

---

## Acceptance Criteria

- [ ] Android uses `ActivityManager.getMemoryInfo()` and returns `availMem` in bytes
- [ ] iOS uses `os_proc_available_memory()` and returns bytes
- [ ] `getAvailableMemory()` returns actual available memory in bytes
- [ ] `useMemoryCheck.ts` applies 10% safety margin
- [ ] If native call fails, falls back to heuristic (no crash)
- [ ] All existing tests pass
- [ ] New tests added for `getAvailableMemory()` 
- [ ] Coverage >= 60%
- [ ] Platform builds succeed (iOS + Android Release)
- [ ] Manual testing on real device shows accurate memory estimation

---

## Phase 2: Track Loaded Model Memory (Enhancement)

### Problem Identified Post-Implementation

The Phase 1 implementation queries current available memory, but this creates an issue when a model is already loaded:

| Scenario | Available Memory | Loaded Model | Check for New 4GB Model |
|----------|------------------|--------------|-------------------------|
| No model | 5GB | - | ✅ Pass (5GB > 4GB needed) |
| 2GB model loaded | 3GB | Using ~2GB | ❌ Fail - but would fit if current released! |

**Issue**: User sees "not enough memory" warning when switching models, even though the new model would fit after releasing the current one.

### Solution: Hybrid Memory Tracking (Approach 4)

1. **Track actual memory delta** when loading a model (before/after measurement)
2. **Fallback to estimate** if delta unavailable (app restart, first check)
3. **Add back loaded model memory** when checking if new model fits

### Phase 2 Requirements

#### Functional
1. [MUST] Track actual memory consumption when model loads (before - after)
2. [MUST] Store `loadedModelMemoryUsage` in ModelStore as observable
3. [MUST] Clear `loadedModelMemoryUsage` when model is released
4. [MUST] In `hasEnoughMemory()`, calculate effective available as: `nativeAvailable + loadedModelMemoryUsage`
5. [MUST] Fallback to `memoryRequirementEstimate()` of current model if actual delta unavailable
6. [SHOULD] Persist memory usage to handle app restart (optional, estimate fallback is acceptable)

#### Non-Functional
- Must not add noticeable latency to model loading
- Memory tracking should work even if interrupted (fallback to estimate)

### Phase 2 Acceptance Criteria

- [ ] ModelStore has `loadedModelMemoryUsage: number | undefined` observable
- [ ] Memory delta measured in `initContext()` (before load - after load)
- [ ] `loadedModelMemoryUsage` cleared in `releaseContext()`
- [ ] `hasEnoughMemory()` adds back current model memory to available
- [ ] Fallback to estimate when `loadedModelMemoryUsage` is undefined
- [ ] User can switch from 2GB model to 4GB model when 5GB total available
- [ ] Tests verify effective available calculation
- [ ] Tests verify fallback behavior

### Phase 2 Affected Files

| File | Action | Reason |
|------|--------|--------|
| `src/store/ModelStore.ts` | MODIFY | Add loadedModelMemoryUsage tracking |
| `src/hooks/useMemoryCheck.ts` | MODIFY | Add effective available calculation |
| `src/hooks/__tests__/useMemoryCheck.test.ts` | MODIFY | Test effective available + fallback |

---

## Affected Files

| File | Action | Reason | Status |
|------|--------|--------|--------|
| `android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt` | MODIFY | Add getAvailableMemory() method | PENDING |
| `ios/PocketPal/HardwareInfoModule.mm` | MODIFY | Add getAvailableMemory() method | PENDING |
| `src/specs/NativeHardwareInfo.ts` | MODIFY | Add getAvailableMemory() to Spec interface | PENDING |
| `src/hooks/useMemoryCheck.ts` | MODIFY | Use native API instead of heuristic | PENDING |
| `src/hooks/__tests__/useMemoryCheck.test.ts` | MODIFY | Update tests to verify native API usage | PENDING |
| `jest/setup.ts` | MODIFY | Add getAvailableMemory() to NativeHardwareInfo mock | PENDING |
| `__mocks__/external/react-native-device-info.js` | MODIFY | (Optional) Add getAvailableMemory() if needed | PENDING |
| `jest/fixtures/device-info.ts` | MODIFY | Add availableMemory fixture data | PENDING |
| `ios/Podfile.lock` | UPDATE | May change after pod install | PENDING |

---

## Implementation Plan

### Step 1: Add getAvailableMemory() to Android Native Module
**Files**: `android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Add import for `android.app.ActivityManager` and `android.content.Context`
- [ ] Add `getAvailableMemory(promise: Promise)` method to `HardwareInfoModule` class
- [ ] Override method from `NativeHardwareInfoSpec` (will be added in Step 3)
- [ ] Use `ActivityManager.getMemoryInfo()` to get available memory (official Android API)
- [ ] Resolve promise with `memInfo.availMem` as Double (already in bytes)
- [ ] Handle errors gracefully (reject promise with error message)

**Why ActivityManager instead of /proc/meminfo:**
- Official Android API - works reliably across all devices and Android versions
- No SELinux permission issues on newer Android or OEM ROMs
- Abstracts away kernel differences (MemAvailable wasn't added until Linux 3.14)
- Provides bonus `lowMemory` flag for free
- Same underlying data: `availMem` equals `MemFree + Cached` from /proc/meminfo

**Pattern Reference**: See `HardwareInfoModule.kt:133-197` (getCPUInfo method) for promise pattern

**Code Guidance**:
```kotlin
// Add imports at top of file
import android.app.ActivityManager
import android.content.Context

// Add method in HardwareInfoModule class
override fun getAvailableMemory(promise: Promise) {
  try {
    val activityManager = reactApplicationContext
      .getSystemService(Context.ACTIVITY_SERVICE) as ActivityManager
    val memInfo = ActivityManager.MemoryInfo()
    activityManager.getMemoryInfo(memInfo)

    // availMem is already in bytes
    promise.resolve(memInfo.availMem.toDouble())
  } catch (e: Exception) {
    promise.reject("ERROR", e.message)
  }
}
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn lint
yarn typecheck
```

### Step 2: Add getAvailableMemory() to iOS Native Module
**Files**: `ios/PocketPal/HardwareInfoModule.mm`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Import `<mach/mach.h>` and `<sys/sysctl.h>` headers at top
- [ ] Add `RCT_EXPORT_METHOD(getAvailableMemory:...)` 
- [ ] Call `os_proc_available_memory()` to get available memory in bytes
- [ ] Resolve promise with bytes as NSNumber
- [ ] Handle errors gracefully (catch exceptions, reject promise)

**Pattern Reference**: See `HardwareInfoModule.mm:12-26` (getCPUInfo method) for promise pattern

**Code Guidance**:
```objective-c
#import <React/RCTBridgeModule.h>
#import <UIKit/UIKit.h>
#import <Metal/Metal.h>
#import <mach/mach.h>
#import <sys/sysctl.h>

// ... existing code ...

RCT_EXPORT_METHOD(getAvailableMemory:(RCTPromiseResolveBlock)resolve
                  rejecter:(RCTPromiseRejectBlock)reject)
{
  @try {
    // Get available memory using os_proc_available_memory()
    uint64_t availableMemory = os_proc_available_memory();
    
    if (availableMemory == 0) {
      reject(@"error_getting_available_memory", @"Could not retrieve available memory", nil);
      return;
    }
    
    resolve(@(availableMemory));
  } @catch (NSException *exception) {
    reject(@"error_getting_available_memory", @"Could not retrieve available memory", nil);
  }
}
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn lint
yarn typecheck
```

### Step 3: Update TurboModule Spec
**Files**: `src/specs/NativeHardwareInfo.ts`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Add `getAvailableMemory(): Promise<number>` to `Spec` interface (line 33-37)
- [ ] Add JSDoc comment explaining return value (bytes)

**Pattern Reference**: See `NativeHardwareInfo.ts:34-36` (getCPUInfo, getGPUInfo methods)

**Code Guidance**:
```typescript
export interface Spec extends TurboModule {
  getCPUInfo(): Promise<CPUInfo>;
  getGPUInfo(): Promise<GPUInfo>;
  getChipset?(): Promise<string>; // Android only
  /**
   * Get available memory in bytes from the operating system.
   * - Android: Reads MemAvailable from /proc/meminfo
   * - iOS: Uses os_proc_available_memory()
   * @returns Promise<number> Available memory in bytes
   */
  getAvailableMemory(): Promise<number>;
}
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn typecheck
```

### Step 4: Update useMemoryCheck to Use Native API
**Files**: `src/hooks/useMemoryCheck.ts`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Import `NativeHardwareInfo` at top: `import NativeHardwareInfo from '../specs/NativeHardwareInfo';`
- [ ] Replace `hasEnoughMemory()` implementation (lines 20-30):
  - Call `NativeHardwareInfo.getAvailableMemory()` to get actual available memory
  - Apply 10% safety margin: `actualAvailable * 0.9`
  - If native call fails, fallback to current heuristic formula
  - Add console.log in debug mode to compare actual vs heuristic
- [ ] Keep `memoryRequirementEstimate()` unchanged (model requirement calculation is correct)

**Pattern Reference**: See `src/utils/deviceCapabilities.ts:85-88` for calling NativeHardwareInfo pattern

**Code Guidance**:
```typescript
export const hasEnoughMemory = async (
  modelSize: number,
  isMultimodal = false,
): Promise<boolean> => {
  let availableMemoryGB: number;

  try {
    // Try native API first (actual available memory from OS)
    const availableBytes = await NativeHardwareInfo.getAvailableMemory();
    // Apply 10% safety margin
    const safeAvailableBytes = availableBytes * 0.9;
    availableMemoryGB = safeAvailableBytes / 1000 / 1000 / 1000;
    
    // Debug logging to validate accuracy
    if (__DEV__) {
      const totalMemory = await DeviceInfo.getTotalMemory();
      const totalMemoryGB = totalMemory / 1000 / 1000 / 1000;
      const heuristicAvailable = Math.min(totalMemoryGB * 0.65, totalMemoryGB - 1.2);
      console.log('[MemoryCheck] Actual available:', availableMemoryGB.toFixed(2), 'GB');
      console.log('[MemoryCheck] Heuristic estimate:', heuristicAvailable.toFixed(2), 'GB');
      console.log('[MemoryCheck] Difference:', ((heuristicAvailable - availableMemoryGB) / availableMemoryGB * 100).toFixed(1), '%');
    }
  } catch (error) {
    // Fallback to heuristic if native API fails
    console.warn('[MemoryCheck] Native API failed, using heuristic:', error);
    const totalMemory = await DeviceInfo.getTotalMemory();
    const totalMemoryGB = totalMemory / 1000 / 1000 / 1000;
    availableMemoryGB = Math.min(totalMemoryGB * 0.65, totalMemoryGB - 1.2);
  }

  const memoryRequirement = memoryRequirementEstimate(modelSize, isMultimodal);
  return memoryRequirement <= availableMemoryGB;
};
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn lint
yarn typecheck
yarn test --findRelatedTests src/hooks/useMemoryCheck.ts
```

### Step 5: Update Test Mocks
**Files**: `jest/setup.ts`, `jest/fixtures/device-info.ts`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] In `jest/setup.ts` (lines 56-75), add `getAvailableMemory` to NativeHardwareInfo mock
- [ ] Mock should return realistic value: ~2.5GB available for 4GB total device
- [ ] In `jest/fixtures/device-info.ts`, add `availableMemory: 2.5 * 1000 ** 3` to fixture
- [ ] Ensure mock is consistent with totalMemory (availableMemory < totalMemory)

**Pattern Reference**: See `jest/setup.ts:60-74` for existing NativeHardwareInfo mock structure

**Code Guidance**:
```typescript
// jest/setup.ts (add to existing mock around line 74)
jest.mock('../src/specs/NativeHardwareInfo', () => ({
  __esModule: true,
  default: {
    getCPUInfo: jest.fn(() => Promise.resolve({cores: 4})),
    getGPUInfo: jest.fn(() =>
      Promise.resolve({
        renderer: 'Mock GPU',
        vendor: 'Mock Vendor',
        version: 'Mock Version',
        hasAdreno: false,
        hasMali: false,
        hasPowerVR: false,
        supportsOpenCL: false,
        gpuType: 'Mock GPU',
      }),
    ),
    getChipset: jest.fn(() => Promise.resolve('Mock Chipset')),
    getAvailableMemory: jest.fn(() => Promise.resolve(2.5 * 1000 * 1000 * 1000)), // 2.5GB
  },
}));
```

```typescript
// jest/fixtures/device-info.ts (add to deviceInfo object)
export const deviceInfo = {
  freeDiskStorage: 8 * 1000 ** 3,
  totalMemory: 4 * 1000 ** 3,
  usedMemory: 2 * 1000 ** 3,
  availableMemory: 2.5 * 1000 ** 3, // More accurate than totalMemory - usedMemory
  version: '1.0.0',
  buildNumber: '1',
};
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn test
```

### Step 6: Update useMemoryCheck Tests
**Files**: `src/hooks/__tests__/useMemoryCheck.test.ts`
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Add test: "should use native getAvailableMemory() API"
- [ ] Add test: "should fallback to heuristic if native API fails"
- [ ] Add test: "should apply 10% safety margin to native available memory"
- [ ] Verify existing tests still pass with new implementation

**Pattern Reference**: See `useMemoryCheck.test.ts:58-92` for error handling test pattern

**Code Guidance**:
```typescript
import NativeHardwareInfo from '../../specs/NativeHardwareInfo';

// Add after existing tests
it('should use native getAvailableMemory() API', async () => {
  // Mock native API to return 3GB available
  (NativeHardwareInfo.getAvailableMemory as jest.Mock).mockResolvedValue(
    3 * 1000 * 1000 * 1000
  );

  const {result, waitForNextUpdate} = renderHook(() =>
    useMemoryCheck(localModel.size),
  );

  try {
    await waitForNextUpdate();
  } catch {
    // Ignoring timeout
  }

  // Should pass because model is small and 3GB * 0.9 = 2.7GB available
  expect(result.current).toEqual({
    memoryWarning: '',
    shortMemoryWarning: '',
    multimodalWarning: '',
  });
  
  expect(NativeHardwareInfo.getAvailableMemory).toHaveBeenCalled();
});

it('should fallback to heuristic if native API fails', async () => {
  (NativeHardwareInfo.getAvailableMemory as jest.Mock).mockRejectedValue(
    new Error('Native API error')
  );

  const consoleWarnSpy = jest.spyOn(console, 'warn').mockImplementation(() => {});

  const {result, waitForNextUpdate} = renderHook(() =>
    useMemoryCheck(localModel.size),
  );

  try {
    await waitForNextUpdate();
  } catch {
    // Ignoring timeout
  }

  // Should still work with fallback
  expect(result.current.memoryWarning).toBe('');
  expect(consoleWarnSpy).toHaveBeenCalledWith(
    expect.stringContaining('Native API failed'),
    expect.any(Error)
  );

  consoleWarnSpy.mockRestore();
});

it('should apply 10% safety margin to available memory', async () => {
  // Mock exactly enough memory (3.5GB available)
  // Model needs ~3.2GB (after margin it's 3.15GB available)
  const modelSize = 3.5 * 1000 * 1000 * 1000; // 3.5GB model
  (NativeHardwareInfo.getAvailableMemory as jest.Mock).mockResolvedValue(
    3.5 * 1000 * 1000 * 1000
  );

  const {result, waitForNextUpdate} = renderHook(() =>
    useMemoryCheck(modelSize),
  );

  try {
    await waitForNextUpdate();
  } catch {
    // Ignoring timeout
  }

  // After 10% margin, 3.5GB becomes 3.15GB available
  // Model requirement: 0.43 + (0.92 * 3.5) = 3.65GB
  // Should show warning because 3.65GB > 3.15GB
  expect(result.current.memoryWarning).toBe(l10n.en.memory.warning);
});
```

**Verification**:
```bash
cd "${WORKTREE_PATH}"
yarn test src/hooks/__tests__/useMemoryCheck.test.ts
```

### Step 7: Platform Verification (NATIVE_CHANGES=YES)
**Status**: `PENDING`
**Commit**: [commit hash when done]

**Change**:
- [ ] Run `cd ios && pod install && cd ..`
- [ ] Commit Podfile.lock changes (if any)
- [ ] Run `yarn ios --configuration Release` (verify iOS build succeeds)
- [ ] Run `yarn android --variant=release` (verify Android build succeeds)
- [ ] Test on real device (optional but recommended for validation)

**Verification**:
```bash
cd "${WORKTREE_PATH}"

# iOS
cd ios && pod install && cd ..
yarn ios --configuration Release

# Android
yarn android --variant=release

# Manual test on device
# 1. Open app on physical device
# 2. Check logs for "[MemoryCheck] Actual available" debug output
# 3. Try loading a model near memory limit
# 4. Verify no OOM crash
```

---

## Test Requirements

### Unit Tests
| Test Case | File | Priority | Status |
|-----------|------|----------|--------|
| Should use native getAvailableMemory() API | `useMemoryCheck.test.ts` | MUST | PENDING |
| Should fallback to heuristic if native API fails | `useMemoryCheck.test.ts` | MUST | PENDING |
| Should apply 10% safety margin | `useMemoryCheck.test.ts` | MUST | PENDING |
| Existing: returns no warning when safe | `useMemoryCheck.test.ts` | MUST | PENDING |
| Existing: returns warning when unsafe | `useMemoryCheck.test.ts` | MUST | PENDING |
| Existing: handles errors gracefully | `useMemoryCheck.test.ts` | MUST | PENDING |

### Integration Tests
| Test Case | File | Priority | Status |
|-----------|------|----------|--------|
| ModelStore.initContext() uses new memory check | Manual | SHOULD | PENDING |

### Manual Testing
- [ ] Test on Android device (Pixel 9 preferred for validation)
- [ ] Verify debug logs show "Actual available" vs "Heuristic estimate"
- [ ] Load model near memory limit - should not crash
- [ ] Compare memory estimation accuracy to heuristic

---

## Coding Standards

### Testing Infrastructure (CRITICAL)
```
# Read these BEFORE writing tests:
${WORKTREE_PATH}/jest/setup.ts      # Global mocks
${WORKTREE_PATH}/jest/test-utils.tsx # Custom render
${WORKTREE_PATH}/__mocks__/stores/  # Mock stores

# DO NOT mock stores inline - they're globally mocked
# Use runInAction() for MobX state changes
# Import render from jest/test-utils, NOT @testing-library/react-native
```

### Patterns to Follow
- **Native Modules**: Follow existing TurboModule pattern in `HardwareInfoModule.kt` and `HardwareInfoModule.mm`
- **Hooks**: Follow existing hook pattern in `useMemoryCheck.ts` (async, error handling)
- **Types**: Strict TypeScript, avoid `any`
- **Error Handling**: Always have try-catch with graceful fallback

### Commit Format (enforced by commitlint)
```
type(scope): subject
```

**Rules**:
- Header max: 100 chars total
- Types allowed: `feat`, `fix`, `docs`, `chore` (only these 4)
- No Co-Authored-By needed
- Keep it short and clear

**Examples**:
```
feat(memory): add native getAvailableMemory() API
fix(memory): fallback to heuristic if native API fails
chore(native): update HardwareInfoModule with memory API
```

### Naming Conventions
- Native methods: camelCase (`getAvailableMemory`)
- TypeScript: camelCase for functions, PascalCase for types/interfaces
- Test files: Match source file name with `.test.ts` suffix

---

## Reference Code

### Pattern Example: Android File Reading
**File**: `android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt`
**Lines**: 133-197
```kotlin
override fun getCPUInfo(promise: Promise) {
  try {
    val cpuInfo = Arguments.createMap()
    cpuInfo.putInt("cores", Runtime.getRuntime().availableProcessors())

    val processors = Arguments.createArray()
    val features = mutableSetOf<String>()
    val cpuInfoFile = File("/proc/cpuinfo")

    if (cpuInfoFile.exists()) {
      val cpuInfoLines = cpuInfoFile.readLines()
      var currentProcessor = Arguments.createMap()
      var hasData = false

      for (line in cpuInfoLines) {
        // ... parse lines ...
      }
    }
    
    promise.resolve(cpuInfo)
  } catch (e: Exception) {
    promise.reject("ERROR", e.message)
  }
}
```

### Pattern Example: iOS Promise Method
**File**: `ios/PocketPal/HardwareInfoModule.mm`
**Lines**: 12-26
```objective-c
RCT_EXPORT_METHOD(getCPUInfo:(RCTPromiseResolveBlock)resolve
                  rejecter:(RCTPromiseRejectBlock)reject)
{
  @try {
    NSUInteger numberOfCPUCores = [[NSProcessInfo processInfo] activeProcessorCount];

    NSDictionary *result = @{
      @"cores": @(numberOfCPUCores)
    };

    resolve(result);
  } @catch (NSException *exception) {
    reject(@"error_getting_cpu_info", @"Could not retrieve CPU info", nil);
  }
}
```

### Pattern Example: Calling Native API
**File**: `src/utils/deviceCapabilities.ts`
**Lines**: 85-88
```typescript
const [gpuInfo, cpuInfo] = await Promise.all([
  NativeHardwareInfo.getGPUInfo(),
  NativeHardwareInfo.getCPUInfo(),
]);
```

---

## Dependencies

### Blocked By
- None

### Blocks
- None (standalone improvement)

---

## Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| `ActivityManager.getMemoryInfo()` unavailable | Very Low | Medium | API available since Android API 1; fallback to heuristic |
| `os_proc_available_memory()` requires iOS 13+ | Low | Low | PocketPal already targets iOS 14+, documented in code |
| Native API returns incorrect value | Low | High | 10% safety margin + debug logging for validation |
| Performance overhead of API call | Very Low | Low | ActivityManager API is fast, only called once per model load |
| Breaking existing tests | Medium | Medium | Update mocks comprehensively in Step 5 |
| iOS build fails after adding new method | Medium | Medium | Follow exact TurboModule codegen pattern, test builds in Step 7 |
| Android build fails with Kotlin compilation error | Medium | Medium | Follow existing method patterns in HardwareInfoModule.kt |

---

## Open Questions

### For Human
- [ ] Should we add caching for `getAvailableMemory()` to reduce native calls? (Current: called once per model load, probably fine)
- [ ] Should we expose this API to users in DeviceInfoCard screen? (Current: internal only)
- [ ] Should we collect telemetry on actual vs heuristic memory to validate improvement? (Analytics)

### Resolved
- [x] **Which Android API to use?** → Use `ActivityManager.getMemoryInfo()` instead of `/proc/meminfo`
  - Official Android API, works reliably across all devices
  - No SELinux permission issues on newer Android or OEM ROMs
  - `availMem` equals `MemFree + Cached` from /proc/meminfo (same data)
  - Sources: [Android Developers](https://developer.android.com/reference/android/app/ActivityManager.MemoryInfo), [Memory Management Guide](https://developer.android.com/topic/performance/memory)

---

## Agent Reports

### Planner Report
```
Research completed: 2026-01-26
Agent: pocketpal-planner

Findings:
1. Current memory estimation in useMemoryCheck.ts uses heuristic formula
2. Native modules exist for CPU/GPU info, but not memory
3. iOS has os_proc_available_memory() API available (iOS 12+, PocketPal targets 14+)
4. TurboModule pattern requires: Spec update -> Native implementation -> JS usage
5. Tests mock NativeHardwareInfo globally in jest/setup.ts
6. ModelStore.initContext() calls hasEnoughMemory() at line 1031

Android API Research (Updated):
- Initially planned to use /proc/meminfo (Linux kernel interface)
- After research, switched to ActivityManager.getMemoryInfo() because:
  * Official Android API - works reliably across all devices
  * No SELinux permission issues on newer Android or OEM ROMs
  * Abstracts kernel differences (MemAvailable added in Linux 3.14)
  * Same data: availMem equals MemFree + Cached from /proc/meminfo
  * Sources: developer.android.com/reference/android/app/ActivityManager.MemoryInfo

Key files researched:
- android/app/src/main/java/com/pocketpalai/HardwareInfoModule.kt (198 lines)
- ios/PocketPal/HardwareInfoModule.mm (63 lines)
- src/specs/NativeHardwareInfo.ts (40 lines)
- src/hooks/useMemoryCheck.ts (74 lines)
- jest/setup.ts (mocking patterns)

Implementation approach:
1. Add native methods (Android: ActivityManager, iOS: os_proc_available_memory)
2. Update TurboModule spec
3. Update JS logic with fallback
4. Update test mocks
5. Platform builds verification

Risks: Minimal - fallback to heuristic ensures no regression. Safety margin prevents over-estimation.

Story complexity: Standard (requires native code + tests)
```

### Implementation Report
```
Date: 2026-01-26
Agent: pocketpal-implementer

Environment:
- Task ID: TASK-20260126-1433
- Worktree: /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
- Branch: feature/TASK-20260126-1433

Story: Fix Memory Estimation for Model Loading by Adding Native getAvailableMemory() API

Status: COMPLETE

Changes Made:

| File | Change | Commit |
|------|--------|--------|
| android/app/.../HardwareInfoModule.kt | Added getAvailableMemory() using ActivityManager | a4e3e41 |
| ios/PocketPal/HardwareInfoModule.mm | Added getAvailableMemory() using os_proc_available_memory() | 85f65e4 |
| src/specs/NativeHardwareInfo.ts | Added getAvailableMemory() to Spec interface | 044dd9a |
| src/hooks/useMemoryCheck.ts | Updated to use native API with 10% margin + fallback | 3067109 |
| jest/setup.ts | Added getAvailableMemory mock (3GB) | d249574 |
| jest/fixtures/device-info.ts | Added availableMemory: 3GB | d249574 |
| ios/PocketPal/HardwareInfoModule.mm | Fixed header to use <os/proc.h> | 5016e1b |

Implementation Details:
1. Android: Uses ActivityManager.getMemoryInfo().availMem - official API, no permission issues
2. iOS: Uses os_proc_available_memory() from <os/proc.h> header
3. JavaScript: Applies 10% safety margin, falls back to heuristic on error
4. Debug logging added to compare actual vs heuristic in __DEV__ mode
5. All existing tests pass with updated mocks

Deviations from Plan:
- Android: Used ActivityManager.getMemoryInfo() instead of /proc/meminfo
  Reason: Official API, more reliable, no SELinux permission issues on newer Android
- iOS: Fixed header from <sys/sysctl.h> to <os/proc.h>
  Reason: os_proc_available_memory() requires <os/proc.h>
- Mock memory: Increased from 2.5GB to 3GB
  Reason: After 10% margin (2.7GB), needed to fit 2GB test model (requires 2.27GB)

Verification Results:
- Lint: PASS (only pre-existing warnings in other files)
- TypeCheck: PASS
- Tests: PASS (3/3 tests in useMemoryCheck.test.ts)
- Pod Install: PASS (no Podfile.lock changes)
- iOS Build: PASS (Release configuration, iphonesimulator)
- Android Build: PASS (assembleRelease)

Notes for Tester:
- New tests needed for:
  1. Native API returns correct value
  2. Fallback works when native API fails
  3. 10% safety margin is applied correctly
- Mock is configured at 3GB available memory
- NativeHardwareInfo.getAvailableMemory is mocked in jest/setup.ts
- Test pattern should follow existing useMemoryCheck tests

Blockers: None
```

### Test Report
```
[Filled by tester after tests written]
```

### Phase 2 Implementation Report
```
Date: 2026-01-26
Agent: pocketpal-implementer

Environment:
- Task ID: TASK-20260126-1433
- Worktree: /Users/aghorbani/codes/pocketpal-dev-team/worktrees/TASK-20260126-1433
- Branch: feature/TASK-20260126-1433

Story: Phase 2 - Track Loaded Model Memory for Accurate Model Switching

Status: COMPLETE

Changes Made:

| File | Change | Commit |
|------|--------|--------|
| src/store/ModelStore.ts | Added loadedModelMemoryUsage observable | df836c9 |
| src/store/ModelStore.ts | Import NativeHardwareInfo | d03d4f5 |
| src/store/ModelStore.ts | Measure memory before/after model load | d03d4f5 |
| src/store/ModelStore.ts | Clear loadedModelMemoryUsage on release | 65176a4 |
| src/hooks/useMemoryCheck.ts | Import modelStore | ed04082 |
| src/hooks/useMemoryCheck.ts | Calculate effective available memory | ed04082 |
| src/store/ModelStore.ts | Prettier formatting fix | b46f15b |
| __mocks__/stores/modelStore.ts | Add loadedModelMemoryUsage to mock | 1ad99e6 |
| __mocks__/stores/modelStore.ts | Add isMultimodalActive to mock | 1ad99e6 |
| src/hooks/__tests__/useMemoryCheck.test.ts | Add 3 Phase 2 tests | 1ad99e6 |
| src/store/ModelStore.ts | Move memory measurement after initMultimodal | fbeb971 |

Implementation Details:
1. ModelStore now tracks actual memory consumption during model load
2. Measures memory before initLlama() and AFTER initMultimodal()
3. Stores delta in loadedModelMemoryUsage observable (bytes)
4. Only stores positive deltas (sanity check)
5. Clears tracking when context is released (finally block)
6. hasEnoughMemory() now calculates effective available memory:
   - If loadedModelMemoryUsage exists: uses actual measured value
   - Else if activeModel exists: uses estimated memory requirement
   - Else: uses raw available memory (no adjustment)
7. Debug logging added to show measured vs estimated memory

mmproj Fix (commit fbeb971):
- Original measurement was taken right after initLlama(), missing mmproj
- For vision models, mmproj adds ~1.8GB which wasn't captured
- Fixed by moving measurement to AFTER initMultimodal() block
- Now delta captures: main model + mmproj (if multimodal enabled)
- Updated debug log: "Memory consumed (including mmproj if multimodal)"

Deviations from Plan:
- Added post-implementation fix for mmproj memory tracking (fbeb971)

Verification Results:
- Lint: PASS (only pre-existing warnings)
- TypeCheck: PASS
- Tests: PASS (all 1361 tests, 9/9 in useMemoryCheck.test.ts)
- Coverage: Tests cover all 3 scenarios (actual, estimate, no model)
- No platform builds needed (JS-only changes)

Notes for Reviewer:
- Phase 2 is pure JavaScript - no native changes
- All new code paths are tested
- Memory tracking is defensive (undefined fallback)
- Debug logs will help validate accuracy in production
- Effective available calculation enables better model switching UX
  (Users won't see "not enough memory" when switching models)

Blockers: None
```

### Review Report
```
[Filled by reviewer after review]
```

---

## Phase 3: Improved Memory Estimation (Research & Design)

### Problem Observed Post-Phase-2

Testing revealed that Android's `availMem` (and `/proc/meminfo` `MemAvailable`) are **inherently volatile**:

```
# Measurements taken within minutes on same device:
MemAvailable: 2824356 kB (~2.69 GB)
MemAvailable: 3286816 kB (~3.13 GB)
MemAvailable: 3054040 kB (~2.91 GB)
```

**Key observation**: When memory pressure is applied (model loaded), Android makes MORE memory available than the API reported beforehand:
- Passive check (no pressure): ~2.83 GB available
- After release, before new load: 3.02 → 3.28 → 3.29 GB (converging to actual ceiling)

### Industry Research Findings

**This is an industry-wide problem. No one has solved accurate "available memory" detection on Android.**

| Project | Approach | Status |
|---------|----------|--------|
| **llama.cpp** | No pre-load estimation | [Issue #4315](https://github.com/ggml-org/llama.cpp/issues/4315) closed due to inactivity |
| **MLC LLM** | Compile-time static estimation | [Issue #2748](https://github.com/mlc-ai/mlc-llm/issues/2748) - can't measure runtime accurately |
| **Ollama** | Pre-calculated estimates | [Issue #10359](https://github.com/ollama/ollama/issues/10359) - overestimates by 2.2x |
| **Google Memory Advice API** | ML-based multi-signal | **Deprecated** - not reliable enough |
| **TensorFlow Lite** | Profiling tools only | No pre-load estimation |

**Common workarounds:**
1. Static model-size estimation (ignore volatile runtime APIs)
2. Device tiering (% of total RAM by device class)
3. Warn but allow override + handle failure gracefully
4. User choice / manual configuration

---

## FINAL DESIGN (Clean Implementation)

> **Status**: This is the definitive design after multiple iterations. Previous sections contain research and deliberation history.

### Problem Statement

We need to determine if a device can load a given model. Two questions:
1. **How much memory does the model need?** → Model Memory Estimation
2. **How much memory can the device provide?** → Available Memory Calibration

### Core Principles

1. **One estimation function** - Single source of truth for model memory requirements
2. **Two calibration variables** - Simple, no complex branching
3. **Consistency** - Same formula used for checking AND tracking
4. **Reliability** - Only use `getAvailableMemory()` at clean states (no model loaded)

### Part 1: Model Memory Estimation

**One function** that handles everything:

```
getModelMemoryRequirement(model, projectionModel?, contextSettings?) → bytes

  IF model has GGUF metadata AND contextSettings available:
    Use accurate formula: (Weights + KV Cache + Compute Buffer) × 1.1
  ELSE:
    Use fallback: (modelSize + mmProjSize) × 1.1
```

**Key points:**
- Lives in `memoryEstimator.ts`
- Takes Model object directly (not scattered parameters)
- Handles multimodal (mmproj) internally
- Returns bytes (consistent unit)
- Used everywhere: memory checks, calibration tracking

### Part 2: Available Memory Calibration

**Two variables** (persisted):

| Variable | Purpose | When Updated |
|----------|---------|--------------|
| `availableMemoryCeiling` | OS-reported ceiling | App startup (if undefined), after model release |
| `largestSuccessfulLoad` | Ground truth of what worked | After successful model load |

**Memory check logic:**
```
ceiling = max(largestSuccessfulLoad, availableMemoryCeiling)
safeCeiling = ceiling × 0.9   // 10% safety margin
PASS if modelRequirement <= safeCeiling
```

**Calibration lifecycle:**
```
APP STARTUP (before any model loads):
  if availableMemoryCeiling is undefined:
    availableMemoryCeiling = getAvailableMemory()

SUCCESSFUL LOAD:
  estimated = getModelMemoryRequirement(model, projectionModel, contextSettings)
  largestSuccessfulLoad = max(largestSuccessfulLoad, estimated)

MODEL RELEASE:
  availableMemoryCeiling = max(availableMemoryCeiling, getAvailableMemory())
```

### Part 3: GGUF-Based Estimation Formula

When GGUF metadata is available:

```
Total = (Weights + KV Cache + Compute Buffer) × 1.1

Weights = file_size (already quantized in GGUF)

KV Cache = n_layers × effective_ctx × (head_k + head_v) × n_head_kv × bytes_per_element
  - effective_ctx = min(n_ctx, sliding_window) for SWA layers
  - bytes_per_element: f16=2.0, q8_0≈1.06, q4_0≈0.56

Compute Buffer = (n_vocab + n_embd) × n_ubatch × 4

Multimodal: Add mmProjSize × 1.1
```

### Why This Design

1. **Single function** → No scattered/duplicated logic
2. **Two variables** → Simple state, no complex "cold start" vs "calibrated" branching
3. **Estimator for tracking** → Don't use unreliable memory delta measurements
4. **getAvailableMemory() only at clean states** → More reliable readings

---

## Historical Research (Reference)

> The sections below contain research and deliberation history that led to the final design.

### Previous Approaches (Superseded)
  const totalModelSize = modelSize + (mmProjSize || 0);
  return totalModelSize * 1.1;  // 10% overhead for compute buffers
}
```

**Why simplified:**
- Uses actual `projectionModel.size` instead of hardcoded 1.8GB
- Single multiplier (1.1x) is defensible and minimal
- Calibration system compensates for any estimation errors
- Complex KV cache formulas can be added later if empirically validated

### mmproj Size Ranges (Reference)

| Model Family | mmproj Size Range |
|--------------|-------------------|
| SmolVLM | ~104 MB |
| LLaVA | 177 MB - 700 MB |
| Qwen2-VL/Qwen2.5-VL | 500 MB - 2.7 GB |

**Conclusion**: Must use `projectionModel.size` instead of hardcoded 1.8GB.

### Industry Formulas (Reference Only)

These formulas are preserved for future reference if we need more precise estimation:

<details>
<summary>KV Cache Formulas (not implemented - needs validation)</summary>

**KolosalAI Simple:**
```
KV Cache = bytes_per_value × hidden_size × n_layers × context_tokens
```

**Theoretical:**
```
KV Cache = 2 × n_layers × n_kv_heads × head_dim × n_tokens × bytes_per_element
```

**Oobabooga VRAM (365 MiB median error):**
```
vram = (size_per_layer - 17.99 + 3.15e-05 * kv_cache_factor)
       * (gpu_layers + ...)
```

</details>

### Phase 3 Implementation Steps

| Step | Description | Priority |
|------|-------------|----------|
| 1 | Add `CalibrationData` interface and AsyncStorage persistence | HIGH |
| 2 | Update `largestSuccessfulLoad` on successful model load | HIGH |
| 3 | Measure and track `maxAvailMemAfterRelease` on model release | HIGH |
| 4 | Update `hasEnoughMemory()` to use calibrated ceiling | HIGH |
| 5 | Replace hardcoded 1.8GB with `projectionModel.size` in estimation | HIGH |
| 6 | Update cold start logic: `max(totalRAM × 0.5, currentAvailMem)` | MEDIUM |

---

## Phase 4: GGUF-Based Memory Estimation (Planned)

### Problem

Phase 3 uses `totalModelSize × 1.1` which doesn't account for:
1. **Context length**: KV cache scales linearly with `n_ctx`
2. **KV cache quantization**: `cache_type_k`/`cache_type_v` (q8_0, q4_0, f16) significantly affects memory
3. **Architecture variations**: SWA models (Gemma), hybrid models (LFM), GQA (different n_head_kv)

Testing with `temp/memory_estimator.py` shows that it accurately estimates memory for most of the models and with 10% overhead on top of the formula, we can almost always cover the rest.

### Proposed Solution

Use GGUF metadata from `loadLlamaModelInfo()` combined with user's context settings.

#### Memory Formula

```
Total Memory = (Weights + KV Cache + Compute Buffer) × 1.1

Where:
  Weights = file_size (already quantized)

  KV Cache = n_layers × n_ctx × (n_embd_head_k + n_embd_head_v) × n_head_kv × bytes_per_element
    - bytes_per_element depends on cache_type_k/cache_type_v:
      f16: 2.0, q8_0: 1.0625, q4_0: 0.5625, etc.
    - SWA models: SWA layers use min(n_ctx, sliding_window)
    - Hybrid models: only attention layers have KV cache

  Compute Buffer = (n_vocab + n_embd) × n_ubatch × 4

Note: Output Buffer (n_vocab × 4) dropped - contributes <1% to total.
```

#### Data Model

Add `ggufMetadata` field to Model type:

```typescript
interface GGUFMetadata {
  architecture: string;
  n_layers: number;
  n_embd: number;
  n_head: number;
  n_head_kv: number;
  n_vocab: number;
  n_embd_head_k?: number;  // Key head dimension
  n_embd_head_v?: number;  // Value head dimension
  sliding_window?: number; // For Gemma SWA models
}
```

#### Data Flow

```
DOWNLOADED MODELS:
  Download completes → loadLlamaModelInfo() → Parse & persist in Model.ggufMetadata
  Memory check → Use ggufMetadata + n_ctx + cache_type_k/v → Accurate estimate

HF BROWSING (not downloaded):
  Model card → Fallback to file_size × 1.1 (acceptable pre-download)
```

#### Integration Points

1. **After download completes**: Call `loadLlamaModelInfo()`, parse relevant fields, persist
2. **`memoryRequirementEstimate()`**: Use GGUF metadata if available, else fallback
3. **Context from settings**: Read `n_ctx`, `cache_type_k`, `cache_type_v` from `contextInitParams`

### Fallback Chain

```
1. ggufMetadata available → Full formula with n_ctx + kv_type
2. Downloaded but no metadata → Read once, persist, then use
3. HF browsing → file_size × 1.1 (Phase 3 formula)
4. All else fails → file_size × 1.1
```

### Phase 4 Implementation Steps

| Step | Description | Priority |
|------|-------------|----------|
| 1 | Create `src/utils/memoryEstimator.ts` with core calculation functions | HIGH |
| 2 | Add `GGUFMetadata` interface and add to Model type | HIGH |
| 3 | Parse and persist GGUF metadata after download completes | HIGH |
| 4 | Update `memoryRequirementEstimate()` to use GGUF-based calculation | HIGH |
| 5 | Pass `n_ctx`, `cache_type_k`, `cache_type_v` from context settings | MEDIUM |
| 6 | Handle SWA models (Gemma) with sliding window | LOW |
| 7 | Handle hybrid models (LFM) with per-layer KV heads | LOW |

### KV Cache Type Bytes Reference

| Type | Bytes per element |
|------|-------------------|
| f32 | 4.0 |
| f16 | 2.0 |
| bf16 | 2.0 |
| q8_0 | 1.0625 (34/32) |
| q4_0 | 0.5625 (18/32) |
| q4_1 | 0.625 (20/32) |
| q5_0 | 0.6875 (22/32) |
| q5_1 | 0.75 (24/32) |

### Sources

- [llama.cpp Memory Estimation Issue #4315](https://github.com/ggml-org/llama.cpp/issues/4315)
- [MLC LLM Memory Measurement Issue #2748](https://github.com/mlc-ai/mlc-llm/issues/2748)
- [Ollama Memory Allocation Issue #10359](https://github.com/ollama/ollama/issues/10359)
- [Oobabooga VRAM Formula](https://oobabooga.github.io/blog/posts/gguf-vram-formula/)
- [KolosalAI Memory Calculator](https://github.com/KolosalAI/model-memory-calculator)
- [llama.cpp Memory Discussion #9936](https://github.com/ggml-org/llama.cpp/discussions/9936)
- [Android Memory Management](https://developer.android.com/topic/performance/memory-management)

---

## Changelog

| Date | Agent/Human | Change |
|------|-------------|--------|
| 2026-01-26 | orchestrator | Created worktree and task |
| 2026-01-26 | planner | Initial story draft with detailed research |
| 2026-01-27 | human | Added Phase 3 research: volatile memory problem & improved estimation design |
| 2026-01-27 | deliberation | AI deliberation (proposer/challenger) converged on calibration approach |
| 2026-01-27 | human | Refined solution: dual signals (largestSuccessfulLoad + maxAvailMemAfterRelease), simplified model estimation (totalModelSize × 1.1) |
| 2026-01-29 | implementer | Implemented Phase 3: calibration tracking, updated hasEnoughMemory with cold start logic |
| 2026-01-29 | human | Added Phase 4 plan: GGUF-based memory estimation using model metadata + context settings |
| 2026-01-29 | implementer | Implemented Phase 4: GGUF-based estimation, memoryEstimator.ts, GGUFMetadata in Model type |
| 2026-01-29 | human | Major refactor: simplified to 2 variables (largestSuccessfulLoad + availableMemoryCeiling), removed loadedModelMemoryUsage |
| 2026-01-29 | human | Final fix: use GGUF estimator for largestSuccessfulLoad instead of unreliable memory delta measurement |
| 2026-01-29 | human | **RESTART**: Code became fragmented. Documented clean design, reverting to implement from scratch |

---

## Implementation Status

**Current**: Reverting messy implementation to start fresh with clean architecture.

See **FINAL DESIGN** section above for the definitive approach.

### What Needs Implementation

1. **`memoryEstimator.ts`** - Single `getModelMemoryRequirement()` function
2. **`ModelStore.ts`** - Two variables, calibration lifecycle
3. **`useMemoryCheck.ts`** - Simple check using the single function
4. **Types** - `GGUFMetadata` in Model type

---

## Previous Implementation Report (Reference)

**Removed:**
- ~~`maxAvailMemAfterRelease`~~ - Merged into `availableMemoryCeiling`
- ~~`loadedModelMemoryUsage`~~ - Not needed with proper initialization timing
- ~~Cold start vs calibrated branching~~ - One simple path

### Files Modified

**ModelStore.ts:**
- Two observable properties: `largestSuccessfulLoad`, `availableMemoryCeiling`
- `initializeStore()`: Initialize `availableMemoryCeiling` at startup if undefined
- On successful load: Update `largestSuccessfulLoad` using **GGUF estimator** (not memory delta!)
- On release: Update `availableMemoryCeiling` with max (only place we use getAvailableMemory)

**Key Consistency Fix:**
- Old approach: `largestSuccessfulLoad = memoryBefore - memoryAfter` (unreliable!)
- New approach: `largestSuccessfulLoad = estimateMemory(model)` (consistent with checks)
- If we can't trust `getAvailableMemory()` for checking, we can't trust it for measurement
- Now the same GGUF-based formula is used everywhere

**useMemoryCheck.ts:**
- Simple logic: `ceiling = max(largestSuccessfulLoad, availableMemoryCeiling)`
- Fallback: `totalRAM × 0.5` if both undefined (rare edge case)
- Apply 10% safety margin
- Compare required vs safe ceiling

### Phase 4: GGUF-Based Estimation

**Files Created:**
- `src/utils/memoryEstimator.ts` - Core estimation module
- `src/utils/__tests__/memoryEstimator.test.ts` - 11 unit tests

**Files Modified:**
- `src/utils/types.ts` - Added GGUFMetadata interface, ggufMetadata field to Model
- `src/store/ModelStore.ts` - Added fetchAndPersistGGUFMetadata() method
- `src/hooks/useMemoryCheck.ts` - GGUF-based memoryRequirementEstimate()
- `src/screens/ModelsScreen/ModelCard/ModelCard.tsx` - Pass ggufMetadata to useMemoryCheck

**Key Implementation Details:**
- Memory formula: `(Weights + KV Cache + Compute Buffer) × 1.1`
- KV cache: `n_layers × n_ctx × (head_k + head_v) × n_head_kv × bytes_per_element`
- SWA support: Gemma models with sliding_window properly handled
- Fallback: `file_size × 1.1` when GGUF metadata unavailable
- GGUF metadata parsed from loadLlamaModelInfo() after download completes
- Context settings (n_ctx, cache_type_k, cache_type_v) read from modelStore.contextInitParams

### Test Results

All tests passing:
- `memoryEstimator.test.ts`: 11 tests
- `useMemoryCheck.test.ts`: 8 tests
- `ModelStore.test.ts`: 112 tests
